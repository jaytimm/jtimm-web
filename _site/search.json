[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jason Timm",
    "section": "",
    "text": "nlp\n\n\nlexical semantics\n\n\n\n\nA uniform approach\n\n\n\n\n\n\nJul 12, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\ndemographics\n\n\n\n\nGenerations in Congress\n\n\n\n\n\n\nJun 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\n\n\nA structured, Wikipedia-based timeline of American presidencies 45 & 46\n\n\n\n\n\n\nJun 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ntwitter\n\n\namerican politics\n\n\nclassification\n\n\n\n\nA look at the prevalence of incivility towards US lawmakers on Twitter\n\n\n\n\n\n\nJun 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\npython\n\n\n\n\nA quick demo using the spacyfishing python library\n\n\n\n\n\n\nJun 26, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nclassification\n\n\n\n\nsome approaches to text classification using R\n\n\n\n\n\n\nJun 14, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\ntwitter\n\n\ndemographics\n\n\nclassification\n\n\n\n\nImplementing M3 for demographic inference\n\n\n\n\n\n\nJun 10, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\npython\n\n\nlexical semantics\n\n\n\n\nSome notes on implementing reticulate, spacy, and BERT\n\n\n\n\n\n\nJul 9, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\n\n\nThe basics of gerrymandering: a simple simulation\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\ngeospatial\n\n\n\n\nA ggplot reference for performing some common geo-spatial analyses\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\ndemographics\n\n\n\n\nA quick look at the composition of American generations\n\n\n\n\n\n\nJun 10, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "linguistic innovation and change in online speech communities;\npolitical communication on social media;\nthe integration of corpus methods and geographic information systems; and\nthe development of open source tools for linguistic analysis & text featurization.\n\nThis site is built with r::blogdown and Hugo, and deployed with Netlify. Check out R-bloggers & rweekly.org for additonal R resources.\nI live in Albuquerque, New Mexico, and mostly just bicycle about."
  },
  {
    "objectID": "posts/text-class/index.html",
    "href": "posts/text-class/index.html",
    "title": "notes on text classification",
    "section": "",
    "text": "A re-worked version of a previous post. A very small survey of some simple, but effective approaches to text classification using R, with a focus on Naive Bayes and FastText classifiers."
  },
  {
    "objectID": "posts/text-class/index.html#labeled-data",
    "href": "posts/text-class/index.html#labeled-data",
    "title": "notes on text classification",
    "section": "1 Labeled data",
    "text": "1 Labeled data\nFor demonstration purposes, we build a corpus using the quicknews package. The corpus is comprised of articles returned from a set of health-related queries. Search terms, then, serve as classification labels. An imperfect annotation process, but fine for our purposes here. As “distant” supervision.\n\nlibrary(tidyverse)\n\nterms <- c('heart disease', \n           'diabetes',\n           'mental health',\n           'substance abuse',\n           'obesity',\n           'kidney disease')\n\nrss <- lapply(terms, function(x) {\n  \n  quicknews::qnews_build_rss(x) %>%\n    quicknews::qnews_strip_rss() }) \n\nnames(rss) <- terms\n  \nrss0 <- rss %>%\n  bind_rows(.id = 'term') %>%\n  mutate(term = gsub(' ', '_', term)) %>%\n  distinct(link, .keep_all = TRUE) %>%\n  mutate(doc_id = as.character(row_number())) %>%\n  mutate(term = as.factor(term))  %>%\n  select(doc_id, term:link)\n\n\narticles <- quicknews::qnews_extract_article(url = rss0$link, cores = 7) \narticles0 <- articles %>% left_join(rss0)\n\nDescriptives for the resulting corpus by search term. So, a super small demo corpus.\n\narticles0 %>%\n  mutate(words = tokenizers::count_words(text)) %>%\n  group_by(term) %>%\n  summarize(n = n(), words = sum(words)) %>%\n  janitor::adorn_totals() %>%\n  knitr::kable()\n\n\n\n\nterm\nn\nwords\n\n\n\n\ndiabetes\n81\n72045\n\n\nheart_disease\n82\n65505\n\n\nkidney_disease\n80\n68379\n\n\nmental_health\n82\n73804\n\n\nobesity\n81\n69179\n\n\nsubstance_abuse\n84\n64133\n\n\nTotal\n490\n413045\n\n\n\n\n\nA sample of articles from the GoogleNews/quicknews query:\n\nset.seed(99)\narticles0 %>% \n  select(term, date, source, title) %>% \n  sample_n(5) %>%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nterm\ndate\nsource\ntitle\n\n\n\n\nkidney_disease\n2022-05-23\nReuters.com\nU.S. Task Force to consider routine kidney disease screening as new drugs available\n\n\nsubstance_abuse\n2022-06-15\nThe Sylva Herald\nDistrict Court | Court News | thesylvaherald.com\n\n\nsubstance_abuse\n2022-05-28\nWUSF News\nNortheast Florida’s health care concerns: from substance abuse to food deserts\n\n\nheart_disease\n2022-05-23\nBloomberg\nTake Breaks and Watch Less TV to Slash Heart Disease Risk, Experts Say\n\n\nsubstance_abuse\n2022-06-14\nWilliston Daily Herald\nReport: U.S. records highest ever rates of substance abuse, suicide-related deaths"
  },
  {
    "objectID": "posts/text-class/index.html#data-structures",
    "href": "posts/text-class/index.html#data-structures",
    "title": "notes on text classification",
    "section": "2 Data structures",
    "text": "2 Data structures\n\n2.1 Document-Term Matrix\n\nAs bag-of-words\n\n\ndtm <- articles0 %>% \n  mutate(wds = tokenizers::count_words(text)) %>%\n  filter(wds > 200 & wds < 1500) %>%\n  text2df::tif2token() %>%\n  text2df::token2df() %>%\n  mutate(token = tolower(token)) \n  # mutate(stem = quanteda::char_wordstem(token))\n\ndtm %>% head() %>% knitr::kable()\n\n\n\n\ndoc_id\ntoken\ntoken_id\n\n\n\n\n2\nheart\n1\n\n\n2\ndisease\n2\n\n\n2\nis\n3\n\n\n2\na\n4\n\n\n2\nvery\n5\n\n\n2\ngeneral\n6\n\n\n\n\n\n\ndtm_tok <- dtm %>%  \n  count(doc_id, token) %>%\n  group_by(token) %>%\n  mutate(docf = length(unique(doc_id))) %>% ungroup() %>%\n  mutate(docf = round(docf/length(unique(doc_id)), 3 )) %>%\n  filter(docf >= 0.01 & docf < 0.5 & \n           !grepl('^[0-9]|^[[:punct:]]', token))\n\ndtm_tok %>% head() %>% knitr::kable()\n\n\n\n\ndoc_id\ntoken\nn\ndocf\n\n\n\n\n10\nacknowledge\n1\n0.031\n\n\n10\nacross\n1\n0.207\n\n\n10\nadditional\n1\n0.151\n\n\n10\naddress\n3\n0.217\n\n\n10\nadhere\n1\n0.018\n\n\n10\nads\n1\n0.026\n\n\n\n\n\n\ndtm_sparse <- dtm_tok %>%\n  tidytext::bind_tf_idf(term = token, \n                        document = doc_id,\n                        n = n) %>% \n  tidytext::cast_sparse(row = doc_id, \n                        column = token, \n                        value = tf_idf)\n\n\n\n2.2 Cleaned text\n\nctext <- dtm %>%\n  group_by(doc_id) %>%\n  summarize(text = paste0(token, collapse = ' ')) %>% ungroup()\n\nstrwrap(ctext$text[5], width = 60)[1:5]\n\n[1] \"upon receiving a diagnosis of type 1 diabetes ( t1d ) ,\"    \n[2] \"many people have the same reaction : “ but why me ? ” some\" \n[3] \"people have t1d that runs in their family , while others\"   \n[4] \"have no idea how or why they received a diagnosis . often ,\"\n[5] \"to their frustration , those questions go unanswered . but\" \n\n\n\n\n2.3 Word embeddings\n\n## devtools::install_github(\"pommedeterresautee/fastrtext\") \ntmp_file_txt <- tempfile()\ntmp_file_model <- tempfile()\nwriteLines(text = ctext$text, con = tmp_file_txt)\ndims <- 25\nwindow <- 5\n\nfastrtext::execute(commands = c(\"skipgram\",\n                                \"-input\", tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", gsub('^.*\\\\.', '', dims),\n                                \"-ws\", window, \n                                \"-verbose\", 1))\n\n\nRead 0M words\nNumber of words:  5318\nNumber of labels: 0\n\nProgress: 100.0% words/sec/thread:   21912 lr:  0.000000 avg.loss:  2.412294 ETA:   0h 0m 0s\n\nfast.model <- fastrtext::load_model(tmp_file_model)\n\nadd .bin extension to the path\n\nfast.dict <- fastrtext::get_dictionary(fast.model)\nembeddings <- fastrtext::get_word_vectors(fast.model, fast.dict)"
  },
  {
    "objectID": "posts/text-class/index.html#classification",
    "href": "posts/text-class/index.html#classification",
    "title": "notes on text classification",
    "section": "3 Classification",
    "text": "3 Classification\n\narticles1 <- articles0 %>%\n  arrange(doc_id) %>%\n  filter(doc_id %in% unique(dtm_tok$doc_id))\n\n\nset.seed(99)\ntrainIndex <- caret::createDataPartition(articles1$term, p = .7)$Resample1\n\n\n3.1 Document-Term Matrix & Naive Bayes\n\ndtm_train <- dtm_sparse[trainIndex, ]\ndtm_test <- dtm_sparse[-trainIndex, ] \ndtm_classifier <- e1071::naiveBayes(as.matrix(dtm_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\ndtm_predicted <- predict(dtm_classifier, as.matrix(dtm_test))\n\n\n\n3.2 Word embeddings & Naive Bayes\n\nv1 <- embeddings %>% \n  data.frame() %>%\n  mutate(token = rownames(embeddings)) %>%\n  filter(token %in% unique(dtm_tok$token)) %>%\n  inner_join(dtm)\n\nJoining, by = \"token\"\n\navg0 <- lapply(unique(dtm$doc_id), function(y){\n  \n  d0 <- subset(v1, doc_id == y)\n  d1 <- as.matrix(d0[, 1:dims])\n  d2 <-Matrix.utils::aggregate.Matrix(d1,\n                                      groupings = rep(y, nrow(d0)),\n                                      fun = 'mean')\n  as.matrix(d2)\n})\n\ndoc_embeddings <- do.call(rbind, avg0)\n\n\nemb_train <- doc_embeddings[trainIndex, ]\nemb_test <- doc_embeddings[-trainIndex, ] \nemb_classifier <- e1071::naiveBayes(as.matrix(emb_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\nemb_predicted <- predict(emb_classifier, as.matrix(emb_test))\n\n\n\n3.3 Text classification via FastText\n\nfast_train <- articles1[trainIndex, ]\nfast_test <- articles1[-trainIndex, ]\n\nPrepare data for FastText.\n\ntmp_file_model <- tempfile()\n\ntrain_labels <- paste0(\"__label__\", fast_train$term)\ntrain_texts <- tolower(fast_train$text)\ntrain_to_write <- paste(train_labels, train_texts)\ntrain_tmp_file_txt <- tempfile()\nwriteLines(text = train_to_write, con = train_tmp_file_txt)\n\ntest_labels <- paste0(\"__label__\", fast_test$term)\ntest_texts <- tolower(fast_test$text)\ntest_to_write <- paste(test_labels, test_texts)\n\n\nfastrtext::execute(commands = c(\"supervised\", \n                                \"-input\", train_tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", 25, \n                                \"-lr\", 1, \n                                \"-epoch\", 20, \n                                \"-wordNgrams\", 2, \n                                \"-verbose\", 1))\n\nmodel <- fastrtext::load_model(tmp_file_model)\nfast_predicted0 <- predict(model, sentences = test_to_write)\nfast_predicted <- as.factor(names(unlist(fast_predicted0)))"
  },
  {
    "objectID": "posts/text-class/index.html#evaluation",
    "href": "posts/text-class/index.html#evaluation",
    "title": "notes on text classification",
    "section": "4 Evaluation",
    "text": "4 Evaluation\n\npredictions <- list('BOW' = dtm_predicted, \n                    'Word embeddings' = emb_predicted, \n                    'FastText' = fast_predicted)\n\n\n4.1 Model accuracy\n\nconf_mats <- lapply(predictions, \n                    caret::confusionMatrix,\n                    reference = articles1[-trainIndex, ]$term)\n\nsums <- lapply(conf_mats, '[[', 3) \nsums0 <- as.data.frame(do.call(rbind, sums)) %>%\n  select(1:4) %>%\n  mutate_at(1:4, round, 3)\n\nsums0 %>% arrange(desc(Accuracy)) %>% knitr::kable()\n\n\n\n\n\nAccuracy\nKappa\nAccuracyLower\nAccuracyUpper\n\n\n\n\nFastText\n0.757\n0.708\n0.668\n0.832\n\n\nBOW\n0.661\n0.593\n0.567\n0.747\n\n\nWord embeddings\n0.435\n0.320\n0.343\n0.530\n\n\n\n\n\n\n\n4.2 FastText classifier: Model accuracy by class\n\nconf_mats[['FastText']]$byClass %>% data.frame() %>%\n  select (Sensitivity, Specificity, Balanced.Accuracy) %>%\n  rownames_to_column(var = 'topic') %>%\n  mutate(topic = gsub('Class: ','', topic)) %>% \n  mutate_if(is.numeric, round, 2) %>% \n  knitr::kable() \n\n\n\n\ntopic\nSensitivity\nSpecificity\nBalanced.Accuracy\n\n\n\n\ndiabetes\n0.75\n1.00\n0.88\n\n\nheart_disease\n0.56\n0.99\n0.77\n\n\nkidney_disease\n0.89\n0.86\n0.88\n\n\nmental_health\n0.89\n0.96\n0.93\n\n\nobesity\n0.74\n0.93\n0.83\n\n\nsubstance_abuse\n0.70\n0.97\n0.83\n\n\n\n\n\n\n\n4.3 FastText classifier: Confusion matrix\n\ndp <- as.data.frame(conf_mats[['FastText']]$table)\n\nggplot(data = dp,\n       aes(x = Reference, y = Prediction)) +\n  \n    geom_tile(aes(fill = log(Freq)), \n              colour = \"white\") +\n    scale_fill_gradient(low = \"white\", \n                        high = \"steelblue\") +\n  \n    geom_text(data = dp,\n              aes(x = Reference, \n                  y = Prediction, \n                  label = Freq)) +\n    theme(legend.position = \"none\",\n          axis.text.x=element_text(angle=45,\n                                   hjust=1)) + \n    labs(title=\"Confusion Matrix\")"
  },
  {
    "objectID": "posts/text-class/index.html#summary",
    "href": "posts/text-class/index.html#summary",
    "title": "notes on text classification",
    "section": "5 Summary",
    "text": "5 Summary"
  },
  {
    "objectID": "posts/demographic-inference/demographic-inference.html",
    "href": "posts/demographic-inference/demographic-inference.html",
    "title": "twitter users, demographic inference & reticulate",
    "section": "",
    "text": "A simple code-through for using the Python library m3inference in R via reticulate. As described in Wang et al. (2019). Library facilitates demographic attribute inference of Twitter users, namely, gender, age, and organizational status, based on profile images, screen names, names, and biographies."
  },
  {
    "objectID": "posts/demographic-inference/demographic-inference.html#reticulate-python",
    "href": "posts/demographic-inference/demographic-inference.html#reticulate-python",
    "title": "twitter users, demographic inference & reticulate",
    "section": "1 Reticulate & Python",
    "text": "1 Reticulate & Python\nFirst, we build a conda environment (via the terminal) comprised of m3inference and pip (and their respective dependencies).\n\n## <TERMINAL>\nconda create -n m3demo\nsource activate m3demo\nconda install pip \n/home/jtimm/anaconda3/envs/m3demo/bin/pip install m3inference\n\nThen we establish Python and conda environment paths.\n\n## <R-console>\nSys.setenv(RETICULATE_PYTHON = \"/home/jtimm/anaconda3/envs/m3demo/bin/python\")\n\nlibrary(reticulate)\n#reticulate::use_python(\"/home/jtimm/anaconda3/envs/m3demo/bin/python\")\nreticulate::use_condaenv(condaenv = \"m3demo\",\n                         conda = \"/home/jtimm/anaconda3/bin/conda\")"
  },
  {
    "objectID": "posts/demographic-inference/demographic-inference.html#twitter-data",
    "href": "posts/demographic-inference/demographic-inference.html#twitter-data",
    "title": "twitter users, demographic inference & reticulate",
    "section": "2 Twitter data",
    "text": "2 Twitter data\nFor demonstration purposes, we identify/extract my Twitter followers (and some of their M3-relevant features) using the rtweet package.\n\n## <R-console>\nlibrary(tidyverse)\nfws  <- rtweet::get_followers(user = 'DrJayTimm') \n\nusers <- rtweet::lookup_users(fws$user_id) %>%\n  select(user_id, name, screen_name, description, profile_image_url)\n\nBelow is a simple hack to provide the M3 model with an actual image file for Twitter profiles that lack profile pics.\n\n## <R-console>\njk <- 'http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png'\njk0 <- 'https://twirpz.files.wordpress.com/2015/06/twitter-avi-gender-balanced-figure.png'\n\ndir0 <- tempdir()\n\nusers2 <- users %>%\n  mutate(profile_image_url = ifelse(profile_image_url == jk, jk0, profile_image_url)) %>%\n  rename(id_str = user_id)"
  },
  {
    "objectID": "posts/demographic-inference/demographic-inference.html#profile-pics-via-m3",
    "href": "posts/demographic-inference/demographic-inference.html#profile-pics-via-m3",
    "title": "twitter users, demographic inference & reticulate",
    "section": "3 Profile pics via M3",
    "text": "3 Profile pics via M3\nOutput Twitter user details to local temp directory as a ~ ndjson file.\n\n## <R-console>\ntmp2 <- tempfile()\njsonlite::stream_out(users2, file(tmp1 <- tempfile()), verbose = F)\n\nIn a Python console, we then import the M3Twitter module, and set the directory in which Twitter profile pics will be stored. (Note that the directory established in the R chunk above is accessed below via the r. prefix.)\n\n## <PYTHON-console>\nfrom m3inference import M3Twitter\nm3twitter = M3Twitter(cache_dir = r.dir0) \n\nThen, via the transform_jsonl function, we restructure the ndjson/jsonl file and download Twitter users’ profile pics to the temp directory. This function also identifies description language. Note: While we can download profile images and identify description language in R, things tend to go much more smoothly (& quicker) using the functionality included in m3inference.\n\n## <PYTHON-console>\nm3twitter.transform_jsonl(input_file = r.tmp1, \n                          output_file = r.tmp2, \n                          img_path_key = \"profile_image_url\")#, \n                          #lang_key = \"lang\")\n\n/home/jtimm/anaconda3/envs/m3demo/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn("
  },
  {
    "objectID": "posts/demographic-inference/demographic-inference.html#deomgraphic-inference-via-m3",
    "href": "posts/demographic-inference/demographic-inference.html#deomgraphic-inference-via-m3",
    "title": "twitter users, demographic inference & reticulate",
    "section": "4 Deomgraphic inference via M3",
    "text": "4 Deomgraphic inference via M3\nApply M3 classification model. Attribute classes:\n\nGender: male, female;\nAge: <= 18, 19-29, 30-39, >=40; and\nOrganization: non-org, is-org.\n\n\n## <PYTHON-console>\nfrom m3inference import M3Inference\nm3 = M3Inference() \npred = m3.infer(r.tmp2)\n\n\n4.1 Accessing classifications\nOutput/predictions from the Python-based M3 model can be moved into R via the (R-based) reticulate::py function.\n\n## <R-console>\npy_predictions <- reticulate::py$pred\n\nThe table below details age-gender-organization inferences by Twitter ID for a small subset of my followers.\n\n## <R-console>\ndf <- reshape2::melt(py_predictions) \ndf0 <- data.table::setDT(df)[, .SD[which.max(value)], by = list(L1, L2)]\ndf1 <- data.table::dcast(df0, L1  ~ L2, value.var = 'L3')\n\n\n## <R-console>\ndf1 %>% sample_n(10) %>% knitr::kable()\n\n\n\n\nid\nage\ngender\norg\n\n\n\n\n3294608261\n>=40\nmale\nnon-org\n\n\n159724355\n>=40\nmale\nnon-org\n\n\n1373105712\n19-29\nmale\nnon-org\n\n\n712116930782433284\n30-39\nfemale\nnon-org\n\n\n14933875\n>=40\nmale\nnon-org\n\n\n586304056\n30-39\nmale\nnon-org\n\n\n1139150461599195137\n19-29\nmale\nnon-org\n\n\n477371410\n<=18\nmale\nnon-org\n\n\n46111691\n>=40\nfemale\nnon-org\n\n\n734578981954031616\n>=40\nfemale\nis-org\n\n\n\n\n\n\n\n4.2 Demographic summary\n\n4.2.1 By Organization\n\ntable(df1$org)\n\n\n is-org non-org \n     16     159 \n\n\n\n\n4.2.2 By Age & Gender\n(for followers that have not been classified as organizations):\n\n## <R-console>\ndf2 <- df1 %>%\n  mutate(age = factor(age, levels = c('<=18', '19-29', '30-39', '>=40'))) %>%\n  filter(org != 'is-org') %>%\n  count(gender, age) %>%\n  mutate(percent = round(n/sum(n)*100,1)) %>%\n  mutate(percent = ifelse(gender == \"male\", percent*-1, percent))\n\ndf2 %>% knitr::kable()\n\n\n\n\ngender\nage\nn\npercent\n\n\n\n\nfemale\n19-29\n12\n7.5\n\n\nfemale\n30-39\n5\n3.1\n\n\nfemale\n>=40\n23\n14.5\n\n\nmale\n<=18\n6\n-3.8\n\n\nmale\n19-29\n32\n-20.1\n\n\nmale\n30-39\n25\n-15.7\n\n\nmale\n>=40\n56\n-35.2\n\n\n\n\n\n\n\n\n4.3 Age-Gender “pyramid”\n\n## <R-console>\nmaxs <- max(abs(df2$percent))\ndf2 %>%\n  ggplot(aes(x = age, y = percent, fill =gender)) +\n  geom_col(alpha = .75) + \n  ylim(-maxs - 1, maxs + 1) +\n  coord_flip() +\n  ggthemes::scale_fill_stata() +\n  # scale_y_continuous(breaks = c(-5, 0, 5),\n  #                    labels = c(\"5%\", \"0%\", \"5%\")) +\n  labs(title=\"Inferred age-gender demographics of my followers\")"
  },
  {
    "objectID": "posts/demographic-inference/demographic-inference.html#profile-pics-demographic-inference",
    "href": "posts/demographic-inference/demographic-inference.html#profile-pics-demographic-inference",
    "title": "twitter users, demographic inference & reticulate",
    "section": "5 Profile pics & demographic inference",
    "text": "5 Profile pics & demographic inference\n\n## <R-console>\nusers2$paths <- grep('224x224', dir(dir0, full.names = TRUE), value = T)\n\nusers3 <- users2 %>%\n  arrange(id_str) %>%\n  mutate(paths = grep('224x224', dir(dir0, full.names = TRUE), value = T)) %>%\n  left_join(df1, by = c('id_str' = 'id'))\n\nA simple function for modifying profile pics. Including: (1) “charcoal-ing” photos for user privacy, and (2) labeling photos with predicted age, gender, and organization classes.\n\n## <R-console>\nmodify_images <- function(paths){\n  \n  for(i in 1:length(paths)){\n    y1 <- magick::image_read(paths[i])\n    y2 <- magick::image_charcoal(y1)\n    y3 <- magick::image_border(y2, 'white', '5x5')\n    \n    ll <- paste0(users3$org[i], '\\n',\n                 users3$gender[i], '\\n',\n                 users3$age[i])\n    \n    y4 <- magick::image_annotate(y3, \n                                 text = ll, \n                                 color = \"black\", \n                                 size = 26,\n                                 weight = 700,\n                                 location = \"+10+10\")\n        \n    magick::image_write(y4, paths[i]) \n    }\n}\n\nApply function, and build a collage of profile pics with predicted demographics using the photomoe package.\n\n## <R-console>\nmodify_images(paths = users3$paths)\n\n# devtools::install_github(\"jaytimm/photomoe\")\nphotomoe::img_build_collage(paths = users3$paths, \n                            dimx = 7, \n                            dimy = 12)"
  },
  {
    "objectID": "posts/cracking-and-packing/index.html",
    "href": "posts/cracking-and-packing/index.html",
    "title": "cracking and packing",
    "section": "",
    "text": "A brief note on gerrymandering, and cracking & packing. Specifically, a simple simulation demonstrating how gross partisan asymmetries in the composition of state legislatures can be crafted from statewide populations evenly split between two parties.\nPer function below, we designate individuals in a population of N voters as either Republican or Democrat (50-50 split). Then we randomly assign each voter a district."
  },
  {
    "objectID": "posts/cracking-and-packing/index.html#example-1",
    "href": "posts/cracking-and-packing/index.html#example-1",
    "title": "cracking and packing",
    "section": "1 Example #1",
    "text": "1 Example #1\nWe simulate 1,000 election results for a state with a population of 10,000, evenly distributed across 10 legislative districts. Here, a comparison of two of these elections.\n\nset.seed(999)\nf1a <- lapply(1:1000, function(x) {simulate_election(state_pop = 10000,\n                                                     district_n = 10,\n                                                     dem_prop = .50)}) %>%\n  bind_rows(.id = 'vote') \n\nIn the first, per the vote tally below, Democrats won 5/10 seats in the legislature. A reasonable result in a state split 50-50 among Ds and Rs.\n\nf1a %>% \n  filter(dseats == 5) %>% \n  slice(1:10) %>%\n  select(district, dwin, D, R) %>%\n  mutate(Total = R + D) %>%\n  janitor::adorn_totals(where = c('row')) %>%\n  knitr::kable()\n\n\n\n\ndistrict\ndwin\nD\nR\nTotal\n\n\n\n\n1\n0\n497\n503\n1000\n\n\n2\n0\n472\n528\n1000\n\n\n3\n1\n506\n494\n1000\n\n\n4\n1\n508\n492\n1000\n\n\n5\n0\n486\n514\n1000\n\n\n6\n1\n521\n479\n1000\n\n\n7\n0\n499\n501\n1000\n\n\n8\n1\n500\n500\n1000\n\n\n9\n0\n498\n502\n1000\n\n\n10\n1\n513\n487\n1000\n\n\nTotal\n5\n5000\n5000\n10000\n\n\n\n\n\nIn the second, Democrats won 8/10 seats, despite the statewide 50-50 split. Clearly a preferable outcome for Democrats. How did they do it? Well, election results show that Dems won lots of seats by very slim margins in the simulation – maxing out at only 511 votes in districts 2 and 6. Republicans, on the other hand, won only two seats (districts 1 & 10); however, both by more sizable margins.\n\nf1a %>% \n  filter(dseats == 8) %>% \n  slice(1:10) %>%\n  select(district, dwin, D, R) %>%\n  mutate(Total = R + D) %>%\n  janitor::adorn_totals(where = c('row')) %>%\n  knitr::kable()\n\n\n\n\ndistrict\ndwin\nD\nR\nTotal\n\n\n\n\n1\n0\n483\n517\n1000\n\n\n2\n1\n511\n489\n1000\n\n\n3\n1\n503\n497\n1000\n\n\n4\n1\n502\n498\n1000\n\n\n5\n1\n506\n494\n1000\n\n\n6\n1\n511\n489\n1000\n\n\n7\n1\n505\n495\n1000\n\n\n8\n1\n504\n496\n1000\n\n\n9\n1\n502\n498\n1000\n\n\n10\n0\n473\n527\n1000\n\n\nTotal\n8\n5000\n5000\n10000\n\n\n\n\n\nSo, if we were to ascribe some agency to how individuals in our simulation were assigned to districts, and perhaps call it a “decennial redistricting panel,” for example, we would say that they created district boundaries such that Republicans did not have the numbers to win in most of the state’s districts, and achieved this by creating two districts (1 & 10) in which Republicans were many. The first part of this plan is traditionally called “cracking”; the second, “packing”."
  },
  {
    "objectID": "posts/cracking-and-packing/index.html#example-2",
    "href": "posts/cracking-and-packing/index.html#example-2",
    "title": "cracking and packing",
    "section": "2 Example #2",
    "text": "2 Example #2\nA closer look at cracking and packing, then. Parameters of our new simulation include a population of 10,000 and a legislative body comprised of 20 districts. The histogram below summarizes counts of seats won by Democrats based on election results from 1,000 simulations. The most common outcome is Democrats winning 10/20 seats. The most favorable outcome for Democrats is 14/20 seats; however, this outcome occurs in less than 0.5% of simulations.\n\nset.seed(123)\nf1 <- lapply(1:1000, function(x) {\n  simulate_election(state_pop = 10000,\n                    district_n = 20,\n                    dem_prop = .50)}) %>%\n  bind_rows(.id = 'vote')\n\nf1 %>% \n  group_by(vote) %>% \n  summarise(D = sum(dwin)) %>%\n  ggplot() +\n  geom_histogram(aes(D), binwidth = .5) +\n  scale_x_continuous(breaks=seq(min(f1$dseats), max(f1$dseats), 1))+\n  ggtitle('Dem seats won in 1K simulated elections')\n\n\n\n\nThe next plot summarizes the vote distribution for a simulated election in which Dems won 14 seats; districts have been sorted in increasing order of Republican vote share. The fourteen seats won by Dems are denoted in blue; Rs in red.\n\nset.seed(99)\nf1 %>%\n  mutate(rank = rank(r, ties.method = 'first'),\n         party = ifelse(r > 50, 'r', 'd')) %>%\n  filter(dseats == max(dseats)) %>%\n  group_by(vote) %>% nest() %>% ungroup() %>%\n  sample_n(1) %>% unnest(cols = c(data)) %>%\n  \n  ggplot(aes(x = factor(rank), \n                 y = r,\n                 color = party)) + \n  geom_point(size = 2) +\n  geom_hline(yintercept = 50, lty = 3) +\n \n  ylim(40, 60) +\n  scale_color_manual(values = c('#437193', '#ae4952')) +\n  \n  theme_minimal() +\n  theme(axis.text.x=element_blank(),\n        legend.position = 'none') +\n  xlab('Districts ordered from least to most Republican') +\n  ylab('Percentage of votes for a Republican') \n\n\n\n\nAs can be noted, at the 50% threshold, the slope of this vote distribution shows a marked increase, with Republicans garnering higher vote shares for the six seats they won in comparison to vote shares garnered by Dems for their fourteen seats. So, lots of mis-spent votes for Republicans. This is generally what crack-and-pack gerrymandering looks like (see, eg, Warrington 2018).\nA cleaner vote distribution – results from a simulated election in which Dems won 10/20 seats."
  },
  {
    "objectID": "posts/demographic-inference/index.html",
    "href": "posts/demographic-inference/index.html",
    "title": "twitter users, demographic inference & reticulate",
    "section": "",
    "text": "A simple code-through for using the Python library m3inference in R via reticulate. As described in Wang et al. (2019). Library facilitates demographic attribute inference of Twitter users, namely, gender, age, and organizational status, based on profile images, screen names, names, and biographies."
  },
  {
    "objectID": "posts/demographic-inference/index.html#reticulate-python",
    "href": "posts/demographic-inference/index.html#reticulate-python",
    "title": "twitter users, demographic inference & reticulate",
    "section": "1 Reticulate & Python",
    "text": "1 Reticulate & Python\nFirst, we build a conda environment (via the terminal) comprised of m3inference and pip (and their respective dependencies).\n\n## <TERMINAL>\nconda create -n m3demo\nsource activate m3demo\nconda install pip \n/home/jtimm/anaconda3/envs/m3demo/bin/pip install m3inference\n\nThen we establish Python and conda environment paths.\n\n## <R-console>\nSys.setenv(RETICULATE_PYTHON = \"/home/jtimm/anaconda3/envs/m3demo/bin/python\")\n\nlibrary(reticulate)\n#reticulate::use_python(\"/home/jtimm/anaconda3/envs/m3demo/bin/python\")\nreticulate::use_condaenv(condaenv = \"m3demo\",\n                         conda = \"/home/jtimm/anaconda3/bin/conda\")"
  },
  {
    "objectID": "posts/demographic-inference/index.html#twitter-data",
    "href": "posts/demographic-inference/index.html#twitter-data",
    "title": "twitter users, demographic inference & reticulate",
    "section": "2 Twitter data",
    "text": "2 Twitter data\nFor demonstration purposes, we identify/extract my Twitter followers (and some of their M3-relevant features) using the rtweet package.\n\n## <R-console>\nlibrary(tidyverse)\nfws  <- rtweet::get_followers(user = 'DrJayTimm') \n\nusers <- rtweet::lookup_users(fws$user_id) %>%\n  select(user_id, name, screen_name, description, profile_image_url)\n\nBelow is a simple hack to provide the M3 model with an actual image file for Twitter profiles that lack profile pics.\n\n## <R-console>\njk <- 'http://abs.twimg.com/sticky/default_profile_images/default_profile_normal.png'\njk0 <- 'https://twirpz.files.wordpress.com/2015/06/twitter-avi-gender-balanced-figure.png'\n\ndir0 <- tempdir()\n\nusers2 <- users %>%\n  mutate(profile_image_url = ifelse(profile_image_url == jk, jk0, profile_image_url)) %>%\n  rename(id_str = user_id)"
  },
  {
    "objectID": "posts/demographic-inference/index.html#profile-pics-via-m3",
    "href": "posts/demographic-inference/index.html#profile-pics-via-m3",
    "title": "twitter users, demographic inference & reticulate",
    "section": "3 Profile pics via M3",
    "text": "3 Profile pics via M3\nOutput Twitter user details to local temp directory as a ~ ndjson file.\n\n## <R-console>\ntmp2 <- tempfile()\njsonlite::stream_out(users2, file(tmp1 <- tempfile()), verbose = F)\n\nIn a Python console, we then import the M3Twitter module, and set the directory in which Twitter profile pics will be stored. (Note that the directory established in the R chunk above is accessed below via the r. prefix.)\n\n## <PYTHON-console>\nfrom m3inference import M3Twitter\nm3twitter = M3Twitter(cache_dir = r.dir0) \n\nThen, via the transform_jsonl function, we restructure the ndjson/jsonl file and download Twitter users’ profile pics to the temp directory. This function also identifies description language. Note: While we can download profile images and identify description language in R, things tend to go much more smoothly (& quicker) using the functionality included in m3inference.\n\n## <PYTHON-console>\nm3twitter.transform_jsonl(input_file = r.tmp1, \n                          output_file = r.tmp2, \n                          img_path_key = \"profile_image_url\")#, \n                          #lang_key = \"lang\")\n\n/home/jtimm/anaconda3/envs/m3demo/lib/python3.10/site-packages/PIL/Image.py:992: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n  warnings.warn("
  },
  {
    "objectID": "posts/demographic-inference/index.html#deomgraphic-inference-via-m3",
    "href": "posts/demographic-inference/index.html#deomgraphic-inference-via-m3",
    "title": "twitter users, demographic inference & reticulate",
    "section": "4 Deomgraphic inference via M3",
    "text": "4 Deomgraphic inference via M3\nApply M3 classification model. Attribute classes:\n\nGender: male, female;\nAge: <= 18, 19-29, 30-39, >=40; and\nOrganization: non-org, is-org.\n\n\n## <PYTHON-console>\nfrom m3inference import M3Inference\nm3 = M3Inference() \npred = m3.infer(r.tmp2)\n\n\n4.1 Accessing classifications\nOutput/predictions from the Python-based M3 model can be moved into R via the (R-based) reticulate::py function.\n\n## <R-console>\npy_predictions <- reticulate::py$pred\n\nThe table below details age-gender-organization inferences by Twitter ID for a small subset of my followers.\n\n## <R-console>\ndf <- reshape2::melt(py_predictions) \ndf0 <- data.table::setDT(df)[, .SD[which.max(value)], by = list(L1, L2)]\ndf1 <- data.table::dcast(df0, L1  ~ L2, value.var = 'L3')\n\n\n## <R-console>\ndf1 %>% sample_n(10) %>% knitr::kable()\n\n\n\n\nid\nage\ngender\norg\n\n\n\n\n1139150461599195137\n19-29\nmale\nnon-org\n\n\n394161473\n>=40\nmale\nnon-org\n\n\n164731384\n30-39\nmale\nnon-org\n\n\n700915202779279360\n>=40\nfemale\nnon-org\n\n\n477371410\n<=18\nmale\nnon-org\n\n\n1075633925521846272\n19-29\nmale\nnon-org\n\n\n35538459\n>=40\nmale\nnon-org\n\n\n232957049\n>=40\nmale\nnon-org\n\n\n785346816\n30-39\nmale\nnon-org\n\n\n381070776\n>=40\nmale\nnon-org\n\n\n\n\n\n\n\n4.2 Demographic summary\n\n4.2.1 By Organization\n\ntable(df1$org)\n\n\n is-org non-org \n     16     159 \n\n\n\n\n4.2.2 By Age & Gender\n(for followers that have not been classified as organizations):\n\n## <R-console>\ndf2 <- df1 %>%\n  mutate(age = factor(age, levels = c('<=18', '19-29', '30-39', '>=40'))) %>%\n  filter(org != 'is-org') %>%\n  count(gender, age) %>%\n  mutate(percent = round(n/sum(n)*100,1)) %>%\n  mutate(percent = ifelse(gender == \"male\", percent*-1, percent))\n\ndf2 %>% knitr::kable()\n\n\n\n\ngender\nage\nn\npercent\n\n\n\n\nfemale\n19-29\n12\n7.5\n\n\nfemale\n30-39\n5\n3.1\n\n\nfemale\n>=40\n23\n14.5\n\n\nmale\n<=18\n6\n-3.8\n\n\nmale\n19-29\n32\n-20.1\n\n\nmale\n30-39\n25\n-15.7\n\n\nmale\n>=40\n56\n-35.2\n\n\n\n\n\n\n\n\n4.3 Age-Gender “pyramid”\n\n## <R-console>\nmaxs <- max(abs(df2$percent))\ndf2 %>%\n  ggplot(aes(x = age, y = percent, fill =gender)) +\n  geom_col(alpha = .75) + \n  ylim(-maxs - 1, maxs + 1) +\n  coord_flip() +\n  ggthemes::scale_fill_stata() +\n  # scale_y_continuous(breaks = c(-5, 0, 5),\n  #                    labels = c(\"5%\", \"0%\", \"5%\")) +\n  labs(title=\"Inferred age-gender demographics of my followers\")"
  },
  {
    "objectID": "posts/demographic-inference/index.html#profile-pics-demographic-inference",
    "href": "posts/demographic-inference/index.html#profile-pics-demographic-inference",
    "title": "twitter users, demographic inference & reticulate",
    "section": "5 Profile pics & demographic inference",
    "text": "5 Profile pics & demographic inference\n\n## <R-console>\nusers2$paths <- grep('224x224', dir(dir0, full.names = TRUE), value = T)\n\nusers3 <- users2 %>%\n  arrange(id_str) %>%\n  mutate(paths = grep('224x224', dir(dir0, full.names = TRUE), value = T)) %>%\n  left_join(df1, by = c('id_str' = 'id'))\n\nA simple function for modifying profile pics. Including: (1) “charcoal-ing” photos for user privacy, and (2) labeling photos with predicted age, gender, and organization classes.\n\n## <R-console>\nmodify_images <- function(paths){\n  \n  for(i in 1:length(paths)){\n    y1 <- magick::image_read(paths[i])\n    y2 <- magick::image_charcoal(y1)\n    y3 <- magick::image_border(y2, 'white', '5x5')\n    \n    ll <- paste0(users3$org[i], '\\n',\n                 users3$gender[i], '\\n',\n                 users3$age[i])\n    \n    y4 <- magick::image_annotate(y3, \n                                 text = ll, \n                                 color = \"black\", \n                                 size = 26,\n                                 weight = 700,\n                                 location = \"+10+10\")\n        \n    magick::image_write(y4, paths[i]) \n    }\n}\n\nApply function, and build a collage of profile pics with predicted demographics using the photomoe package.\n\n## <R-console>\nmodify_images(paths = users3$paths)\n\n# devtools::install_github(\"jaytimm/photomoe\")\nphotomoe::img_build_collage(paths = users3$paths, \n                            dimx = 7, \n                            dimy = 12)"
  },
  {
    "objectID": "posts/bert/index.html",
    "href": "posts/bert/index.html",
    "title": "BERT, reticulate & lexical semantics",
    "section": "",
    "text": "This post provides some quick details on using reticulate to interface Python from RStudio; and, more specifically, using the spacy library and BERT for fine-grained lexical semantic investigation. Here we present a (very cursory) usage-based/BERT-based perspective on the semantic distinction between further and farther, using example contexts extracted from the Corpus of Contemporary American English (COCA)."
  },
  {
    "objectID": "posts/bert/index.html#python-reticulate-set-up",
    "href": "posts/bert/index.html#python-reticulate-set-up",
    "title": "BERT, reticulate & lexical semantics",
    "section": "1 Python & reticulate set-up",
    "text": "1 Python & reticulate set-up\nThe Python code below sets up a conda environment and installs relevant libraries, as well as the BERT transformer, en_core_web_trf. The following should be run in the terminal.\n\nconda create -n poly1\nsource activate poly1\nconda install -c conda-forge spacy\npython -m spacy download en_core_web_trf\nconda install numpy scipy pandas\n\nThe R code below directs R to our Python environment and Python installation.\n\nSys.setenv(RETICULATE_PYTHON = \"/home/jtimm/anaconda3/envs/m3demo/bin/python\")\n\nlibrary(reticulate)\nreticulate::use_condaenv(condaenv = \"poly1\",\n                         conda = \"/home/jtimm/anaconda3/bin/conda\")"
  },
  {
    "objectID": "posts/bert/index.html#coca",
    "href": "posts/bert/index.html#coca",
    "title": "BERT, reticulate & lexical semantics",
    "section": "2 COCA",
    "text": "2 COCA\nThe Corpus of Contemporary American English (COCA) is an absolutely lovely resource, and is one of many corpora made available by the folks at BYU. Here, we utilize COCA to build a simple data set of further-farther example usages. I have copied/pasted from COCA’s online search interface – the data set includes ~500 contexts of usage per form.\n\nlibrary(tidyverse)\ngw <- read.csv(paste0(ld, 'further-farther.csv'), sep = '\\t')\ngw$sent <- tolower(gsub(\"([[:punct:]])\", \" \\\\1 \", gw$text))\ngw$sent <- gsub(\"^ *|(?<= ) | *$\", \"\", gw$sent, perl = TRUE)\n\ngw$count <- stringr::str_count(gw$sent, 'further|farther')\ngw0 <- subset(gw, count == 1)\n\nFor a nice discussion on the semantics of further-farther, see this Merriam-Webster post. The standard semantic distinction drawn between the two forms is physical versus metaphorical distance.\nSome highlighting & sample data below.\n\nfu <- '\\\\1 <span style=\"background-color:lightgreen\">\\\\2</span> \\\\3'\nfa <- '\\\\1 <span style=\"background-color:lightblue\">\\\\2</span> \\\\3'\n\ngw0$text <- gsub('(^.+)(further)(.+$)', fu, gw0$text, ignore.case = T)\ngw0$text <- gsub('(^.+)(farther)(.+$)', fa, gw0$text, ignore.case = T)\ngw0$text <- paste0('... ', gw0$text, ' ...')\n\nset.seed(99)\ngw0 %>% select(year, genre, text) %>% sample_n(10) %>% \n  DT::datatable(rownames = F, escape = F,\n                options = list(dom = 't',\n                               pageLength = 10,\n                               scrollX = TRUE))\n\n\n\n\n\n\nLastly, we identify the location (ie, context position) of the target token within each context (as token index).\n\ngw0$idx <- sapply(gsub(' (farther|further).*$', '', gw0$sent, ignore.case = T), \n                  function(x){\n                    length(corpus::text_tokens(x)[[1]]) })"
  },
  {
    "objectID": "posts/bert/index.html#bert-contextual-embeddings",
    "href": "posts/bert/index.html#bert-contextual-embeddings",
    "title": "BERT, reticulate & lexical semantics",
    "section": "3 BERT & contextual embeddings",
    "text": "3 BERT & contextual embeddings\nUsing BERT and spacy for computing contextual word embeddings is actually fairly straightforward. A very nice resource for some theoretical overview as well as code demo with BERT/spacy is available here.\nGetting started, we pass our data set from R to Python via the r_to_py function.\n\ndf <- reticulate::r_to_py(gw0)\n\nThen, from a Python console, we load the BERT transformer using spacy.\n\nimport spacy\nnlp = spacy.load('en_core_web_trf')\n\nThe stretch of Python code below does all the work here. The transformer computes a 768 dimension vector per token/sub-token comprising each context – then we extract the tensor for either further/farther using the token index. The resulting data structure is matrix-like, with each instantiation of further-farther represented in 768 dimensions.\n\ndef encode(sent, index):\n  doc = nlp(sent.lower())\n  tensor_ix = doc._.trf_data.align[index].data.flatten()\n  out_dim = doc._.trf_data.tensors[0].shape[-1]\n  tensor = doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n  ## tensor.__len__()\n  return tensor.mean(axis=0)\n\nr.df[\"emb\"] = r.df[[\"sent\", \"idx\"]].apply(lambda x: encode(x[0], x[1]), axis = 1)"
  },
  {
    "objectID": "posts/bert/index.html#tsne",
    "href": "posts/bert/index.html#tsne",
    "title": "BERT, reticulate & lexical semantics",
    "section": "4 tSNE",
    "text": "4 tSNE\nTo plot these contexts in two dimensions, we use tSNE to reduce the 768-dimension word embeddings to two. Via Python and numpy, we create a matrix-proper from the further-farther token embeddings extracted above.\n\nimport numpy as np\nX, y  = r.df[\"emb\"].values, r.df[\"id\"].values\nX = np.vstack(X)\n\nFor good measure, we switch back to R to run tSNE. The matrix X, built in Python, is accessed in the R console below via reticulate::py$X.\n\nset.seed(999) ## \ntsne <- Rtsne::Rtsne(X = as.matrix(reticulate::py$X), \n                     check_duplicates = FALSE)\n\ntsne_clean <- data.frame(reticulate::py_to_r(df), tsne$Y) %>%\n  \n  mutate(t1 = gsub('(further|farther)', '\\\\<\\\\1\\\\>', text, ignore.case = T),\n         t2 = stringr::str_wrap(string = t1,\n                                  width = 20,\n                                  indent = 1,\n                                  exdent = 1),\n         id = row_number()) %>%\n  select(id, form, X1, X2, t1, t2) \n\nThe scatter plot below summarizes contextual embeddings for individual tokens of further-farther. So, a nice space for further used adjectivally on the right side of the plot. Other spaces less obviously structured, and some confused spaces as well where speakers seem to have quite a bit of leeway.\n\np <- ggplot2::ggplot(tsne_clean, \n                          aes(x = X1, \n                              y = X2,\n                              color = form,\n                              text = t2,\n                              key = id )) + \n  \n  geom_hline(yintercept = 0, color = 'gray') +\n  geom_vline(xintercept = 0, color = 'gray') +\n  \n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  ggthemes::scale_colour_economist() +\n  ggtitle('further-farther') \n\nplotly::ggplotly(p,  tooltip = 'text') \n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nPlease use `gather()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated."
  },
  {
    "objectID": "posts/bert/index.html#summary",
    "href": "posts/bert/index.html#summary",
    "title": "BERT, reticulate & lexical semantics",
    "section": "5 Summary",
    "text": "5 Summary\nSo, some notes on reticulate and Python environments, and spacy and BERT. While a computational beast, BERT seems fantastically suited for more fine-grained, qualitative semantic analyses and case studies, and lexicography in general."
  },
  {
    "objectID": "posts/seven-generations/index.html",
    "href": "posts/seven-generations/index.html",
    "title": "the seven living generations in america",
    "section": "",
    "text": "A quick look at the composition of American generations. Per Pew Research definitions & US Census data."
  },
  {
    "objectID": "posts/seven-generations/index.html#monthly-us-population-estimates",
    "href": "posts/seven-generations/index.html#monthly-us-population-estimates",
    "title": "the seven living generations in america",
    "section": "2 Monthly US population estimates",
    "text": "2 Monthly US population estimates\nMonthly Postcensal Resident Population plus Armed Forces Overseas, December 2018. Made available by the US Census here. The census has transitioned to a new online interface, and (seemingly) many data sets have been discontinued. Hence, the data set utilized here is slightly dated.\n\npops <- read.csv (\n  url('https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/national/asrh/nc-est2018-alldata-p-File18.csv')) %>%\n  filter(MONTH == '12' & YEAR == '2018') %>%\n  gather(key = 'race', value = 'pop', -UNIVERSE:-AGE)\n\nA more detailed description of the population estimates can be found here. Note: Race categories reflect non-Hispanic populations.\n\nrace <- c('NHWA', 'NHBA', 'NHIA', \n          'NHAA', 'NHNA', 'NHTOM', 'H')\n\nrace1 <- c('White Alone',\n           'Black Alone',\n           'American Indian Alone',\n           'Asian Alone',\n           'Native Hawaiian Alone',\n           'Two or More Races',\n           'Hispanic')\n\nlabels <- data.frame(race = race, \n                     race1=race1, \n                     stringsAsFactors = FALSE)\n\nsearch <- paste(paste0('^',race, '_'), collapse =  '|')\n\nThe following table details a random sample of the data set – with Pew Research defined generations & estimated year-of-birth.\n\ngen_pops <- pops %>%\n  filter(grepl(search, race)) %>%\n  mutate(race = gsub('_.*$', '', race)) %>%\n  group_by(AGE, race) %>%\n  summarise(pop = sum(pop))%>%\n  left_join(labels) %>%\n  filter(AGE != '999') %>%\n  mutate(yob = 2019 - AGE)  %>% ##\n  mutate (gen = case_when (\n    yob < 2013 & yob > 1996 ~ 'Gen Z',\n    yob < 1997 & yob > 1980 ~ 'Millennial',\n    yob < 1981 & yob > 1964 ~ 'Gen X',\n    yob < 1965 & yob > 1945 ~ 'Boomers',\n    yob < 1946 & yob > 1927 ~ 'Silent',\n    yob < 1928 ~ 'Greatest',\n    yob > 2012 ~ 'Post-Z')) %>%\n  left_join(gen_desc) %>%\n  ungroup() %>%\n  select(gen, rank, range, race, \n         race1, yob, AGE, pop)\n\nset.seed(999)\ngen_pops %>% sample_n(7)  %>%\n  select(gen, range, race1:pop) %>%\n  DT::datatable(rownames = FALSE, options = list(dom = 't',\n                                                 scrollX = TRUE))"
  },
  {
    "objectID": "posts/seven-generations/index.html#composition-of-american-generations",
    "href": "posts/seven-generations/index.html#composition-of-american-generations",
    "title": "the seven living generations in america",
    "section": "3 Composition of American generations",
    "text": "3 Composition of American generations\n\n3.1 Population by generation\nThe figure below summarizes the US population by generation. These numbers will vary some depending on the data source. Millenials constitute the plurality of Americans, more recently overtaking a Boomer generation on the wane.\n\ngen_pops %>%\n  group_by(gen, rank) %>%\n  summarize(pop = sum(pop)) %>%\n  mutate(lab = round(pop/1000000, 1)) %>%\n  ggplot(aes(x = reorder(gen, rank), \n             y = pop, \n             fill = gen)) +\n  geom_col(show.legend = FALSE, \n           alpha = 0.75)  +\n  geom_text(aes(label = lab), \n            size = 3.5)+\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  xlab('') + ylab('') +\n  coord_flip()+\n  ggthemes::scale_fill_stata() +\n  theme_minimal() +\n  labs(title = 'Population by American generation',\n       caption = 'SOURCE: US Census, Monthly Postcensal Resident Population plus Armed Forces Overseas, December 2018.')\n\n`summarise()` has grouped output by 'gen'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n3.2 Population by single year of age & generation\n\ngg <- gen_pops %>% \n  group_by(yob, AGE, gen) %>%\n  summarize(tot = sum(pop)) %>%\n  group_by(gen) %>%\n  mutate(tot = max(tot)) %>% #For labels below.\n  filter(yob %in% c('1919', '1928', '1946', '1965', \n                    '1981', '1997', '2013'))\n\n`summarise()` has grouped output by 'yob', 'AGE'. You can override using the\n`.groups` argument.\n\n\nThe figure below illustrates the US population by single year of age, ranging from the population aged less than a year to the population over 100 (as of December 2018). Generation membership per single year of age is specified by color.\n\ngen_pops %>%\n  ggplot(aes(x = AGE, \n             y = pop, \n             fill = gen)) +\n  geom_vline(xintercept = gg$AGE,\n             linetype =2, \n             color = 'gray', \n             size = .25)+\n  \n  geom_col(show.legend = FALSE, \n           alpha = 0.85,\n           width = .7)   +\n  annotate(geom=\"text\", \n           x = gg$AGE - 4.5, \n           y = gg$tot + 70000, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('Age')+ \n  ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank(),\n        panel.grid.minor.y=element_blank()) +\n  ggthemes::scale_fill_stata()+\n  scale_x_reverse(breaks = rev(gg$AGE)) +\n  labs(title = 'American population by single-year age & generation')\n\n\n\n\n\n\n3.3 Population by single year of age, race & generation\n\ngen_pal <- c('#b0bcc1','#ead8c3', '#437193', \n             '#c66767', '#55752f', '#dae2ba', \n             '#7b9bb3')\n\nNext, we crosscut the single year of age counts presented above by race & ethnicity.\n\ngen_pops %>%\n  ggplot(aes(x = AGE, \n             y = pop, \n             fill = race1)) +\n  geom_area(stat = \"identity\",\n            color = 'white',\n            alpha = 0.85) +\n  scale_fill_manual(values = gen_pal) +\n  geom_vline(xintercept = gg$AGE,\n             linetype =2, color = 'gray', size = .25)+\n  annotate(geom=\"text\", \n           x = gg$AGE - 4.5, \n           y = gg$tot + 70000, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('')+ ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank(),\n        panel.grid.minor.y=element_blank()) +\n  \n  scale_x_reverse(breaks = rev(gg$AGE) )+\n  labs(title ='American population by age, race & generation')\n\n\n\n\n\n\n3.4 White America on the wane\n\nwhite_label <- gen_pops %>% \n  group_by(gen, AGE) %>%\n  mutate(per = pop/sum(pop))%>%\n  filter(race1 == 'White Alone') %>%\n  group_by(gen) %>%\n  mutate(per = max(per)) %>% #For labels below.\n  arrange(yob) %>%\n  filter(yob %in% c('1919', '1928', '1946', '1965', \n                    '1981', '1997', '2013'))\n\nThe last figure illustrates a proportional perspective of race & ethnicity in America by single year of age. Per figure, generational differences (at a single point in time) can shed light on (the direction of) potential changes in the overall composition of a given populace. As well as a view of what that populace may have looked like in the past.\n\ngen_pops %>%\n  group_by(gen, AGE) %>%\n  mutate(per = pop/sum(pop)) %>%\n  ggplot(aes(x = (AGE), \n             y = per, \n             fill = race1)) +\n  geom_area(stat = \"identity\",\n            color = 'white',\n            alpha = 0.85) +\n  geom_hline(yintercept = .5, \n             linetype = 4,\n             color = 'white') +\n  scale_fill_manual(values = gen_pal) +\n  geom_vline(xintercept = gg$AGE,\n             linetype = 2, \n             color = 'gray', \n             size = .25)+\n  annotate(geom=\"text\", \n           x = gg$AGE-4.5, \n           y = white_label$per - .05, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('')+ ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank()) +\n  \n  scale_x_reverse(breaks = rev(gg$AGE)) +\n  labs(title = 'American population by age, race & generation')"
  },
  {
    "objectID": "posts/seven-generations/index.html#summary",
    "href": "posts/seven-generations/index.html#summary",
    "title": "the seven living generations in america",
    "section": "4 Summary",
    "text": "4 Summary\nSome different perspectives on the composition of America & American generations."
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html",
    "href": "posts/mapping-roll-calls/index.html",
    "title": "mapping congressional roll calls",
    "section": "",
    "text": "Methods for mapping with R & ggplot, in the context of visualizing historical roll calls from the US House of Representatives. Roll call data accessed via VoteView and the RVoteview package (Jeffrey et al. 2020); shapefiles for historical US Congressional Districts downloaded from the Political Science Dept @ UCLA (Lewis et al. 2013). Visual summary via the patchwork package."
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#historical-urban-centers",
    "href": "posts/mapping-roll-calls/index.html#historical-urban-centers",
    "title": "mapping congressional roll calls",
    "section": "1 Historical urban centers",
    "text": "1 Historical urban centers\nMost populous US cities by decade, from 1790 to 2010; scraped from Wikipedia. For zooming-in on district roll call results for, eg, the ten most populous cities during a given congress.\n\nwiki <- 'https://en.wikipedia.org/wiki/List_of_most_populous_cities_in_the_United_States_by_decade'\n\ndecade <- seq(from = 1780, to = 2010, by = 10)\npops_list <- xml2::read_html(wiki) %>% \n  rvest::html_nodes(\"table\") %>%\n  rvest::html_table(fill = TRUE)\n\npops <- lapply(2:24, function(x) {\n  y <- pops_list[[x]] %>%\n    select(1:4) %>%\n    mutate(decade = decade[x])\n  \n  colnames(y) <- c('rank', 'city', 'state', 'pop', 'decade')\n  return(y) }) %>%\n  bind_rows() %>%\n  mutate(pop = as.integer(gsub(\"[^0-9]\", \"\", pop)))\n\nMost populated US cities circa 1800:\n\n\n\n\n\nrank\ncity\nstate\npop\ndecade\n\n\n\n\n1\nNew York\nNew York\n60514\n1800\n\n\n2\nPhiladelphia\nPennsylvania\n41220\n1800\n\n\n3\nBaltimore\nMaryland\n26514\n1800\n\n\n4\nBoston\nMassachusetts\n24937\n1800\n\n\n5\nCharleston\nSouth Carolina\n18824\n1800\n\n\n6\nNorthern Liberties\nPennsylvania\n10718\n1800\n\n\n7\nSouthwark\nPennsylvania\n9621\n1800\n\n\n8\nSalem\nMassachusetts\n9457\n1800\n\n\n9\nProvidence\nRhode Island\n7614\n1800\n\n\n10\nNorfolk\nVirginia\n6926\n1800"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#historical-congressional-districts",
    "href": "posts/mapping-roll-calls/index.html#historical-congressional-districts",
    "title": "mapping congressional roll calls",
    "section": "2 Historical congressional districts",
    "text": "2 Historical congressional districts\nAgain, via the folks at the Political Science Dept @ UCLA. The Voting Rights Act of 1965 was passed during the 89th congress; a local copy of the shapefile for this congress is loaded below.\n\nfname <- 'districts089'\n\ncd_sf <- sf::st_read(dsn = paste0(cd_directory, fname), \n                    layer = fname, \n                    quiet = TRUE) %>%\n  mutate(STATEFP = substr(ID, 2, 3),\n         district_code = as.numeric(substr(ID, 11, 12))) %>%\n  left_join(states, by = \"STATEFP\") %>%\n  filter(!STATEFP %in% nonx) %>%\n  select(STATEFP, state_abbrev, district_code)"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#voteview-roll-call-data",
    "href": "posts/mapping-roll-calls/index.html#voteview-roll-call-data",
    "title": "mapping congressional roll calls",
    "section": "3 VoteView roll call data",
    "text": "3 VoteView roll call data\nDownloading roll call data for a specific bill via RVoteview requires a bit of trial and error; different bill versions and vote types complicate things for the layman.\n\nvra <- Rvoteview::voteview_search('(\"VOTING RIGHTS ACT OF 1965\") AND (congress:89) \n                                  AND (chamber:house)') %>%\n                                  filter( date == '1965-07-09') %>%\n  janitor::clean_names()\n\nvotes <- Rvoteview::voteview_download(vra$id)\nnames(votes) <- gsub('\\\\.', '_', names(votes))\n\nA quick re-structure of the roll call output:\n\nbig_votes <- votes$legis_long_dynamic %>%\n  left_join(votes$votes_long, by = c(\"id\", \"icpsr\")) %>%\n  filter(!grepl('POTUS', cqlabel)) %>%\n  group_by(state_abbrev) %>%\n  mutate(n = length(district_code)) %>%\n  ungroup() %>%\n  mutate(avote = case_when(vote %in% c(1:3) ~ 'Yea',\n                           vote %in% c(4:6) ~ 'Nay',\n                           vote %in% c(7:9) ~ 'Not Voting'),\n         \n         party_code = case_when(party_code == 100 ~ 'Dem',\n                                party_code == 200 ~ 'Rep' ), \n         Party_Member_Vote = paste0(party_code, ': ', avote),\n         \n         ## fix at-large -- \n         district_code = ifelse(district_code %in% c(98, 99), 0, district_code),\n         district_code = ifelse(n == 1 & district_code == 1, 0, district_code),\n         district_code = as.integer(district_code)) %>%\n  select(-n)\n#Members who represent historical “at-large” districts are \n##assigned 99, 98, or 1 in various circumstances. Per VoteView."
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#roll-call-stats",
    "href": "posts/mapping-roll-calls/index.html#roll-call-stats",
    "title": "mapping congressional roll calls",
    "section": "4 Roll call stats",
    "text": "4 Roll call stats\n\nbig_votes$Party_Member_Vote <- factor(big_votes$Party_Member_Vote)\nbig_votes$Party_Member_Vote <- \n  factor(big_votes$Party_Member_Vote, \n         levels(big_votes$Party_Member_Vote)[c(3,6,1,4,2,5)])\n\n\n4.1 Results\n\nsummary <- big_votes %>%\n  group_by(party_code, avote) %>%\n  count() %>%\n  spread(avote, n) %>%\n  janitor::adorn_totals(where = c('row', 'col')) %>%\n  rename(Party = party_code,\n         NV = `Not Voting`) %>%\n  select(Party, Yea, Nay, NV, Total)\n\n\n\n\nRoll call results for the VRA\n\n\nParty\nYea\nNay\nNV\nTotal\n\n\n\n\nDem\n224\n65\n4\n293\n\n\nRep\n112\n23\n5\n140\n\n\nTotal\n336\n88\n9\n433\n\n\n\n\n\n\n\n4.2 By party affiliation\n\nroll <- big_votes %>% \n  group_by(Party_Member_Vote) %>%\n  count() %>%\n  ungroup() %>%\n  rename(Vote = Party_Member_Vote) \n\nrsum <- roll %>% \n  ggplot(aes(x=Vote, y=n, fill= Vote, label = n)) +\n    geom_col(width=.65, color = 'lightgray') +  \n    geom_text(size = 2.5) +\n    wnomadds::scale_color_rollcall(aesthetics = c(\"fill\")) +\n    scale_x_discrete(limits = rev(levels(roll$Vote)))+\n    coord_flip() +\n  ylim (0, 240) +\n    theme_minimal() + \n      theme(axis.title.x=element_blank(),\n            axis.text.x=element_blank(),\n            axis.title.y=element_blank(),\n            #axis.text.y=element_blank(),\n            legend.position = 'none')\n\nrsum + ggtitle(vra$short_description)\n\n\n\n\n\n\n4.3 By congressional district\n\ncd_sf_w_rolls <- cd_sf %>% \n  left_join(big_votes, by = c(\"state_abbrev\", \"district_code\")) \n\nmain1 <- cd_sf_w_rolls %>%\n  ggplot() + \n  geom_sf(aes(fill = Party_Member_Vote), \n          color = 'white',\n          size = .25) + \n  \n  wnomadds::scale_fill_rollcall() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        legend.position = 'none') # +\n\nmain1 + ggtitle(vra$short_description)"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#zooming-in-to-urban-centers",
    "href": "posts/mapping-roll-calls/index.html#zooming-in-to-urban-centers",
    "title": "mapping congressional roll calls",
    "section": "5 Zooming in to urban centers",
    "text": "5 Zooming in to urban centers\nA zoom function for closer inspection of roll call results in urban areas. The sub_geo parameter is used to specify a vector of city/state pairs (eg, “Chicago, Illinois”) to be geocoded via the tmaptools::geocode_OSM function. The geo parameter specifies the full map – as sf object.\n\nmaps_get_minis <- function(sub_geos, geo){\n                           \n  lapply(sub_geos, function(x) {\n    \n    lc <- tmaptools::geocode_OSM (q = x, as.sf = T)\n    lc$bbox <- sf::st_set_crs(lc$bbox, sf::st_crs(geo))\n    cropped <- sf::st_crop(geo, lc$bbox)\n    \n    ggplot() + geom_sf(data = cropped, \n                     aes(fill = Party_Member_Vote),\n                     color = 'white', size = .25) +\n      \n      # ggsflabel::geom_sf_text_repel(data = cropped, \n      #                               aes(label = district_code), \n      #                               size = 2.2) + \n      \n      wnomadds::scale_fill_rollcall() +\n      theme_minimal() + \n      theme(axis.title.x=element_blank(),\n            axis.text.x=element_blank(),\n            axis.title.y=element_blank(),\n            axis.text.y=element_blank(),\n            plot.title = element_text(size=9),\n            legend.position = 'none') +\n      ggtitle(gsub(',.*$', '', x))   })\n}\n\n\n5.1 Coordinates\n\n# x <- 'Albuquerque, New Mexico'\npops1 <- pops %>%\n  filter(decade == paste0(gsub('.-.*$', '', vra$date), 0)) %>%\n  arrange(desc(pop)) %>%\n  mutate(locations = paste0(city, ', ', state)) %>%\n  slice(1:10)\n\nsub_maps <- maps_get_minis(geo = cd_sf_w_rolls, sub_geos = pops1$locations)\n\n\n\n5.2 Zooms\nRoll call results for the VRA (1965) – zoomed in to the ten most populous US cities during the 1960s.\n\npatchwork::wrap_plots(sub_maps, nrow = 2)"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#a-patchwork-perspective",
    "href": "posts/mapping-roll-calls/index.html#a-patchwork-perspective",
    "title": "mapping congressional roll calls",
    "section": "6 A patchwork perspective",
    "text": "6 A patchwork perspective\n\nt2 <- gridExtra::tableGrob(summary, \n                           rows = NULL, \n                           theme = gridExtra::ttheme_minimal(base_size = 8)) \n\np0 <- sub_maps[[1]] + sub_maps[[2]] + sub_maps[[3]] +\n  rsum + patchwork::plot_layout(nrow = 1, widths = c(1,1,1,1))\n\np1 <- sub_maps[[4]] + sub_maps[[5]] + sub_maps[[6]] +\n  t2 + patchwork::plot_layout(nrow = 1, widths = c(1,1,1,1))\n\np2 <- p0/p1 + patchwork::plot_layout(nrow = 2)#, heights = c(4, 1))\n\n\nmain1 / p2  + patchwork::plot_layout(ncol = 1, heights = c(5, 4)) +\n  plot_annotation(\n    title = vra$short_description, \n    subtitle = paste0('Congress ', vra$congress, ' | ',\n                             vra$date, ' | ', vra$bill_number, ' | ',\n                             'Support: ', round(vra$support, 1), '%'),\n     caption = 'Sources: VoteView | Polisci @ UCLA')"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#on-the-popular-vote",
    "href": "posts/mapping-roll-calls/index.html#on-the-popular-vote",
    "title": "mapping congressional roll calls",
    "section": "7 On the popular vote",
    "text": "7 On the popular vote\nPer code above, we can create a simple & reproducible work-flow for quickly exploring historical roll calls in the US Congress. For the Bayh–Celler amendment (circa 1969), then, we (down-) load the congressional district shapefile for the 91st congress from UCLA, and re-query RVoteview."
  },
  {
    "objectID": "posts/seven-generations/index.html#american-generations",
    "href": "posts/seven-generations/index.html#american-generations",
    "title": "the seven living generations in america",
    "section": "1 American generations",
    "text": "1 American generations\n\nlibrary(tidyverse)\ngen <- c('Post-Z', 'Gen Z', 'Millennial', \n         'Gen X', 'Boomers', 'Silent', \n         'Greatest')\n\nrange <- c('> 2012', '1997-2012', '1981-1996', \n           '1965-1980', '1946-1964', '1928-1945', \n           '< 1927')\n\ngen_desc <- data.frame(rank = 7:1,\n                       gen = gen,\n                       range = range,\n                       stringsAsFactors = FALSE) %>%\n  arrange(rank)\n\nA summary:\n\n\n\n\n\n\n\nFour of America’s seven living generations are more or less “complete,” and only getting smaller (albeit at different rates): Greatest, Silent, Boomers, and Gen X. The generation comprised of Millenials is complete as well, in that it has been delineated chronologically; however, the group likely continues to grow via immigration.\nWhile Gen Z has been tentatively stamped chronologically by the folks at Pew Research, only the very eldest in the group have just entered the work force. So lots can happen still. And although we include them here, the Post-Z generation is mostly but a thought; half of the group has yet to be born."
  },
  {
    "objectID": "posts/entity-linking/index.html",
    "href": "posts/entity-linking/index.html",
    "title": "linking entities and wikidata",
    "section": "",
    "text": "A quick demo using the spacyfishing library – “a spaCy wrapper for Entity-Fishing, a tool for named entity recognition, linking and disambiguation against Wikidata.” Facilitates disambiguating/linking named entities to the Wikidata knowledge base."
  },
  {
    "objectID": "posts/incivility/index.html",
    "href": "posts/incivility/index.html",
    "title": "incivility on twitter & political ideology",
    "section": "",
    "text": "{.preview-image, width=100%}"
  },
  {
    "objectID": "posts/incivility/index.html#incivility-and-ideological-extremism-per-dw-nominate",
    "href": "posts/incivility/index.html#incivility-and-ideological-extremism-per-dw-nominate",
    "title": "incivilty on twitter & politial ideology",
    "section": "1 incivility and ideological extremism – per dw-nominate –",
    "text": "1 incivility and ideological extremism – per dw-nominate –\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nleg_dets <- 'https://theunitedstates.io/congress-legislators/legislators-current.csv'\nreps <- read.csv((url(leg_dets)), stringsAsFactors = FALSE) |>\n  dplyr::filter(type == 'rep')\n\n\ntweet_set <- lapply(paste0('@', reps$twitter),\n                    function(x) {\n                      ts <- rtweet::search_tweets(q = x,\n                                                  ## n = 200,\n                                                  type = \"recent\",\n                                                  include_rts = FALSE,\n                                                  geocode = NULL,\n                                                  max_id = NULL,\n                                                  parse = TRUE,\n                                                  token = tk,\n                                                  retryonratelimit = TRUE)\n                      \n                      if(nrow(ts) == 0) {return(NA)} else{\n                        \n                        ts |> dplyr::select(1:5)\n                      }\n                      }\n                    )\n\nnames(tweet_set) <- reps$bioguide_id\n\n#then filter -- \ntweet_set0 <- Filter(length, tweet_set)\ntweet_set1 <- Filter(function(a) any(!is.na(a)), tweet_set0)\ntweet_set2 <- tweet_set1 |> data.table::rbindlist(idcol = 'bioguide_id') \n\n\ntweet_set2$incivil <- predict_incivility(tweet_set2$text, \n                                         old_dfm = dfm,\n                                         classifier = lasso)\n\n\ntweet_set3 <- tweet_set2 |> \n  mutate(ats = stringr::str_count(text, pattern = '\\\\@')) |>\n  filter(ats == 1)\n\n\ntsum <- tweet_set3 |> \n  mutate(is_uncivil = ifelse(incivil > 0.5, 1, 0)) |>\n  group_by(bioguide_id) |>\n  summarise(mean_score = mean(incivil),\n            mentions = n(),\n            uncivil_mentions = sum(is_uncivil)) |>\n  dplyr::ungroup() |>\n  mutate(per_uncivil = round(uncivil_mentions/mentions*100, 1)) |>\n  merge(reps)\n\n\nhouse_members <- Rvoteview::download_metadata(type = 'members',\n                                              chamber = 'house',\n                                              congress = '117') \n\n[1] \"/tmp/RtmpCHCn89/H117_members.csv\"\n\n\n\nset.seed(99)\ntweet_set3 |>\n  inner_join(reps) |>\n  filter(incivil > .75) |>\n  sample_n(10) %>%\n  select(full_name, state, party, text) |>\n  DT::datatable(rownames = F)\n\n\n\n\n\n\n\nlibrary(ggplot2)\ntsum |>\n  merge(house_members) |>\n  filter(mean_score < 0.5) |>\n  ggplot(aes(nominate_dim1, \n             mean_score, #per_uncivil, \n             color = as.factor(party),\n             label = last_name))+ \n  geom_point()+ #\n  geom_smooth(method=\"lm\", se=T) +\n  \n  ggrepel::geom_text_repel(\n    nudge_y = -0.0025,\n    segment.color = \"grey50\",\n    direction = \"y\",\n    hjust = 0, \n    size = 2.5 ) +\n  \n  ggthemes::scale_color_stata() +\n  theme(legend.position = \"none\", \n        plot.title = element_text(size=12),\n        axis.title = element_text())+\n  xlab('DW-Nominate D1') + ylab('Mean Incivility Score') +\n  labs(title=\"DW-Nominate scores & Mean Incivility Score\")"
  },
  {
    "objectID": "posts/incivility/index.html#classifier",
    "href": "posts/incivility/index.html#classifier",
    "title": "incivility on twitter & political ideology",
    "section": "1 Classifier",
    "text": "1 Classifier\nThe authors make their “incivility classifier” available here.\n\nsetwd(locald)\nlibrary(quanteda)\nlibrary(glmnet)\nsource(\"incivility-sage-open-master/functions.R\")\nload(\"incivility-sage-open-master/data/lasso-classifier.rdata\")\nload(\"incivility-sage-open-master/data/dfm-file.rdata\")"
  },
  {
    "objectID": "posts/incivility/index.html#house-members-details-ideology-scores",
    "href": "posts/incivility/index.html#house-members-details-ideology-scores",
    "title": "incivility on twitter & political ideology",
    "section": "2 House members: details & ideology scores",
    "text": "2 House members: details & ideology scores\nThe unitedstates project provides demographic info and social details for members of congress.\n\nlibrary(dplyr)\nleg_dets <- 'https://theunitedstates.io/congress-legislators/legislators-current.csv'\nreps <- read.csv((url(leg_dets)), stringsAsFactors = FALSE) |>\n  dplyr::filter(type == 'rep')\n\nThe package Rvoteview provides access to the Voteview database and DW-Nominate ideology scores for US lawmakers.\n\nhouse_members <- Rvoteview::download_metadata(type = 'members',\n                                              chamber = 'house',\n                                              congress = '117') \n\n[1] \"/tmp/RtmpIn1JfV/H117_members.csv\""
  },
  {
    "objectID": "posts/incivility/index.html#twitter-mentions",
    "href": "posts/incivility/index.html#twitter-mentions",
    "title": "incivilty on twitter & politial ideology",
    "section": "3 Twitter mentions",
    "text": "3 Twitter mentions\n\n3.1 Collect tweets\n\ntweet_set <- lapply(paste0('@', reps$twitter),\n                    function(x) {\n                      ts <- rtweet::search_tweets(q = x,\n                                                  ## n = 200,\n                                                  type = \"recent\",\n                                                  include_rts = FALSE,\n                                                  geocode = NULL,\n                                                  max_id = NULL,\n                                                  parse = TRUE,\n                                                  token = tk,\n                                                  retryonratelimit = TRUE)\n                      \n                      if(nrow(ts) == 0) {return(NA)} else{\n                        \n                        ts |> dplyr::select(1:5)\n                      }\n                      }\n                    )\n\n\nnames(tweet_set) <- reps$bioguide_id\ntweet_set0 <- Filter(length, tweet_set)\ntweet_set1 <- Filter(function(a) any(!is.na(a)), tweet_set0)\ntweet_set2 <- tweet_set1 |> data.table::rbindlist(idcol = 'bioguide_id') \n\n\n\n3.2 Classify civility\n\ntweet_set2$incivil <- predict_incivility(tweet_set2$text, \n                                         old_dfm = dfm,\n                                         classifier = lasso)\n\n\n\n3.3 Filter tweets\n\ntweet_set3 <- tweet_set2 |> \n  mutate(ats = stringr::str_count(text, pattern = '\\\\@')) |>\n  filter(ats == 1)\n\n\ntsum <- tweet_set3 |> \n  mutate(is_uncivil = ifelse(incivil > 0.5, 1, 0)) |>\n  group_by(bioguide_id) |>\n  summarise(mean_score = mean(incivil),\n            mentions = n(),\n            uncivil_mentions = sum(is_uncivil)) |>\n  dplyr::ungroup() |>\n  mutate(per_uncivil = round(uncivil_mentions/mentions*100, 1)) |>\n  merge(reps)\n\n\n\n3.4 Sample tweets\n\nset.seed(99)\ntweet_set3 |>\n  inner_join(reps) |>\n  filter(incivil > .75) |>\n  sample_n(10) %>%\n  select(full_name, state, party, text) |>\n  DT::datatable(rownames = F)"
  },
  {
    "objectID": "posts/incivility/index.html#filter-tweets",
    "href": "posts/incivility/index.html#filter-tweets",
    "title": "incivilty on twitter & politial ideology",
    "section": "4 Filter tweets",
    "text": "4 Filter tweets\n\ntweet_set3 <- tweet_set2 |> \n  mutate(ats = stringr::str_count(text, pattern = '\\\\@')) |>\n  filter(ats == 1)\n\n\ntsum <- tweet_set3 |> \n  mutate(is_uncivil = ifelse(incivil > 0.5, 1, 0)) |>\n  group_by(bioguide_id) |>\n  summarise(mean_score = mean(incivil),\n            mentions = n(),\n            uncivil_mentions = sum(is_uncivil)) |>\n  dplyr::ungroup() |>\n  mutate(per_uncivil = round(uncivil_mentions/mentions*100, 1)) |>\n  merge(reps)\n\n\n4.1 Sample tweets\n\nset.seed(99)\ntweet_set3 |>\n  inner_join(reps) |>\n  filter(incivil > .75) |>\n  sample_n(10) %>%\n  select(full_name, state, party, text) |>\n  DT::datatable(rownames = F)"
  },
  {
    "objectID": "posts/incivility/index.html#incivility-ideology",
    "href": "posts/incivility/index.html#incivility-ideology",
    "title": "incivility on twitter & political ideology",
    "section": "4 Incivility & ideology",
    "text": "4 Incivility & ideology\nPer plot below, then, tweeters tend to be less civil when mentioning (or addressing) more ideologically extreme members of Congress in their tweets.\n\nlibrary(ggplot2)\ntsum |>\n  merge(house_members) |>\n  filter(mean_score < 0.5) |>\n  ggplot(aes(nominate_dim1, \n             mean_score, #per_uncivil, \n             color = as.factor(party),\n             label = last_name))+ \n  geom_point()+ #\n  geom_smooth(method=\"lm\", se=T) +\n  \n  ggrepel::geom_text_repel(\n    nudge_y = -0.0025,\n    segment.color = \"grey50\",\n    direction = \"y\",\n    hjust = 0, \n    size = 2.5 ) +\n  \n  ggthemes::scale_color_stata() +\n  theme(legend.position = \"none\", \n        plot.title = element_text(size=12),\n        axis.title = element_text())+\n  xlab('DW-Nominate D1') + ylab('Mean Incivility') +\n  labs(title=\"DW-Nominate scores & Mean Incivility scores\")"
  },
  {
    "objectID": "posts/incivility/index.html#twitter-data",
    "href": "posts/incivility/index.html#twitter-data",
    "title": "incivility on twitter & political ideology",
    "section": "3 Twitter data",
    "text": "3 Twitter data\n\n3.1 Collect tweets\nWe use the rtweet package to gather the most recent tweets mentioning each member of the US House of Representatives. Here, “mention” is in the Twitter sense, ie, a username prefixed with the @ symbol.\n\ntweet_set <- lapply(paste0('@', reps$twitter),\n                    function(x) {\n                      ts <- rtweet::search_tweets(\n                        q = x,\n                        ## n = 200,\n                        type = \"recent\",\n                        include_rts = FALSE,\n                        geocode = NULL,\n                        max_id = NULL,\n                        parse = TRUE,\n                        token = tk,\n                        retryonratelimit = TRUE)\n                      \n                      if(nrow(ts) == 0) {return(NA)} else{\n                        \n                        ts |> dplyr::select(1:5)\n                      }\n                      }\n                    )\n\n\nnames(tweet_set) <- reps$bioguide_id\ntweet_set0 <- Filter(length, tweet_set)\ntweet_set1 <- Filter(function(a) any(!is.na(a)), tweet_set0)\ntweet_set2 <- tweet_set1 |> data.table::rbindlist(idcol = 'bioguide_id') \n\n\n\n3.2 Classify civility\n\ntweet_set2$incivil <- predict_incivility(tweet_set2$text, \n                                         old_dfm = dfm,\n                                         classifier = lasso)\n\n\n\n3.3 Filter tweets\nSubset tweet corpus to only those that mention a single US lawmaker.\n\ntweet_set3 <- tweet_set2 |> \n  mutate(ats = stringr::str_count(text, pattern = '\\\\@')) |>\n  filter(ats == 1)\n\nTheocharis et al. (2020) ’s classifier computes the probability that given tweet is uncivil. One approach is to treat that probability as an incivility score; another is to convert the probability into a categorical variable, +/- uncivil. Here, we implement the former.\n\ntsum <- tweet_set3 |> \n  mutate(is_uncivil = ifelse(incivil > 0.5, 1, 0)) |>\n  group_by(bioguide_id) |>\n  summarise(mean_score = mean(incivil),\n            mentions = n(),\n            uncivil_mentions = sum(is_uncivil)) |>\n  dplyr::ungroup() |>\n  mutate(per_uncivil = round(uncivil_mentions/mentions*100, 1)) |>\n  merge(reps)\n\n\n\n3.4 Sample tweets\n\nset.seed(99)\ntweet_set3 |>\n  inner_join(reps) |>\n  filter(incivil > .75) |>\n  sample_n(5) %>%\n  select(full_name, state, party, text) |>\n  DT::datatable(rownames = F)"
  },
  {
    "objectID": "posts/entity-linking/index.html#build-a-simple-news-corpus",
    "href": "posts/entity-linking/index.html#build-a-simple-news-corpus",
    "title": "linking entities and wikidata",
    "section": "2 Build a simple news corpus",
    "text": "2 Build a simple news corpus\n\nqn <- quicknews::qnews_build_rss('war in ukraine') |>\n  quicknews::qnews_strip_rss()\n\nqn[1:5,1:3] |> knitr::kable() \n\n\n\n\n\n\n\n\n\ndate\nsource\ntitle\n\n\n\n\n2022-06-28\nNPR\nRussia-Ukraine war: What happened today (June 28)\n\n\n2022-06-28\nThe Washington Post\nLatest Russia-Ukraine war news: Live updates\n\n\n2022-06-28\nVOA News\nLatest Developments in Ukraine: June 28\n\n\n2022-06-28\nThe New York Times\nThe West Seeks a More Effective Way to Tighten Sanctions on Russia\n\n\n2022-06-28\nCNN\nRussia’s war in Ukraine: Live updates\n\n\n\n\n\n\narts <- quicknews::qnews_extract_article(qn$link[1:3], cores = 3)\ntext_en <- arts$text[1]"
  },
  {
    "objectID": "posts/entity-linking/index.html#spacy-entity-fishing",
    "href": "posts/entity-linking/index.html#spacy-entity-fishing",
    "title": "linking entities and wikipedia",
    "section": "3 Spacy & entity fishing",
    "text": "3 Spacy & entity fishing\n\nimport spacy \nnlp = spacy.load(\"en_core_web_sm\")\n                            \nnlp.add_pipe(\"entityfishing\", config={\"extra_info\": True})\n\n<spacyfishing.entity_fishing_linker.EntityFishing object at 0x7f3783277070>\n\nnlp.add_pipe('sentencizer')\n\n<spacy.pipeline.sentencizer.Sentencizer object at 0x7f378348b240>\n\n\n\ndoc = nlp(r.text_en)\n\n\nimport pandas as pd\n# \n# d = []\n# for idno, sentence in enumerate(doc.sents):\n#     d.append({\"id\": idno, \"sentence\":str(sentence)})\n#     print 'Sentence {}:'.format(idno + 1), sentence \n#     \n# df = pd.DataFrame(d)\n\n\n#reticulate::py$df |> dplyr::slice(1:5) |> knitr::kable()\n\n\n3.1 Entities to Wikipedia\n\nentities = [(e.label_, \n             e.text, \n             e._.normal_term, \n             e._.kb_qid, \n             e._.url_wikidata, \n             e._.nerd_score,\n             e._.description) for e in doc.ents]\n             \ndf99 = pd.DataFrame(entities, \n                    columns=['type',\n                             'entity', \n                             'normed', \n                             'qid', \n                             'url', \n                             'score', \n                             'description'])\n\n\nreticulate::py$df99 |> \n  dplyr::select(-description) |> \n  DT::datatable(rownames = F)\n\n\n\n\n\n\n\n\n3.2 Wikipedia description\n\nstrwrap(reticulate::py$df99$description[[3]], width = 60)[1:10]\n\n [1] \"'''Ukraine''', [[Name of Ukraine#\\\"Ukraine\\\" versus \\\"the\"  \n [2] \"Ukraine\\\"|sometimes called '''the Ukraine''']], is a\"       \n [3] \"[[sovereign state]] in [[Eastern Europe]], [[State Border\"  \n [4] \"of Ukraine|bordered]] by [[Russia]] to the east and\"        \n [5] \"northeast, [[Belarus]] to the northwest, [[Poland]],\"       \n [6] \"[[Hungary]] and [[Slovakia]] to the west, [[Romania]], and\" \n [7] \"[[Moldova]] to the southwest, and the [[Black Sea]] and\"    \n [8] \"[[Sea of Azov]] to the south and southeast, respectively.\"  \n [9] \"Ukraine is currently in [[territorial dispute]] with Russia\"\n[10] \"over the [[Crimean Peninsula]] which [[Annexation of Crimea\""
  },
  {
    "objectID": "posts/entity-linking/index.html#displacy",
    "href": "posts/entity-linking/index.html#displacy",
    "title": "linking entities and wikidata",
    "section": "4 displaCy",
    "text": "4 displaCy\nfrom spacy import displacy\nss = list(doc.sents)\n\ndisplacy.render(ss[:4], style=\"ent\")\n’\n\nA photograph taken TuesdayDATEshows charred goods in a grocery store of the destroyed Amstor mallPERSONin KremenchukLOC, central UkraineGPE, one dayDATEafter it was hit by a RussianNORPmissile strike.\n\n\n\nThe death toll climbed to at least 20CARDINALafter MondayDATE's missile attack on a crowded mall in the central UkrainianGPEcity of KremenchukORG, which leaders at a Group of SevenORGmeeting called a \"war crime.\"\n\n\n\nOn TuesdayDATE, emergency responders ended a rescue search for survivors.\n\n\n\nRussiaGPE's government denied hitting the shopping center, claiming it caught fire after RussiaGPEstruck a nearby weapons depot.\n\n’"
  },
  {
    "objectID": "posts/entity-linking/index.html#reticulate-python",
    "href": "posts/entity-linking/index.html#reticulate-python",
    "title": "linking entities and wikidata",
    "section": "1 Reticulate & Python",
    "text": "1 Reticulate & Python\n\nconda create -n fishing\nsource activate fishing\nconda install numpy pip pandas\n/home/jtimm/anaconda3/envs/fishing/bin/pip install spacyfishing\npython -m spacy download en_core_web_sm\n\n\nSys.setenv(RETICULATE_PYTHON = \"/home/jtimm/anaconda3/envs/fishing/bin/python\")\nreticulate::use_condaenv(condaenv = \"fishing\",\n                         conda = \"/home/jtimm/anaconda3/bin/conda\")"
  },
  {
    "objectID": "posts/entity-linking/index.html#spacy",
    "href": "posts/entity-linking/index.html#spacy",
    "title": "linking entities and wikidata",
    "section": "3 spaCy",
    "text": "3 spaCy\n\nimport spacy \nnlp = spacy.load(\"en_core_web_sm\")\nnlp.add_pipe(\"entityfishing\", config={\"extra_info\": True})\nnlp.add_pipe('sentencizer')\n\n\ndoc = nlp(r.text_en)\n\n\n3.1 Entities to Wikipedia\n\nimport pandas as pd\nentities = [(e.label_, \n             e.text, \n             e._.normal_term, \n             e._.kb_qid, \n             e._.url_wikidata, \n             e._.nerd_score,\n             e._.description) for e in doc.ents]\n             \ndf99 = pd.DataFrame(entities, \n                    columns=['type',\n                             'entity', \n                             'normed', \n                             'qid', \n                             'url', \n                             'score', \n                             'description'])\n\n\nreticulate::py$df99 |> \n  dplyr::select(-description) |> \n  DT::datatable(rownames = F)\n\n\n\n\n\n\n\n\n3.2 Wikidata description\n\nstrwrap(reticulate::py$df99$description[[3]], width = 60)[1:10]\n\n [1] \"'''Kremenchuk''' (, ;, [[Romanization of\"                   \n [2] \"Russian|translit.]] ''Kremenchug''), an important\"          \n [3] \"industrial [[city]] in central [[Ukraine]], stands on the\"  \n [4] \"banks of the [[Dnieper]] River. Kremenchuk is the [[Capital\"\n [5] \"city|administrative center]] of the [[Kremenchuk Raion]]\"   \n [6] \"([[Raion|district]]) in [[Poltava Oblast]]\"                 \n [7] \"([[Oblast|province]]). Kremenchuk is administratively\"      \n [8] \"incorporated as a [[City of regional significance\"          \n [9] \"(Ukraine)|city of oblast significance]] and does not belong\"\n[10] \"to the raion. Population: Along with [[Svitlovodsk]] and\""
  },
  {
    "objectID": "posts/text-class/index.html#classifiers",
    "href": "posts/text-class/index.html#classifiers",
    "title": "notes on text classification",
    "section": "3 Classifiers",
    "text": "3 Classifiers\n\narticles1 <- articles0 %>%\n  arrange(doc_id) %>%\n  filter(doc_id %in% unique(dtm_tok$doc_id))\n\n\nset.seed(99)\ntrainIndex <- caret::createDataPartition(articles1$term, p = .7)$Resample1\n\n\n3.1 Bag-of-words & Naive Bayes\n\nDocument represented as bag-of-words.\n\n\ndtm_train <- dtm_sparse[trainIndex, ]\ndtm_test <- dtm_sparse[-trainIndex, ] \ndtm_classifier <- e1071::naiveBayes(as.matrix(dtm_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\ndtm_predicted <- predict(dtm_classifier, as.matrix(dtm_test))\n\n\n\n3.2 Word embeddings & Naive Bayes\n\nDocument represented as an aggregate (here, mean) of constituent word embeddings. Custom/FastText word embeddings derived from quicknews corpus (above).\n\n\nv1 <- embeddings %>% \n  data.frame() %>%\n  mutate(token = rownames(embeddings)) %>%\n  filter(token %in% unique(dtm_tok$token)) %>%\n  inner_join(dtm)\n\navg0 <- lapply(unique(dtm$doc_id), function(y){\n  \n  d0 <- subset(v1, doc_id == y)\n  d1 <- as.matrix(d0[, 1:dims])\n  d2 <-Matrix.utils::aggregate.Matrix(d1,\n                                      groupings = rep(y, nrow(d0)),\n                                      fun = 'mean')\n  as.matrix(d2)\n})\n\ndoc_embeddings <- do.call(rbind, avg0)\n\n\nemb_train <- doc_embeddings[trainIndex, ]\nemb_test <- doc_embeddings[-trainIndex, ] \nemb_classifier <- e1071::naiveBayes(as.matrix(emb_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\nemb_predicted <- predict(emb_classifier, as.matrix(emb_test))\n\n\n\n3.3 FastText classifier\n\nfast_train <- articles1[trainIndex, ]\nfast_test <- articles1[-trainIndex, ]\n\nPrepare data for FastText:\n\ntmp_file_model <- tempfile()\n\ntrain_labels <- paste0(\"__label__\", fast_train$term)\ntrain_texts <- tolower(fast_train$text)\ntrain_to_write <- paste(train_labels, train_texts)\ntrain_tmp_file_txt <- tempfile()\nwriteLines(text = train_to_write, con = train_tmp_file_txt)\n\ntest_labels <- paste0(\"__label__\", fast_test$term)\ntest_texts <- tolower(fast_test$text)\ntest_to_write <- paste(test_labels, test_texts)\n\n\nfastrtext::execute(commands = c(\"supervised\", \n                                \"-input\", train_tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", 25, \n                                \"-lr\", 1, \n                                \"-epoch\", 20, \n                                \"-wordNgrams\", 2, \n                                \"-verbose\", 1))\n\nmodel <- fastrtext::load_model(tmp_file_model)\nfast_predicted0 <- predict(model, sentences = test_to_write)\nfast_predicted <- as.factor(names(unlist(fast_predicted0)))"
  },
  {
    "objectID": "posts/daily-potus/index.html",
    "href": "posts/daily-potus/index.html",
    "title": "dailypotus: an update",
    "section": "",
    "text": "The uspols package includes a simple function for scraping a Wikipedia-based timeline of the Trump presidency – uspols_wiki_timeline().\n\n\ndevtools::install_github(\"jaytimm/dailypotus\")\n\nThe function returns an up-to-date table of daily administration happenings from 20 Jan 2017 onward. Here we walk through a simple work-flow for adding timeline data to time series data. While not an official accounting, a super quick/easy way to get one’s bearings (in a maelstrom) and contextualize data points.\n\nx45.46 <- dailypotus::daily_wiki_trump() |>\n  rbind(dailypotus::daily_wiki_biden())\n\nTable structure is detailed some below. The folks at Wikipedia have delineated events (chronologically?) via bullet points per each day of Trump’s presidency, which have been enumerated here in the bullets column as independent rows. I have mostly done this to make text easier to read/access/etc. I am not sure how meaningful event distinctions (for a given day) actually are.\n\nx45.46 |>\n  dplyr::select(-quarter:-daypres, -dow) |>\n  head() |>\n  DT::datatable(rownames = FALSE, \n                options = list(dom = 't',\n                               pageLength = 6,\n                               scrollX = TRUE))\n\n\n\n\n\n\n\n0.1 A simple use-case\n\nsummary <- x45.46 |>\n  dplyr::filter(!is.na(Events)) |>\n  dplyr::mutate(tokens = tokenizers::count_words(Events)) |>\n  dplyr::group_by(date) |>\n  dplyr::mutate(daily_count = sum(tokens)) |>\n  dplyr::slice(1) |>\n  dplyr::ungroup()\n\nFor demonstration, we calculate total word counts of event descriptions per day from the time-line table. The plot below, then, summarizes these daily counts since inauguration. The plot itself is fairly meaningless, but the hover-action should be useful. For clarity purposes, only the first event for each day is included in the pop-up,.\n\ndp <- summary |>\n  dplyr::mutate(text = stringr::str_wrap(string = Events,\n                                         width = 20,\n                                         indent = 1,\n                                         exdent = 1)) |>\n  \n  plotly::plot_ly(x = ~date, \n                  y = ~daily_count,\n                  color = ~pres,\n                  text = ~text,\n                  type = 'scatter',\n                  mode = 'lines') |>\n  \n  plotly::layout(#atitle = \"Top 10 Drug Types\", \n                 tooltip = c('Events'),\n                 yaxis = list (title = \"Daily event word count per Wikipedia\"))\n\n\n\n0.2 Example plot with Trump daily event on hover"
  },
  {
    "objectID": "posts/daily-potus/index.html#dailypotus",
    "href": "posts/daily-potus/index.html#dailypotus",
    "title": "dailypotus: an update",
    "section": "1 dailypotus",
    "text": "1 dailypotus\n\ndevtools::install_github(\"jaytimm/dailypotus\")\n\nThe two functions are at work below. The daily_wiki_trump returns a table of executive branch happenings from 20 Jan 2017 to 20 Jan 2021. The daily_wiki_biden function returns an up-to-date (and identically structured) table for the ongoing Biden Presidency.\n\nx45.46 <- dailypotus::daily_wiki_trump() |>\n  rbind(dailypotus::daily_wiki_biden())\n\nTable structure is detailed some below. The folks at Wikipedia have delineated events via bullet points per each day of the last two presidencies, which have been enumerated here in the bullets column as independent rows.\n\nlibrary(dplyr)\nx45.46 |>\n  select(-quarter:-daypres, -dow) |>\n  head() |>\n  DT::datatable(rownames = FALSE, \n                options = list(dom = 't',\n                               pageLength = 6,\n                               scrollX = TRUE))"
  },
  {
    "objectID": "posts/daily-potus/index.html#a-simple-use-case",
    "href": "posts/daily-potus/index.html#a-simple-use-case",
    "title": "dailypotus: an update",
    "section": "2 A simple use-case",
    "text": "2 A simple use-case\n\nsummary <- x45.46 |>\n  filter(!is.na(Events)) |>\n  mutate(tokens = tokenizers::count_words(Events)) |>\n  group_by(date) |>\n  mutate(daily_count = sum(tokens)) |>\n  slice(1) |>\n  ungroup()\n\nFor demonstration, we calculate total word counts of event descriptions per day. The plot below, then, summarizes these daily word counts since Jan 2017. The plot itself is fairly meaningless, but the hover-action should be useful (to contextualize other data points, eg). For clarity purposes, only the first event for each day is included in the pop-up.\n\nsummary |>\n  mutate(text = stringr::str_wrap(string = Events,\n                                  width = 20,\n                                  indent = 1,\n                                  exdent = 1)) |>\n  \n  plotly::plot_ly(x = ~date, \n                  y = ~daily_count,\n                  color = ~pres,\n                  text = ~text,\n                  type = 'scatter',\n                  mode = 'lines') |>\n  \n  plotly::layout(#atitle = \"Top 10 Drug Types\", \n                 tooltip = c('Events'),\n                 yaxis = list (title = \"Daily event word count per Wikipedia\"))"
  },
  {
    "objectID": "posts/this-old-house.html#current-us-lawmakers",
    "href": "posts/this-old-house.html#current-us-lawmakers",
    "title": "this old house",
    "section": "1 Current US lawmakers",
    "text": "1 Current US lawmakers\n\nleg_dets <- 'https://theunitedstates.io/congress-legislators/legislators-current.csv'\ndetails <- read.csv((url(leg_dets)), stringsAsFactors = FALSE) \n\nk1 <- c('bioguide_id', 'party', 'birthday', 'gender', \n        'senate_class', 'twitter', 'wikipedia_id')\n\n##\nnominate <- Rvoteview::download_metadata(type = 'members',\n                                         #chamber = 'house',\n                                         congress = '117') |>\n  subset(chamber != 'President')\n\n[1] \"/tmp/RtmpMaRYqR/HS117_members.csv\"\n\nk2 <- c('bioguide_id', 'congress', 'chamber', \n        'district_code', 'state_abbrev', 'bioname',\n        'nominate_dim1', 'nominate_dim2')\n\n##\nmembers <- merge(x = details[,k1], y = nominate[,k2],\n                 by = \"bioguide_id\", all.y = TRUE)\n\nmembers$birthday <- as.Date(members$birthday)\nmembers$age <- trunc(as.numeric(difftime(Sys.Date(), \n                                         members$birthday, \n                                         units = \"days\")) / 365.25)"
  },
  {
    "objectID": "posts/this-old-house.html#american-generations",
    "href": "posts/this-old-house.html#american-generations",
    "title": "this old house",
    "section": "2 American Generations",
    "text": "2 American Generations\n\ngeneration <- c('Greatest',\n                'Silent',\n                'Boomers',\n                'Gen X',\n                'Millenials',\n                'Gen Z',\n                'Post-Z')\n\nstart_yr <- c('1912-01-01',\n              '1928-01-01',\n              '1946-01-01',\n              '1965-01-01',\n              '1981-01-01',\n              '1997-01-01',\n              '2013-01-01')\n\nend_yr <- c('1927-12-31',\n            '1945-12-31',\n            '1964-12-31',\n            '1980-12-31',\n            '1996-12-31',\n            '2012-12-31',\n            '2028-12-31')\n\npew_generations <- data.frame(generation = factor(generation, \n                                                  levels = unique(generation)),\n                              start = as.Date(start_yr), \n                              end = as.Date(end_yr))\n\npew_generations$youngest <- trunc(as.numeric(difftime(Sys.Date(), \n                                         pew_generations$end, \n                                         units = \"days\")) / 365.25)\n\npew_generations |> knitr::kable()\n\n\n\n\ngeneration\nstart\nend\nyoungest\n\n\n\n\nGreatest\n1912-01-01\n1927-12-31\n94\n\n\nSilent\n1928-01-01\n1945-12-31\n76\n\n\nBoomers\n1946-01-01\n1964-12-31\n57\n\n\nGen X\n1965-01-01\n1980-12-31\n41\n\n\nMillenials\n1981-01-01\n1996-12-31\n25\n\n\nGen Z\n1997-01-01\n2012-12-31\n9\n\n\nPost-Z\n2013-01-01\n2028-12-31\n-6\n\n\n\n\n\n\n2.1 Assign generations\n\nmembers$generation <- pew_generations$generation[\n  findInterval(x = members$birthday, vec = pew_generations$start)]\n\n\n\n2.2 Dotplot\n\nlibrary(ggplot2)\nlibrary(dplyr)\nmembers |>\n  filter(chamber == 'House' & !is.na(age)) |>\n \n  ggplot() +\n  geom_dotplot(aes(x = age, \n                   color = gender,\n                   fill = gender,),\n               method=\"histodot\",\n               dotsize = .9, \n               binpositions = 'all', \n               stackratio = 1.3, \n               stackgroups=TRUE,\n               binwidth = 1) + \n  \n  geom_vline(xintercept = pew_generations$youngest[2:5] - 0.5,\n             linetype =2, \n             color = 'black', \n             size = .25) +\n  \n  geom_text(data = pew_generations[2:5,], \n            aes(x = youngest + 2.25, \n                y = 0.95,\n                label = generation),\n            size = 3) +\n  \n  theme_minimal() + \n  ggthemes::scale_fill_economist() +\n  ggthemes::scale_color_economist() +\n  \n  facet_wrap(~party, nrow = 2) +\n  theme(legend.position = \"bottom\",\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank()) +\n  #ylim (0, .5) +\n  \n  labs(title = \"Age distribution of the 117th House by party\")"
  },
  {
    "objectID": "posts/this-old-house/index.html#current-us-lawmakers",
    "href": "posts/this-old-house/index.html#current-us-lawmakers",
    "title": "this old house",
    "section": "1 Current US lawmakers",
    "text": "1 Current US lawmakers\n\nleg_dets <- 'https://theunitedstates.io/congress-legislators/legislators-current.csv'\ndetails <- read.csv((url(leg_dets)), stringsAsFactors = FALSE) \ndetails$birthday <- as.Date(details$birthday)\n\nk1 <- c('bioguide_id', \n        'party', \n        'birthday', \n        'gender', \n        'senate_class',\n        'twitter', \n        'wikipedia_id')\n\n\nnominate <- Rvoteview::download_metadata(type = 'members',\n                                         #chamber = 'house',\n                                         congress = '117') |>\n  subset(chamber != 'President')\n\nk2 <- c('bioguide_id', \n        'congress', \n        'chamber', \n        'district_code', \n        'state_abbrev', \n        'bioname',\n        'nominate_dim1', \n        'nominate_dim2')\n\n\nmembers <- merge(x = details[,k1], \n                 y = nominate[,k2],\n                 by = \"bioguide_id\", \n                 all.y = TRUE)\n\nmembers$age <- trunc(as.numeric(difftime(Sys.Date(), \n                                         members$birthday, \n                                         units = \"days\")) / 365.25)"
  },
  {
    "objectID": "posts/this-old-house/index.html#american-generations",
    "href": "posts/this-old-house/index.html#american-generations",
    "title": "this old house",
    "section": "2 American Generations",
    "text": "2 American Generations\n\ngeneration <- c(#'Missionary',\n                #'Lost',\n                'Greatest',\n                'Silent',\n                'Boomers',\n                'Gen X',\n                'Millenials',\n                'Gen Z',\n                'Post-Z')\n\nstart_yr <- c(#'1860-01-01',\n              #'1883-01-01',\n              '1901-01-01',\n              '1928-01-01',\n              '1946-01-01',\n              '1965-01-01',\n              '1981-01-01',\n              '1997-01-01',\n              '2013-01-01')\n\nend_yr <- c(#'1882-01-01',\n            #'1900-01-01',\n            '1927-12-31',\n            '1945-12-31',\n            '1964-12-31',\n            '1980-12-31',\n            '1996-12-31',\n            '2012-12-31',\n            '2028-12-31')\n\nNote that zero members of Gen Z are old enough to serve in the US House of Representatives– and all Millenials now are.\n\npgs <- data.frame(generation = factor(generation, \n                                      levels = unique(generation)),\n                  start = as.Date(start_yr), \n                  end = as.Date(end_yr))\n\npgs$youngest <- trunc(as.numeric(difftime(Sys.Date(), \n                                          pgs$end,\n                                          units = \"days\")) / 365.25)\n\npgs |> knitr::kable()\n\n\n\n\ngeneration\nstart\nend\nyoungest\n\n\n\n\nGreatest\n1901-01-01\n1927-12-31\n94\n\n\nSilent\n1928-01-01\n1945-12-31\n76\n\n\nBoomers\n1946-01-01\n1964-12-31\n57\n\n\nGen X\n1965-01-01\n1980-12-31\n41\n\n\nMillenials\n1981-01-01\n1996-12-31\n25\n\n\nGen Z\n1997-01-01\n2012-12-31\n9\n\n\nPost-Z\n2013-01-01\n2028-12-31\n-6\n\n\n\n\n\n\n2.1 Assign generations\n\nmembers$generation <- pgs$generation[\n  findInterval(x = members$birthday, vec = pgs$start)]\n\n\nlibrary(dplyr)\nmembers |>\n  filter(chamber == 'House' & !is.na(age)) |>\n  count(party, gender, generation) |>\n  tidyr::spread(generation, n) |>\n  knitr::kable()\n\n\n\n\nparty\ngender\nSilent\nBoomers\nGen X\nMillenials\n\n\n\n\nDemocrat\nF\n11\n40\n34\n5\n\n\nDemocrat\nM\n9\n75\n37\n9\n\n\nRepublican\nF\n2\n13\n11\n6\n\n\nRepublican\nM\n3\n97\n65\n13\n\n\n\n\n\n\n\n2.2 Dotplot\n\nlibrary(ggplot2)\nhs0 <- members |> filter(chamber == 'House' & !is.na(age)) |> \n  arrange(generation)\n \nhs0 |>\n  ggplot() +\n  geom_dotplot(aes(x = age, \n                   color = gender,\n                   fill = gender,),\n               method=\"histodot\",\n               dotsize = .9, \n               binpositions = 'all', \n               stackratio = 1.3, \n               stackgroups=TRUE,\n               binwidth = 1) + \n  \n  geom_vline(xintercept = pgs$youngest[2:5] - 0.5,\n             linetype =2, \n             color = 'black', \n             size = .25) +\n  \n  geom_text(data = pgs[2:5,], \n            aes(x = youngest + 2.25, \n                y = 0.95,\n                label = generation),\n            size = 3) +\n  \n  theme_minimal() + \n  ggthemes::scale_fill_economist() +\n  ggthemes::scale_color_economist() +\n  \n  facet_wrap(~party, nrow = 2) +\n  theme(legend.position = \"bottom\",\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank()) +\n  #ylim (0, .5) +\n  \n  labs(title = \"Age distribution of the 117th House by party\")"
  },
  {
    "objectID": "posts/white-working/index.html",
    "href": "posts/white-working/index.html",
    "title": "race & education in america",
    "section": "",
    "text": "Categories include:\nNote: The “and/or Hispanic” piece is slightly confusing here. For most people, Hispanic = Brown = Race; from this perspective, ethnicity (as distinct from race) is not a meaningful distinction. We include it here because it is included in the census.\nRace-work class distributions"
  },
  {
    "objectID": "posts/white-working/index.html#map",
    "href": "posts/white-working/index.html#map",
    "title": "race & education in america",
    "section": "1 map",
    "text": "1 map\n\ncds_sf <- tigris::congressional_districts(cb = TRUE)\n\nRetrieving data for the year 2020\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\n\ntops <- white_ed |>\n  group_by(GEOID) |>\n  slice(which.max(estimate)) |>\n  ungroup() |>\n  filter(!state_code %in% c('60', '66', '69', '72',\n                            '78', '02', '15'))\n\n\ncds_sf |> inner_join(tops) |>\n  \n  ggplot() + \n  geom_sf(aes(fill = group), \n          color = 'grey',\n          size = .25,\n          alpha=.65) + \n  \n  scale_fill_manual(values = c('#8faabe', \n                               '#1a476f', \n                               '#dae2ba', \n                               '#55752f')) +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        legend.position = 'bottom') #"
  },
  {
    "objectID": "posts/this-old-house/index.html#parliamentary-plot",
    "href": "posts/this-old-house/index.html#parliamentary-plot",
    "title": "this old house",
    "section": "3 Parliamentary Plot",
    "text": "3 Parliamentary Plot\nA real nice function below for drawing ‘parliamentary graphs’ – stolen from this SO post.\n\nseats <- function(N, M, r0 = 2){ \n  radii <- seq(r0, 1, len=M)\n  counts <- numeric(M)\n  pts = do.call(rbind,\n            lapply(1:M, function(i){\n              counts[i] <<- round(N*radii[i]/sum(radii[i:M]))\n              theta <- seq(0, pi, len = counts[i])\n              N <<- N - counts[i]\n              data.frame(x=radii[i]*cos(theta), y=radii[i]*sin(theta), r=i,\n                         theta=theta)}))\n   pts = pts[order(-pts$theta,-pts$r),]\n   pts}\n\n\nsx <- seats(430,8) |> mutate(Generation = hs0$generation)\n\nsx |>\n  ggplot() +\n  geom_point(aes(x, y, \n                 color = Generation), \n             size = 3) +\n  ggthemes::scale_color_stata()+\n  theme_minimal()+\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        legend.position = 'bottom',\n        plot.title = element_text(size=12)\n        )  +\n  labs(title=\"Composition of 117th US House by Generation\")"
  },
  {
    "objectID": "posts/embeddings/index.html#some-text-data-via-pubmed",
    "href": "posts/embeddings/index.html#some-text-data-via-pubmed",
    "title": "global word embeddings in R",
    "section": "1 Some text data via PubMed",
    "text": "1 Some text data via PubMed\n\nlibrary(dplyr)\npmids <- PubmedMTK::pmtk_search_pubmed(search_term = 'medical marijuana', \n                                       fields = c('TIAB','MH'),\n                                       verbose = F)\n\nabstracts0 <- PubmedMTK::pmtk_get_records2(pmids = pmids$pmid, \n                                          cores = 6, \n                                          ncbi_key = key) |> \n  data.table::rbindlist() |> \n  filter(!is.na(abstract)) |>\n  mutate(abstract = tolower(abstract))"
  },
  {
    "objectID": "posts/embeddings/index.html#data-structures-parameters",
    "href": "posts/embeddings/index.html#data-structures-parameters",
    "title": "global word embeddings in R",
    "section": "2 Data structures & parameters",
    "text": "2 Data structures & parameters\n\n2.1 Tokenization\n\ntoks <- abstracts0 |> \n  rename(doc_id = pmid, text = abstract) |>\n  text2df::tif2token()\n\n\nmwes <- text2df::tok2collocations(toks, remove_stops = T)\ntoks0 <- toks |> text2df::token2mwe(mwes)\n\n\n\n2.2 TIF\n\nntif <- data.frame(doc_id = abstracts0$pmid,\n                   text = unlist(lapply(toks0, paste0, collapse = ' ')))\n\n\n\n2.3 Model parameters\n\ndims <- 50\nwindow <- 5\nmin_count <- 5"
  },
  {
    "objectID": "posts/embeddings/index.html#glove-embeddings",
    "href": "posts/embeddings/index.html#glove-embeddings",
    "title": "global word embeddings in R",
    "section": "3 GloVe embeddings",
    "text": "3 GloVe embeddings\n\nit <- text2vec::itoken(toks0, progressbar = FALSE)\nvocab <- text2vec::create_vocabulary(it) |>\n  text2vec::prune_vocabulary(term_count_min = min_count)\n\nvectorizer <- text2vec::vocab_vectorizer(vocab)\ntcm <- text2vec::create_tcm(it, vectorizer, skip_grams_window = window)\n\nglove <- text2vec::GlobalVectors$new(rank = dims, x_max = 10)\nwv_main <- glove$fit_transform(tcm, \n                               n_iter = 10, \n                               convergence_tol = 0.01, \n                               n_threads = 6)\nwv_context <- glove$components\nglove_embeddings <- wv_main + t(wv_context)"
  },
  {
    "objectID": "posts/embeddings/index.html#word2vecdoc2vec-embeddings",
    "href": "posts/embeddings/index.html#word2vecdoc2vec-embeddings",
    "title": "global word embeddings in R",
    "section": "4 word2vec/doc2vec embeddings",
    "text": "4 word2vec/doc2vec embeddings\n\n## d2v <- list(dm = 'PV-DM', bow = 'PV-DBOW')\nmodel.d2v <- doc2vec::paragraph2vec(x = ntif, \n                                    type = \"PV-DM\", \n                                    dim = dims, \n                                    iter = 20,\n                                    min_count = min_count, \n                                    lr = 0.05, \n                                    threads = 5)\n\nd2v_embeddings <- as.matrix(model.d2v, which = \"words\")"
  },
  {
    "objectID": "posts/embeddings/index.html#fasttext-embeddings",
    "href": "posts/embeddings/index.html#fasttext-embeddings",
    "title": "global word embeddings in R",
    "section": "5 fastText embeddings",
    "text": "5 fastText embeddings\n\n## devtools::install_github(\"pommedeterresautee/fastrtext\") \ntmp_file_txt <- tempfile()\ntmp_file_model <- tempfile()\nwriteLines(text = ntif$text, con = tmp_file_txt)\n\nfastrtext::execute(commands = c(\"skipgram\",\n                                \"-input\", tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", gsub('^.*\\\\.', '', dims),\n                                \"-ws\", window, \n                                \"-minCount\", min_count,\n                                \"-verbose\", 1))\n\nfast.model <- fastrtext::load_model(tmp_file_model)\nfast.dict <- fastrtext::get_dictionary(fast.model)\nfast_embeddings <- fastrtext::get_word_vectors(fast.model, fast.dict)"
  },
  {
    "objectID": "posts/embeddings/index.html#pretrained-glove-embeddings",
    "href": "posts/embeddings/index.html#pretrained-glove-embeddings",
    "title": "global word embeddings in R",
    "section": "6 Pretrained GloVe embeddings",
    "text": "6 Pretrained GloVe embeddings\n\nsetwd(locald)\nglove.6B.50d <- data.table::fread('glove.6B.50d.txt')\nglove_pretrained <- as.matrix(glove.6B.50d[, 2:51])\nrownames(glove_pretrained) <- glove.6B.50d$V1\nglove_pretrained <- subset(glove_pretrained, \n                           rownames(glove_pretrained) %in% fast.dict)"
  },
  {
    "objectID": "posts/embeddings/index.html#semantics-cosine-similarity",
    "href": "posts/embeddings/index.html#semantics-cosine-similarity",
    "title": "global word embeddings in R",
    "section": "7 Semantics & cosine similarity",
    "text": "7 Semantics & cosine similarity\n\n7.1 Collate models\n\nNote that the pretrained GloVe model does not include multi-word expressions.\n\n\nmodels <- list('glove' = glove_embeddings,\n               'word2vec' = d2v_embeddings,\n               'fastText' = fast_embeddings,\n               'glove_pretrained' = glove_pretrained)\n\nlapply(models, dim)\n\n$glove\n[1] 5690   50\n\n$word2vec\n[1] 5692   50\n\n$fastText\n[1] 5691   50\n\n$glove_pretrained\n[1] 5062   50\n\n\n\n\n7.2 Cosine similarity\n\nquick_cosine <- function (embeddings,\n                          target, \n                          n = 9) {\n  \n  if(is.character(target)){\n    t0 <- embeddings[target, , drop = FALSE]} else{t0 <- target}\n\n  cos_sim <- text2vec::sim2(x = embeddings,\n                            y = t0,\n                            method = \"cosine\",\n                            norm = \"l2\")\n\n  x1 <- head(sort(cos_sim[,1], decreasing = TRUE), n+1)\n\n  data.frame(rank = 1:(n+1),\n             term1 = rownames(t0),\n             term2 = names(x1),\n             value = round(x1, 3),\n             row.names = NULL)\n}\n\n\nlapply(models, quick_cosine, target = 'legalization') |> #'legality'\n  data.table::rbindlist(idcol = 'model') |>\n  select(-term1, -value) |>\n  tidyr::spread(model, term2) |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nrank\nfastText\nglove\nglove_pretrained\nword2vec\n\n\n\n\n1\nlegalization\nlegalization\nlegalization\nlegalization\n\n\n2\ndecriminalization\nmarijuana\nlegalizing\nlegalisation\n\n\n3\npre-legalization\nrecreational\nlegalize\nlegalizing\n\n\n4\nliberalization\nmedical\ndecriminalization\npassage\n\n\n5\npost-legalization\nuse\nlegalisation\nuse\n\n\n6\ncommercialization\ncannabis\nlegalized\ndecriminalization\n\n\n7\nmedicalization\nits\nproponents\nenactment\n\n\n8\nlegalisation\nstate\nadvocates\nimplementation\n\n\n9\nlegalizing\nmedicinal\ndecriminalisation\nlegalize\n\n\n10\nnormalization\nbefore\nabstinence\nlaws"
  }
]