[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jason Timm",
    "section": "",
    "text": "meadows\n\n\n\nA programmatic build of a plant species reference guide using Google Images and Wikipedia\n\n\n\n\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nlexical semantics\n\n\n\nA uniform approach\n\n\n\n\n\n\nJul 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\n\nA structured, Wikipedia-based timeline of American presidencies 45 & 46\n\n\n\n\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntwitter\n\n\namerican politics\n\n\nclassification\n\n\n\nA look at the prevalence of incivility towards US lawmakers on Twitter\n\n\n\n\n\n\nJun 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nner\n\n\npython\n\n\n\nA quick demo using the spacyfishing python library\n\n\n\n\n\n\nJun 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nclassification\n\n\n\nsome approaches to text classification using R\n\n\n\n\n\n\nJun 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\ntwitter\n\n\ndemographics\n\n\nclassification\n\n\n\nImplementing M3 for demographic inference\n\n\n\n\n\n\nJun 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nlexical semantics\n\n\n\nSome notes on implementing reticulate, spacy, and BERT\n\n\n\n\n\n\nJul 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\n\nThe basics of gerrymandering: a simple simulation\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\ngeospatial\n\n\n\nA ggplot reference for performing some common geo-spatial analyses\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemographics\n\n\n\nA quick look at the composition of American generations\n\n\n\n\n\n\nJun 10, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/bert/index.html",
    "href": "posts/bert/index.html",
    "title": "BERT, reticulate & lexical semantics",
    "section": "",
    "text": "This post provides some quick details on using reticulate to interface Python from RStudio; and, more specifically, using the spacy library and BERT for fine-grained lexical semantic investigation. Here we present a (very cursory) usage-based/BERT-based perspective on the semantic distinction between further and farther, using example contexts extracted from the Corpus of Contemporary American English (COCA)."
  },
  {
    "objectID": "posts/bert/index.html#python-reticulate-set-up",
    "href": "posts/bert/index.html#python-reticulate-set-up",
    "title": "BERT, reticulate & lexical semantics",
    "section": "1 Python & reticulate set-up",
    "text": "1 Python & reticulate set-up\nThe Python code below sets up a conda environment and installs relevant libraries, as well as the BERT transformer, en_core_web_trf. The following should be run in the terminal.\n\nconda create -n poly1\nsource activate poly1\nconda install -c conda-forge spacy\npython -m spacy download en_core_web_trf\nconda install numpy scipy pandas\n\nThe R code below directs R to our Python environment and Python installation.\n\nSys.setenv(RETICULATE_PYTHON = \"/home/jtimm/anaconda3/envs/m3demo/bin/python\")\n\nlibrary(reticulate)\nreticulate::use_condaenv(condaenv = \"poly1\",\n                         conda = \"/home/jtimm/anaconda3/bin/conda\")"
  },
  {
    "objectID": "posts/bert/index.html#coca",
    "href": "posts/bert/index.html#coca",
    "title": "BERT, reticulate & lexical semantics",
    "section": "2 COCA",
    "text": "2 COCA\nThe Corpus of Contemporary American English (COCA) is an absolutely lovely resource, and is one of many corpora made available by the folks at BYU. Here, we utilize COCA to build a simple data set of further-farther example usages. I have copied/pasted from COCA’s online search interface – the data set includes ~500 contexts of usage per form.\n\nlibrary(tidyverse)\ngw <- read.csv(paste0(ld, 'further-farther.csv'), sep = '\\t')\ngw$sent <- tolower(gsub(\"([[:punct:]])\", \" \\\\1 \", gw$text))\ngw$sent <- gsub(\"^ *|(?<= ) | *$\", \"\", gw$sent, perl = TRUE)\n\ngw$count <- stringr::str_count(gw$sent, 'further|farther')\ngw0 <- subset(gw, count == 1)\n\nFor a nice discussion on the semantics of further-farther, see this Merriam-Webster post. The standard semantic distinction drawn between the two forms is physical versus metaphorical distance.\nSome highlighting & sample data below.\n\nfu <- '\\\\1 <span style=\"background-color:lightgreen\">\\\\2</span> \\\\3'\nfa <- '\\\\1 <span style=\"background-color:lightblue\">\\\\2</span> \\\\3'\n\ngw0$text <- gsub('(^.+)(further)(.+$)', fu, gw0$text, ignore.case = T)\ngw0$text <- gsub('(^.+)(farther)(.+$)', fa, gw0$text, ignore.case = T)\ngw0$text <- paste0('... ', gw0$text, ' ...')\n\nset.seed(99)\ngw0 %>% select(year, genre, text) %>% sample_n(10) %>% \n  DT::datatable(rownames = F, escape = F,\n                options = list(dom = 't',\n                               pageLength = 10,\n                               scrollX = TRUE))\n\n\n\n\n\n\nLastly, we identify the location (ie, context position) of the target token within each context (as token index).\n\ngw0$idx <- sapply(gsub(' (farther|further).*$', '', gw0$sent, ignore.case = T), \n                  function(x){\n                    length(corpus::text_tokens(x)[[1]]) })"
  },
  {
    "objectID": "posts/bert/index.html#bert-contextual-embeddings",
    "href": "posts/bert/index.html#bert-contextual-embeddings",
    "title": "BERT, reticulate & lexical semantics",
    "section": "3 BERT & contextual embeddings",
    "text": "3 BERT & contextual embeddings\nUsing BERT and spacy for computing contextual word embeddings is actually fairly straightforward. A very nice resource for some theoretical overview as well as code demo with BERT/spacy is available here.\nGetting started, we pass our data set from R to Python via the r_to_py function.\n\ndf <- reticulate::r_to_py(gw0)\n\nThen, from a Python console, we load the BERT transformer using spacy.\n\nimport spacy\nnlp = spacy.load('en_core_web_trf')\n\nThe stretch of Python code below does all the work here. The transformer computes a 768 dimension vector per token/sub-token comprising each context – then we extract the tensor for either further/farther using the token index. The resulting data structure is matrix-like, with each instantiation of further-farther represented in 768 dimensions.\n\ndef encode(sent, index):\n  doc = nlp(sent.lower())\n  tensor_ix = doc._.trf_data.align[index].data.flatten()\n  out_dim = doc._.trf_data.tensors[0].shape[-1]\n  tensor = doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n  ## tensor.__len__()\n  return tensor.mean(axis=0)\n\nr.df[\"emb\"] = r.df[[\"sent\", \"idx\"]].apply(lambda x: encode(x[0], x[1]), axis = 1)"
  },
  {
    "objectID": "posts/bert/index.html#tsne",
    "href": "posts/bert/index.html#tsne",
    "title": "BERT, reticulate & lexical semantics",
    "section": "4 tSNE",
    "text": "4 tSNE\nTo plot these contexts in two dimensions, we use tSNE to reduce the 768-dimension word embeddings to two. Via Python and numpy, we create a matrix-proper from the further-farther token embeddings extracted above.\n\nimport numpy as np\nX, y  = r.df[\"emb\"].values, r.df[\"id\"].values\nX = np.vstack(X)\n\nFor good measure, we switch back to R to run tSNE. The matrix X, built in Python, is accessed in the R console below via reticulate::py$X.\n\nset.seed(999) ## \ntsne <- Rtsne::Rtsne(X = as.matrix(reticulate::py$X), \n                     check_duplicates = FALSE)\n\ntsne_clean <- data.frame(reticulate::py_to_r(df), tsne$Y) %>%\n  \n  mutate(t1 = gsub('(further|farther)', '\\\\<\\\\1\\\\>', text, ignore.case = T),\n         t2 = stringr::str_wrap(string = t1,\n                                  width = 20,\n                                  indent = 1,\n                                  exdent = 1),\n         id = row_number()) %>%\n  select(id, form, X1, X2, t1, t2) \n\nThe scatter plot below summarizes contextual embeddings for individual tokens of further-farther. So, a nice space for further used adjectivally on the right side of the plot. Other spaces less obviously structured, and some confused spaces as well where speakers seem to have quite a bit of leeway.\n\np <- ggplot2::ggplot(tsne_clean, \n                          aes(x = X1, \n                              y = X2,\n                              color = form,\n                              text = t2,\n                              key = id )) + \n  \n  geom_hline(yintercept = 0, color = 'gray') +\n  geom_vline(xintercept = 0, color = 'gray') +\n  \n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  ggthemes::scale_colour_economist() +\n  ggtitle('further-farther') \n\nplotly::ggplotly(p,  tooltip = 'text') \n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the plotly package.\n  Please report the issue at <\u001b]8;;https://github.com/plotly/plotly.R/issues\u0007https://github.com/plotly/plotly.R/issues\u001b]8;;\u0007>."
  },
  {
    "objectID": "posts/bert/index.html#summary",
    "href": "posts/bert/index.html#summary",
    "title": "BERT, reticulate & lexical semantics",
    "section": "5 Summary",
    "text": "5 Summary\nSo, some notes on reticulate and Python environments, and spacy and BERT. While a computational beast, BERT seems fantastically suited for more fine-grained, qualitative semantic analyses and case studies, and lexicography in general."
  },
  {
    "objectID": "posts/seven-generations/index.html",
    "href": "posts/seven-generations/index.html",
    "title": "the seven living generations in america",
    "section": "",
    "text": "A quick look at the composition of American generations. Per Pew Research definitions & US Census data."
  },
  {
    "objectID": "posts/seven-generations/index.html#american-generations",
    "href": "posts/seven-generations/index.html#american-generations",
    "title": "the seven living generations in america",
    "section": "1 American generations",
    "text": "1 American generations\n\nlibrary(tidyverse)\ngen <- c('Post-Z', 'Gen Z', 'Millennial', \n         'Gen X', 'Boomers', 'Silent', \n         'Greatest')\n\nrange <- c('> 2012', '1997-2012', '1981-1996', \n           '1965-1980', '1946-1964', '1928-1945', \n           '< 1927')\n\ngen_desc <- data.frame(rank = 7:1,\n                       gen = gen,\n                       range = range,\n                       stringsAsFactors = FALSE) %>%\n  arrange(rank)\n\nA summary:\n\n\n\n\n\n\n\nFour of America’s seven living generations are more or less “complete,” and only getting smaller (albeit at different rates): Greatest, Silent, Boomers, and Gen X. The generation comprised of Millenials is complete as well, in that it has been delineated chronologically; however, the group likely continues to grow via immigration.\nWhile Gen Z has been tentatively stamped chronologically by the folks at Pew Research, only the very eldest in the group have just entered the work force. So lots can happen still. And although we include them here, the Post-Z generation is mostly but a thought; half of the group has yet to be born."
  },
  {
    "objectID": "posts/seven-generations/index.html#monthly-us-population-estimates",
    "href": "posts/seven-generations/index.html#monthly-us-population-estimates",
    "title": "the seven living generations in america",
    "section": "2 Monthly US population estimates",
    "text": "2 Monthly US population estimates\nMonthly Postcensal Resident Population plus Armed Forces Overseas, December 2018. Made available by the US Census here. The census has transitioned to a new online interface, and (seemingly) many data sets have been discontinued. Hence, the data set utilized here is slightly dated.\n\npops <- read.csv (\n  url('https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/national/asrh/nc-est2018-alldata-p-File18.csv')) %>%\n  filter(MONTH == '12' & YEAR == '2018') %>%\n  gather(key = 'race', value = 'pop', -UNIVERSE:-AGE)\n\nA more detailed description of the population estimates can be found here. Note: Race categories reflect non-Hispanic populations.\n\nrace <- c('NHWA', 'NHBA', 'NHIA', \n          'NHAA', 'NHNA', 'NHTOM', 'H')\n\nrace1 <- c('White Alone',\n           'Black Alone',\n           'American Indian Alone',\n           'Asian Alone',\n           'Native Hawaiian Alone',\n           'Two or More Races',\n           'Hispanic')\n\nlabels <- data.frame(race = race, \n                     race1=race1, \n                     stringsAsFactors = FALSE)\n\nsearch <- paste(paste0('^',race, '_'), collapse =  '|')\n\nThe following table details a random sample of the data set – with Pew Research defined generations & estimated year-of-birth.\n\ngen_pops <- pops %>%\n  filter(grepl(search, race)) %>%\n  mutate(race = gsub('_.*$', '', race)) %>%\n  group_by(AGE, race) %>%\n  summarise(pop = sum(pop))%>%\n  left_join(labels) %>%\n  filter(AGE != '999') %>%\n  mutate(yob = 2019 - AGE)  %>% ##\n  mutate (gen = case_when (\n    yob < 2013 & yob > 1996 ~ 'Gen Z',\n    yob < 1997 & yob > 1980 ~ 'Millennial',\n    yob < 1981 & yob > 1964 ~ 'Gen X',\n    yob < 1965 & yob > 1945 ~ 'Boomers',\n    yob < 1946 & yob > 1927 ~ 'Silent',\n    yob < 1928 ~ 'Greatest',\n    yob > 2012 ~ 'Post-Z')) %>%\n  left_join(gen_desc) %>%\n  ungroup() %>%\n  select(gen, rank, range, race, \n         race1, yob, AGE, pop)\n\nset.seed(999)\ngen_pops %>% sample_n(7)  %>%\n  select(gen, range, race1:pop) %>%\n  DT::datatable(rownames = FALSE, options = list(dom = 't',\n                                                 scrollX = TRUE))"
  },
  {
    "objectID": "posts/seven-generations/index.html#composition-of-american-generations",
    "href": "posts/seven-generations/index.html#composition-of-american-generations",
    "title": "the seven living generations in america",
    "section": "3 Composition of American generations",
    "text": "3 Composition of American generations\n\n3.1 Population by generation\nThe figure below summarizes the US population by generation. These numbers will vary some depending on the data source. Millenials constitute the plurality of Americans, more recently overtaking a Boomer generation on the wane.\n\ngen_pops %>%\n  group_by(gen, rank) %>%\n  summarize(pop = sum(pop)) %>%\n  mutate(lab = round(pop/1000000, 1)) %>%\n  ggplot(aes(x = reorder(gen, rank), \n             y = pop, \n             fill = gen)) +\n  geom_col(show.legend = FALSE, \n           alpha = 0.75)  +\n  geom_text(aes(label = lab), \n            size = 3.5)+\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  xlab('') + ylab('') +\n  coord_flip()+\n  ggthemes::scale_fill_stata() +\n  theme_minimal() +\n  labs(title = 'Population by American generation',\n       caption = 'SOURCE: US Census, Monthly Postcensal Resident Population plus Armed Forces Overseas, December 2018.')\n\n`summarise()` has grouped output by 'gen'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n3.2 Population by single year of age & generation\n\ngg <- gen_pops %>% \n  group_by(yob, AGE, gen) %>%\n  summarize(tot = sum(pop)) %>%\n  group_by(gen) %>%\n  mutate(tot = max(tot)) %>% #For labels below.\n  filter(yob %in% c('1919', '1928', '1946', '1965', \n                    '1981', '1997', '2013'))\n\n`summarise()` has grouped output by 'yob', 'AGE'. You can override using the\n`.groups` argument.\n\n\nThe figure below illustrates the US population by single year of age, ranging from the population aged less than a year to the population over 100 (as of December 2018). Generation membership per single year of age is specified by color.\n\ngen_pops %>%\n  ggplot(aes(x = AGE, \n             y = pop, \n             fill = gen)) +\n  geom_vline(xintercept = gg$AGE,\n             linetype =2, \n             color = 'gray', \n             size = .25)+\n  \n  geom_col(show.legend = FALSE, \n           alpha = 0.85,\n           width = .7)   +\n  annotate(geom=\"text\", \n           x = gg$AGE - 4.5, \n           y = gg$tot + 70000, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('Age')+ \n  ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank(),\n        panel.grid.minor.y=element_blank()) +\n  ggthemes::scale_fill_stata()+\n  scale_x_reverse(breaks = rev(gg$AGE)) +\n  labs(title = 'American population by single-year age & generation')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n3.3 Population by single year of age, race & generation\n\ngen_pal <- c('#b0bcc1','#ead8c3', '#437193', \n             '#c66767', '#55752f', '#dae2ba', \n             '#7b9bb3')\n\nNext, we crosscut the single year of age counts presented above by race & ethnicity.\n\ngen_pops %>%\n  ggplot(aes(x = AGE, \n             y = pop, \n             fill = race1)) +\n  geom_area(stat = \"identity\",\n            color = 'white',\n            alpha = 0.85) +\n  scale_fill_manual(values = gen_pal) +\n  geom_vline(xintercept = gg$AGE,\n             linetype =2, color = 'gray', size = .25)+\n  annotate(geom=\"text\", \n           x = gg$AGE - 4.5, \n           y = gg$tot + 70000, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('')+ ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank(),\n        panel.grid.minor.y=element_blank()) +\n  \n  scale_x_reverse(breaks = rev(gg$AGE) )+\n  labs(title ='American population by age, race & generation')\n\n\n\n\n\n\n3.4 White America on the wane\n\nwhite_label <- gen_pops %>% \n  group_by(gen, AGE) %>%\n  mutate(per = pop/sum(pop))%>%\n  filter(race1 == 'White Alone') %>%\n  group_by(gen) %>%\n  mutate(per = max(per)) %>% #For labels below.\n  arrange(yob) %>%\n  filter(yob %in% c('1919', '1928', '1946', '1965', \n                    '1981', '1997', '2013'))\n\nThe last figure illustrates a proportional perspective of race & ethnicity in America by single year of age. Per figure, generational differences (at a single point in time) can shed light on (the direction of) potential changes in the overall composition of a given populace. As well as a view of what that populace may have looked like in the past.\n\ngen_pops %>%\n  group_by(gen, AGE) %>%\n  mutate(per = pop/sum(pop)) %>%\n  ggplot(aes(x = (AGE), \n             y = per, \n             fill = race1)) +\n  geom_area(stat = \"identity\",\n            color = 'white',\n            alpha = 0.85) +\n  geom_hline(yintercept = .5, \n             linetype = 4,\n             color = 'white') +\n  scale_fill_manual(values = gen_pal) +\n  geom_vline(xintercept = gg$AGE,\n             linetype = 2, \n             color = 'gray', \n             size = .25)+\n  annotate(geom=\"text\", \n           x = gg$AGE-4.5, \n           y = white_label$per - .05, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('')+ ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank()) +\n  \n  scale_x_reverse(breaks = rev(gg$AGE)) +\n  labs(title = 'American population by age, race & generation')"
  },
  {
    "objectID": "posts/seven-generations/index.html#summary",
    "href": "posts/seven-generations/index.html#summary",
    "title": "the seven living generations in america",
    "section": "4 Summary",
    "text": "4 Summary\nSome different perspectives on the composition of America & American generations."
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html",
    "href": "posts/mapping-roll-calls/index.html",
    "title": "mapping congressional roll calls",
    "section": "",
    "text": "Methods for mapping with R & ggplot, in the context of visualizing historical roll calls from the US House of Representatives. Roll call data accessed via VoteView and the RVoteview package (Jeffrey et al. 2020); shapefiles for historical US Congressional Districts downloaded from the Political Science Dept @ UCLA (Lewis et al. 2013). Visual summary via the patchwork package."
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#historical-urban-centers",
    "href": "posts/mapping-roll-calls/index.html#historical-urban-centers",
    "title": "mapping congressional roll calls",
    "section": "1 Historical urban centers",
    "text": "1 Historical urban centers\nMost populous US cities by decade, from 1790 to 2010; scraped from Wikipedia. For zooming-in on district roll call results for, eg, the ten most populous cities during a given congress.\n\nwiki <- 'https://en.wikipedia.org/wiki/List_of_most_populous_cities_in_the_United_States_by_decade'\n\ndecade <- seq(from = 1780, to = 2010, by = 10)\npops_list <- xml2::read_html(wiki) %>% \n  rvest::html_nodes(\"table\") %>%\n  rvest::html_table(fill = TRUE)\n\npops <- lapply(2:24, function(x) {\n  y <- pops_list[[x]] %>%\n    select(1:4) %>%\n    mutate(decade = decade[x])\n  \n  colnames(y) <- c('rank', 'city', 'state', 'pop', 'decade')\n  return(y) }) %>%\n  bind_rows() %>%\n  mutate(pop = as.integer(gsub(\"[^0-9]\", \"\", pop)))\n\nMost populated US cities circa 1800:\n\n\n\n\n\nrank\ncity\nstate\npop\ndecade\n\n\n\n\n1\nNew York\nNew York\n60514\n1800\n\n\n2\nPhiladelphia\nPennsylvania\n41220\n1800\n\n\n3\nBaltimore\nMaryland\n26514\n1800\n\n\n4\nBoston\nMassachusetts\n24937\n1800\n\n\n5\nCharleston\nSouth Carolina\n18824\n1800\n\n\n6\nNorthern Liberties\nPennsylvania\n10718\n1800\n\n\n7\nSouthwark\nPennsylvania\n9621\n1800\n\n\n8\nSalem\nMassachusetts\n9457\n1800\n\n\n9\nProvidence\nRhode Island\n7614\n1800\n\n\n10\nNorfolk\nVirginia\n6926\n1800"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#historical-congressional-districts",
    "href": "posts/mapping-roll-calls/index.html#historical-congressional-districts",
    "title": "mapping congressional roll calls",
    "section": "2 Historical congressional districts",
    "text": "2 Historical congressional districts\nAgain, via the folks at the Political Science Dept @ UCLA. The Voting Rights Act of 1965 was passed during the 89th congress; a local copy of the shapefile for this congress is loaded below.\n\nfname <- 'districts089'\n\ncd_sf <- sf::st_read(dsn = paste0(cd_directory, fname), \n                    layer = fname, \n                    quiet = TRUE) %>%\n  mutate(STATEFP = substr(ID, 2, 3),\n         district_code = as.numeric(substr(ID, 11, 12))) %>%\n  left_join(states, by = \"STATEFP\") %>%\n  filter(!STATEFP %in% nonx) %>%\n  select(STATEFP, state_abbrev, district_code)"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#voteview-roll-call-data",
    "href": "posts/mapping-roll-calls/index.html#voteview-roll-call-data",
    "title": "mapping congressional roll calls",
    "section": "3 VoteView roll call data",
    "text": "3 VoteView roll call data\nDownloading roll call data for a specific bill via RVoteview requires a bit of trial and error; different bill versions and vote types complicate things for the layman.\n\nvra <- Rvoteview::voteview_search('(\"VOTING RIGHTS ACT OF 1965\") AND (congress:89) \n                                  AND (chamber:house)') %>%\n                                  filter( date == '1965-07-09') %>%\n  janitor::clean_names()\n\nvotes <- Rvoteview::voteview_download(vra$id)\nnames(votes) <- gsub('\\\\.', '_', names(votes))\n\nA quick re-structure of the roll call output:\n\nbig_votes <- votes$legis_long_dynamic %>%\n  left_join(votes$votes_long, by = c(\"id\", \"icpsr\")) %>%\n  filter(!grepl('POTUS', cqlabel)) %>%\n  group_by(state_abbrev) %>%\n  mutate(n = length(district_code)) %>%\n  ungroup() %>%\n  mutate(avote = case_when(vote %in% c(1:3) ~ 'Yea',\n                           vote %in% c(4:6) ~ 'Nay',\n                           vote %in% c(7:9) ~ 'Not Voting'),\n         \n         party_code = case_when(party_code == 100 ~ 'Dem',\n                                party_code == 200 ~ 'Rep' ), \n         Party_Member_Vote = paste0(party_code, ': ', avote),\n         \n         ## fix at-large -- \n         district_code = ifelse(district_code %in% c(98, 99), 0, district_code),\n         district_code = ifelse(n == 1 & district_code == 1, 0, district_code),\n         district_code = as.integer(district_code)) %>%\n  select(-n)\n#Members who represent historical “at-large” districts are \n##assigned 99, 98, or 1 in various circumstances. Per VoteView."
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#roll-call-stats",
    "href": "posts/mapping-roll-calls/index.html#roll-call-stats",
    "title": "mapping congressional roll calls",
    "section": "4 Roll call stats",
    "text": "4 Roll call stats\n\nbig_votes$Party_Member_Vote <- factor(big_votes$Party_Member_Vote)\nbig_votes$Party_Member_Vote <- \n  factor(big_votes$Party_Member_Vote, \n         levels(big_votes$Party_Member_Vote)[c(3,6,1,4,2,5)])\n\n\n4.1 Results\n\nsummary <- big_votes %>%\n  group_by(party_code, avote) %>%\n  count() %>%\n  spread(avote, n) %>%\n  janitor::adorn_totals(where = c('row', 'col')) %>%\n  rename(Party = party_code,\n         NV = `Not Voting`) %>%\n  select(Party, Yea, Nay, NV, Total)\n\n\n\n\nRoll call results for the VRA\n\n\nParty\nYea\nNay\nNV\nTotal\n\n\n\n\nDem\n224\n65\n4\n293\n\n\nRep\n112\n23\n5\n140\n\n\nTotal\n336\n88\n9\n433\n\n\n\n\n\n\n\n4.2 By party affiliation\n\nroll <- big_votes %>% \n  group_by(Party_Member_Vote) %>%\n  count() %>%\n  ungroup() %>%\n  rename(Vote = Party_Member_Vote) \n\nrsum <- roll %>% \n  ggplot(aes(x=Vote, y=n, fill= Vote, label = n)) +\n    geom_col(width=.65, color = 'lightgray') +  \n    geom_text(size = 2.5) +\n    wnomadds::scale_color_rollcall(aesthetics = c(\"fill\")) +\n    scale_x_discrete(limits = rev(levels(roll$Vote)))+\n    coord_flip() +\n  ylim (0, 240) +\n    theme_minimal() + \n      theme(axis.title.x=element_blank(),\n            axis.text.x=element_blank(),\n            axis.title.y=element_blank(),\n            #axis.text.y=element_blank(),\n            legend.position = 'none')\n\nrsum + ggtitle(vra$short_description)\n\n\n\n\n\n\n4.3 By congressional district\n\ncd_sf_w_rolls <- cd_sf %>% \n  left_join(big_votes, by = c(\"state_abbrev\", \"district_code\")) \n\nmain1 <- cd_sf_w_rolls %>%\n  ggplot() + \n  geom_sf(aes(fill = Party_Member_Vote), \n          color = 'white',\n          size = .25) + \n  \n  wnomadds::scale_fill_rollcall() +\n  theme_minimal() +\n  theme(axis.title.x=element_blank(),\n        axis.text.x=element_blank(),\n        axis.title.y=element_blank(),\n        axis.text.y=element_blank(),\n        legend.position = 'none') # +\n\nmain1 + ggtitle(vra$short_description)"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#zooming-in-to-urban-centers",
    "href": "posts/mapping-roll-calls/index.html#zooming-in-to-urban-centers",
    "title": "mapping congressional roll calls",
    "section": "5 Zooming in to urban centers",
    "text": "5 Zooming in to urban centers\nA zoom function for closer inspection of roll call results in urban areas. The sub_geo parameter is used to specify a vector of city/state pairs (eg, “Chicago, Illinois”) to be geocoded via the tmaptools::geocode_OSM function. The geo parameter specifies the full map – as sf object.\n\nmaps_get_minis <- function(sub_geos, geo){\n                           \n  lapply(sub_geos, function(x) {\n    \n    lc <- tmaptools::geocode_OSM (q = x, as.sf = T)\n    lc$bbox <- sf::st_set_crs(lc$bbox, sf::st_crs(geo))\n    cropped <- sf::st_crop(geo, lc$bbox)\n    \n    ggplot() + geom_sf(data = cropped, \n                     aes(fill = Party_Member_Vote),\n                     color = 'white', size = .25) +\n      \n      # ggsflabel::geom_sf_text_repel(data = cropped, \n      #                               aes(label = district_code), \n      #                               size = 2.2) + \n      \n      wnomadds::scale_fill_rollcall() +\n      theme_minimal() + \n      theme(axis.title.x=element_blank(),\n            axis.text.x=element_blank(),\n            axis.title.y=element_blank(),\n            axis.text.y=element_blank(),\n            plot.title = element_text(size=9),\n            legend.position = 'none') +\n      ggtitle(gsub(',.*$', '', x))   })\n}\n\n\n5.1 Coordinates\n\n# x <- 'Albuquerque, New Mexico'\npops1 <- pops %>%\n  filter(decade == paste0(gsub('.-.*$', '', vra$date), 0)) %>%\n  arrange(desc(pop)) %>%\n  mutate(locations = paste0(city, ', ', state)) %>%\n  slice(1:10)\n\nsub_maps <- maps_get_minis(geo = cd_sf_w_rolls, sub_geos = pops1$locations)\n\n\n\n5.2 Zooms\nRoll call results for the VRA (1965) – zoomed in to the ten most populous US cities during the 1960s.\n\npatchwork::wrap_plots(sub_maps, nrow = 2)"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#a-patchwork-perspective",
    "href": "posts/mapping-roll-calls/index.html#a-patchwork-perspective",
    "title": "mapping congressional roll calls",
    "section": "6 A patchwork perspective",
    "text": "6 A patchwork perspective\n\nt2 <- gridExtra::tableGrob(summary, \n                           rows = NULL, \n                           theme = gridExtra::ttheme_minimal(base_size = 8)) \n\np0 <- sub_maps[[1]] + sub_maps[[2]] + sub_maps[[3]] +\n  rsum + patchwork::plot_layout(nrow = 1, widths = c(1,1,1,1))\n\np1 <- sub_maps[[4]] + sub_maps[[5]] + sub_maps[[6]] +\n  t2 + patchwork::plot_layout(nrow = 1, widths = c(1,1,1,1))\n\np2 <- p0/p1 + patchwork::plot_layout(nrow = 2)#, heights = c(4, 1))\n\n\nmain1 / p2  + patchwork::plot_layout(ncol = 1, heights = c(5, 4)) +\n  plot_annotation(\n    title = vra$short_description, \n    subtitle = paste0('Congress ', vra$congress, ' | ',\n                             vra$date, ' | ', vra$bill_number, ' | ',\n                             'Support: ', round(vra$support, 1), '%'),\n     caption = 'Sources: VoteView | Polisci @ UCLA')"
  },
  {
    "objectID": "posts/mapping-roll-calls/index.html#on-the-popular-vote",
    "href": "posts/mapping-roll-calls/index.html#on-the-popular-vote",
    "title": "mapping congressional roll calls",
    "section": "7 On the popular vote",
    "text": "7 On the popular vote\nPer code above, we can create a simple & reproducible work-flow for quickly exploring historical roll calls in the US Congress. For the Bayh–Celler amendment (circa 1969), then, we (down-) load the congressional district shapefile for the 91st congress from UCLA, and re-query RVoteview."
  },
  {
    "objectID": "posts/embeddings/index.html#some-text-data-via-pubmed",
    "href": "posts/embeddings/index.html#some-text-data-via-pubmed",
    "title": "global word embeddings in R",
    "section": "1 Some text data via PubMed",
    "text": "1 Some text data via PubMed\n\nlibrary(dplyr)\npmids <- PubmedMTK::pmtk_search_pubmed(search_term = 'medical marijuana', \n                                       fields = c('TIAB','MH'),\n                                       verbose = F)\n\nabstracts0 <- PubmedMTK::pmtk_get_records2(pmids = pmids$pmid, \n                                          cores = 6, \n                                          ncbi_key = key) |> \n  data.table::rbindlist() |> \n  filter(!is.na(abstract)) |>\n  mutate(abstract = tolower(abstract))"
  },
  {
    "objectID": "posts/embeddings/index.html#data-structures-parameters",
    "href": "posts/embeddings/index.html#data-structures-parameters",
    "title": "global word embeddings in R",
    "section": "2 Data structures & parameters",
    "text": "2 Data structures & parameters\n\n2.1 Tokenization\n\ntoks <- abstracts0 |> \n  rename(doc_id = pmid, text = abstract) |>\n  text2df::tif2token()\n\n\nmwes <- text2df::tok2collocations(toks, remove_stops = T)\ntoks0 <- toks |> text2df::token2mwe(mwes)\n\n\n\n2.2 TIF\n\nntif <- data.frame(doc_id = abstracts0$pmid,\n                   text = unlist(lapply(toks0, paste0, collapse = ' ')))\n\n\n\n2.3 Model parameters\n\ndims <- 50\nwindow <- 5\nmin_count <- 5"
  },
  {
    "objectID": "posts/embeddings/index.html#glove-embeddings",
    "href": "posts/embeddings/index.html#glove-embeddings",
    "title": "global word embeddings in R",
    "section": "3 GloVe embeddings",
    "text": "3 GloVe embeddings\n\nit <- text2vec::itoken(toks0, progressbar = FALSE)\nvocab <- text2vec::create_vocabulary(it) |>\n  text2vec::prune_vocabulary(term_count_min = min_count)\n\nvectorizer <- text2vec::vocab_vectorizer(vocab)\ntcm <- text2vec::create_tcm(it, vectorizer, skip_grams_window = window)\n\nglove <- text2vec::GlobalVectors$new(rank = dims, x_max = 10)\nwv_main <- glove$fit_transform(tcm, \n                               n_iter = 10, \n                               convergence_tol = 0.01, \n                               n_threads = 6)\nwv_context <- glove$components\nglove_embeddings <- wv_main + t(wv_context)"
  },
  {
    "objectID": "posts/embeddings/index.html#word2vecdoc2vec-embeddings",
    "href": "posts/embeddings/index.html#word2vecdoc2vec-embeddings",
    "title": "global word embeddings in R",
    "section": "4 word2vec/doc2vec embeddings",
    "text": "4 word2vec/doc2vec embeddings\n\n## d2v <- list(dm = 'PV-DM', bow = 'PV-DBOW')\nmodel.d2v <- doc2vec::paragraph2vec(x = ntif, \n                                    type = \"PV-DM\", \n                                    dim = dims, \n                                    iter = 20,\n                                    min_count = min_count, \n                                    lr = 0.05, \n                                    threads = 5)\n\nd2v_embeddings <- as.matrix(model.d2v, which = \"words\")"
  },
  {
    "objectID": "posts/embeddings/index.html#fasttext-embeddings",
    "href": "posts/embeddings/index.html#fasttext-embeddings",
    "title": "global word embeddings in R",
    "section": "5 fastText embeddings",
    "text": "5 fastText embeddings\n\n## devtools::install_github(\"pommedeterresautee/fastrtext\") \ntmp_file_txt <- tempfile()\ntmp_file_model <- tempfile()\nwriteLines(text = ntif$text, con = tmp_file_txt)\n\nfastrtext::execute(commands = c(\"skipgram\",\n                                \"-input\", tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", gsub('^.*\\\\.', '', dims),\n                                \"-ws\", window, \n                                \"-minCount\", min_count,\n                                \"-verbose\", 1))\n\nfast.model <- fastrtext::load_model(tmp_file_model)\nfast.dict <- fastrtext::get_dictionary(fast.model)\nfast_embeddings <- fastrtext::get_word_vectors(fast.model, fast.dict)"
  },
  {
    "objectID": "posts/embeddings/index.html#pretrained-glove-embeddings",
    "href": "posts/embeddings/index.html#pretrained-glove-embeddings",
    "title": "global word embeddings in R",
    "section": "6 Pretrained GloVe embeddings",
    "text": "6 Pretrained GloVe embeddings\n\nsetwd(locald)\nglove.6B.50d <- data.table::fread('glove.6B.50d.txt')\nglove_pretrained <- as.matrix(glove.6B.50d[, 2:51])\nrownames(glove_pretrained) <- glove.6B.50d$V1\nglove_pretrained <- subset(glove_pretrained, \n                           rownames(glove_pretrained) %in% fast.dict)"
  },
  {
    "objectID": "posts/embeddings/index.html#semantics-cosine-similarity",
    "href": "posts/embeddings/index.html#semantics-cosine-similarity",
    "title": "global word embeddings in R",
    "section": "7 Semantics & cosine similarity",
    "text": "7 Semantics & cosine similarity\n\n7.1 Collate models\n\nNote that the pretrained GloVe model does not include multi-word expressions.\n\n\nmodels <- list('glove' = glove_embeddings,\n               'word2vec' = d2v_embeddings,\n               'fastText' = fast_embeddings,\n               'glove_pretrained' = glove_pretrained)\n\nlapply(models, dim)\n\n$glove\n[1] 5690   50\n\n$word2vec\n[1] 5692   50\n\n$fastText\n[1] 5691   50\n\n$glove_pretrained\n[1] 5062   50\n\n\n\n\n7.2 Cosine similarity\n\nquick_cosine <- function (embeddings,\n                          target, \n                          n = 9) {\n  \n  if(is.character(target)){\n    t0 <- embeddings[target, , drop = FALSE]} else{t0 <- target}\n\n  cos_sim <- text2vec::sim2(x = embeddings,\n                            y = t0,\n                            method = \"cosine\",\n                            norm = \"l2\")\n\n  x1 <- head(sort(cos_sim[,1], decreasing = TRUE), n+1)\n\n  data.frame(rank = 1:(n+1),\n             term1 = rownames(t0),\n             term2 = names(x1),\n             value = round(x1, 3),\n             row.names = NULL)\n}\n\n\nlapply(models, quick_cosine, target = 'legalization') |> #'legality'\n  data.table::rbindlist(idcol = 'model') |>\n  select(-term1, -value) |>\n  tidyr::spread(model, term2) |>\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\nrank\nfastText\nglove\nglove_pretrained\nword2vec\n\n\n\n\n1\nlegalization\nlegalization\nlegalization\nlegalization\n\n\n2\ndecriminalization\nmarijuana\nlegalizing\nlegalisation\n\n\n3\npre-legalization\nrecreational\nlegalize\nlegalizing\n\n\n4\nliberalization\nmedical\ndecriminalization\npassage\n\n\n5\npost-legalization\nuse\nlegalisation\nuse\n\n\n6\ncommercialization\ncannabis\nlegalized\ndecriminalization\n\n\n7\nmedicalization\nits\nproponents\nenactment\n\n\n8\nlegalisation\nstate\nadvocates\nimplementation\n\n\n9\nlegalizing\nmedicinal\ndecriminalisation\nlegalize\n\n\n10\nnormalization\nbefore\nabstinence\nlaws"
  },
  {
    "objectID": "posts/meadow/index.html",
    "href": "posts/meadow/index.html",
    "title": "My backyard meadow",
    "section": "",
    "text": "I had a vision for turning my mess of a backyard into a high desert meadow. So, I planted a mixture of native grasses and wildflowers. The former is comprised of 12 warm-season grasses; the latter a collection of 18 southwest annuals/perennials. Things grow for sure. But I don’t know what is what, or what is weed.\nI wanted an online reference guide specific to the 30 species of wildflower and grass in my meadow. And I wanted to build this programmatically, using a single block of code, based on a simple table of scientific/common names. While also having a table of contents, headers, etc in quarto.\n\n\n\ndf |> DT::datatable(df, rownames = F)"
  },
  {
    "objectID": "posts/meadow/index.html#building-guide",
    "href": "posts/meadow/index.html#building-guide",
    "title": "My backyard meadow",
    "section": "2 Building guide",
    "text": "2 Building guide\nProcess:\n\nCollect photos for each genus/species from Google using the photomoe package;\ncollect a sentence or two of species description from Wikipedia using the quicknews package; and\noutput everything in one fell swoop.\n\nThe code below is for the wildflower section; we do the same thing again for grasses. The trick, which you can’t see below, is the chunk option results = 'asis', which basically allows you to generate raw markdown by cat-ing and print-ing everything. See this section of the R Markdown Cookbook for a complete discussion.\n\nfor(j in 1:18){\n  ## get photos fro google -- build collage\n  link0 <- photomoe::img_get_gurls(df$scientific[j])\n  photomoe::img_download_images(link = link0, \n                                dir = tempdir(), \n                                prefix = df$scientific[j])\n\n  gg <- photomoe::img_build_collage(\n    paths = list.files(tempdir(), full.names = T),\n    dimx = 5,\n    dimy = 4,\n    prefix = df$scientific[j])\n  \n  ## get first p node from wikipedia\n  urls <- paste0('https://en.wikipedia.org/wiki/',\n               gsub(' ', '_', df$scientific[j]))\n\n  wko <- quicknews::get_site(urls) |>\n    subset(type == 'p' & nchar(text) > 3) |>\n    slice(1) |> \n    mutate(text = gsub('\\\\[[0-9]\\\\]', '', text))\n  \n  ##output\n  cat('\\n\\n### ', df$common[j], '\\n', '> ', \n      df$scientific[j], '\\n\\n')\n  par(bg = 'white', mar=c(0,0,0,0))\n  plot(gg)\n  if(nrow(wko) > 0){cat('\\n', '> ', wko$text)}\n  cat('\\n\\n ---')\n}"
  },
  {
    "objectID": "posts/meadow/index.html#wildflowers",
    "href": "posts/meadow/index.html#wildflowers",
    "title": "My backyard meadow",
    "section": "3 Wildflowers",
    "text": "3 Wildflowers\n\n3.1 Plains Coreopsis\n\nCoreopsis tinctoria\n\n\n\n\n\nPlains coreopsis, garden tickseed,golden tickseed, or calliopsis, Coreopsis tinctoria, is an annual forb. The plant is common in Canada (from Quebec to British Columbia), northeast Mexico (Coahuila, Nuevo León, Tamaulipas), and much of the United States, especially the Great Plains and Southern states where it is often called “calliopsis.” The species is also widely cultivated and naturalized in China.\n\n\n\n\n3.2 California Poppy\n\nEschscholzia\n\n\n\n\n\nEschscholzia /ɛˈʃɒltsiə/ is a genus of 12 annual or perennial plants in the Papaveraceae (poppy) family. The genus was named after the Baltic German/Imperial Russian botanist Johann Friedrich von Eschscholtz (1793–1831). All species are native to Mexico or the southern United States.\n\n\n\n\n3.3 Mexican Gold Poppy\n\nEschscholzia mexicana\n\n\n\n\n\nEschscholzia californica, the California poppy, golden poppy, California sunlight or cup of gold, is a species of flowering plant in the family Papaveraceae, native to the United States and Mexico. It is cultivated as an ornamental plant flowering in summer (spring in southern Australia), with showy cup-shaped flowers in brilliant shades of red, orange and yellow (occasionally pink). It is also used as food or a garnish. It became the official state flower of California in 1903.\n\n\n\n\n3.4 Indian Blanket\n\nGaillardia pulchella\n\n\n\n\n\nGaillardia pulchella (firewheel, Indian blanket, Indian blanketflower, or sundance) is a North American species of short-lived perennial or annual flowering plants in the sunflower family.\n\n\n\n\n3.5 Bird’s Eyes\n\nGilia tricolor\n\n\n\n\n\nIt is native to the Central Valley and foothills of the Sierra Nevada and Coast Ranges in California. Its native habitats include open, grassy plains and slopes below 2,000 feet (610 m).\n\n\n\n\n3.6 Blue Flax\n\nLinum perenne lewisii\n\n\n\n\n\nLinum lewisii (Linum perenne var. lewisii) (Lewis flax, blue flax or prairie flax) is a perennial plant in the family Linaceae, native to western North America from Alaska south to Baja California, and from the Pacific Coast east to the Mississippi River. It grows on ridges and dry slopes, from sea level in the north up to 11,000 feet (3,400 metres) in the Sierra Nevada.\n\n\n\n\n3.7 Tidy Tips\n\nLayia platyglossa\n\n\n\n\n\nTidytips was formerly found throughout low-elevation dry habitats in California including the Mojave Desert and into Arizona and Utah. In pre-European times this plant was common in solid stands at lower elevations. Found in grassy valley floors, slopes of hills, openings in coastal sage scrub and chaparral, coastal plains, and in the High Desert. A member of Spring wildflower ‘displays,’ blooming March to June.\n\n\n\n\n3.8 Arizona Lupine\n\nLupinus arizonicus\n\n\n\n\n\nLupinus arizonicus, the Arizona lupine, is a flowering plant in the legume family Fabaceae, native to the Mojave and Sonoran Deserts of North America, where it can be found growing in open places and sandy washes below 1,100 metres (3,600 ft) elevation. It is common around Joshua Tree National Park and Death Valley National Park in California.\n\n\n\n\n3.9 Arroyo Lupine\n\nLupinus succulentus\n\n\n\n\n\nIt is native to California, where it is common throughout much of the state, and adjacent sections of Arizona and Baja California. Lupinus succulentus is known from many types of habitat and it can colonize disturbed areas.\n\n\n\n\n3.10 Blazing Star\n\nMentzelia lindleyi\n\n\n\n\n\nMentzelia lindleyi, commonly known as golden bartonia,Lindley’s blazingstar,evening star, or blazing star, is an annual wildflower of western North America.\n\n\n\n\n3.11 Five Spot\n\nNemophila maculata\n\n\n\n\n\nThe wildflower is found on slopes in elevations between 20–1,000 metres (66–3,281 ft). The plant is endemic to California. It is most common in the Sierra Nevada, Sacramento Valley, and the California Coast Ranges in the San Francisco Bay Area.\n\n\n\n\n3.12 White Evening Primrose\n\nOenothera pallida\n\n\n\n\n\n\n\n3.13 Showy Pink Evening Primrose\n\nOenothera speciosa\n\n\n\n\n\nOenothera speciosa is a species of evening primrose known by several common names, including pinkladies, pink evening primrose, showy evening primrose, Mexican primrose, amapola, and buttercups (not to be confused with true buttercups in the genus Ranunculus).\n\n\n\n\n3.14 California Bluebell\n\nPhacelia campanularia\n\n\n\n\n\nPhacelia campanularia is a species of flowering plant in the borage family, Boraginaceae, known by the common names desertbells,desert bluebells,California-bluebell,desert scorpionweed, and desert Canterbury bells. Its true native range is within the borders of California, in the Mojave and Sonoran Deserts, but it is commonly cultivated as an ornamental plant and it can be found growing elsewhere as an introduced species.\n\n\n\n\n3.15 Mexican Hat\n\nRatibida columnifera\n\n\n\n\n\nRatibida columnifera, commonly known as upright prairie coneflower or Mexican hat, is a perennial species of flowering plant in the genus Ratibida family Asteraceae. It grows up to 120 centimetres (47 inches) tall with leaves 2.5–15 cm (1–6 in) long. It is native to much of North America and inhabits prairies, plains, roadsides, and disturbed areas from southern Canada through most of the United States to northern Mexico.\n\n\n\n\n3.16 Prairie Aster\n\nAster tanacetifolius\n\n\n\n\n\n\n\n3.17 Desert Marigold\n\nBaileya multiradiata\n\n\n\n\n\nBaileya multiradiata is a North American species of sun-loving wildflowers native to the deserts of northern Mexico and the Southwestern United States. It has been found in the States of Sonora, Chihuahua, Coahuila, Durango, Aguascalientes, California, Arizona, Nevada, Utah, New Mexico, and Texas.\n\n\n\n\n3.18 Farewell to Spring\n\nClarkia unguiculata\n\n\n\n\n\nClarkia unguiculata is a species of wildflower known by the common name elegant clarkia or mountain garland. This plant is endemic to California, where it is found in many woodland habitats. Specifically it is common on the forest floor of many oak woodlands, along with typical understory wildflowers that include Calochortus luteus, Cynoglossum grande and Delphinium variegatum.C. unguiculata presents a spindly, hairless, waxy stem not exceeding a meter in height and bears occasional narrow leaves. The showy flowers have hairy, fused sepals forming a cup beneath the corolla, and four petals each one to 2.5 centimeters long. The paddle-like petals are a shade of pink to reddish to purple and are slender and diamond-shaped or triangular. There are eight long stamens, the outer four of which have large red anthers. The stigma protrudes from the flower and can be quite large. Flowers of the genus Clarkia are primarily pollinated by specialist bees found in their native habitat “Clarkias independently developed self-pollination in 12 lineages.”"
  },
  {
    "objectID": "posts/meadow/index.html#grasses",
    "href": "posts/meadow/index.html#grasses",
    "title": "My backyard meadow",
    "section": "4 Grasses",
    "text": "4 Grasses\n\n4.1 Blue grama, Hachita\n\nBouteloua gracilis\n\n\n\n\n\nBouteloua gracilis, the blue grama, is a long-lived, warm-season (C4) perennial grass, native to North America.\n\n\n\n\n4.2 Little bluestem\n\nSchizachyrium scoparium\n\n\n\n\n\nSchizachyrium scoparium, commonly known as little bluestem or beard grass, is a species of North American prairie grass native to most of the contiguous United States (except California, Nevada, and Oregon) as well as a small area north of the Canada–US border and northern Mexico. It is most common in the Midwestern prairies and is one of the most abundant native plants in Texas grasslands.\n\n\n\n\n4.3 Indian Ricegrass\n\nAchnatherum hymenoides\n\n\n\n\n\nOryzopsis hymenoides (syn: Stipa hymenoides or Achnatherum hymenoides, common names: Indian ricegrass and sand rice grass) is a cool-season, perennial bunchgrass with narrow, rolled leaf blades. It is native to western North America east of the Cascades from British Columbia and Alberta south to southern California, northeastern Mexico, and Texas.\n\n\n\n\n4.4 Sideoats grama, El Reno\n\nBouteloua curtipendula\n\n\n\n\n\nBouteloua curtipendula, commonly known as sideoats grama, is a perennial, short prairie grass that is native throughout the temperate and tropical Western Hemisphere, from Canada south to Argentina.\n\n\n\n\n4.5 Galleta, Viva\n\nHilaria jamesii\n\n\n\n\n\nIt is native to the southwestern United States, where it is widespread in scrub, woodland, grassland, and plateau habitat. It is tolerant of arid environments such as desert floors. It is common in the northern Mojave Desert.\n\n\n\n\n4.6 Alkali sacaton, VNS\n\nSporobolus airoides\n\n\n\n\n\nSporobolus airoides is a species of grass known by the common name alkali sacaton. It is native to western North America, including the Western United States west of the Mississippi River, British Columbia and Alberta in Canada, and northern and central Mexico. It grows in many types of habitat, often in alkali soils, such as in California desert regions.\n\n\n\n\n4.7 Western wheatgrass, Arriba\n\nPascopyrum smithii\n\n\n\n\n\nPascopyrum is a monotypic genus of grass containing the sole species Pascopyrum smithii, which is known by the common names western wheatgrass and red-joint wheatgrass, after the red coloration of the nodes. It is native to North America.\n\n\n\n\n4.8 Sand dropseed, VNS\n\nSporobolus cryptandrus\n\n\n\n\n\nSporobolus cryptandrus is a species of grass known as sand dropseed. It is native to North America, where it is widespread in southern Canada, most of the United States, and northern Mexico.\n\n\n\n\n4.9 Buffalo grass, Texoka\n\nBuchloe dactyloides\n\n\n\n\n\nBouteloua dactyloides, commonly known as buffalograss or buffalo grass, is a North American prairie grass native to Canada, Mexico, and the United States. It is a shortgrass found mainly on the High Plains and is co-dominant with blue grama (B. gracilis) over most of the shortgrass prairie.\n\n\n\n\n4.10 Sheep fescue, Covar\n\nFestuca ovina\n\n\n\n\n\nFestuca ovina, sheep’s fescue or sheep fescue, is a species of grass. It is sometimes confused with hard fescue (Festuca trachyphylla).\n\n\n\n\n4.11 Green needlegrass, Lodorm\n\nNassella viridula\n\n\n\n\n\nNassella viridula is a species of grass known by the common name green needlegrass. It is native to North America, where it is widespread in western Canada and the western and central United States. It is introduced in parts of eastern North America.\n\n\n\n\n4.12 Perennial ryegrass, Linn\n\nLolium perenne\n\n\n\n\n\nLolium perenne, common name perennial ryegrass,[failed verification]English ryegrass, winter ryegrass, or ray grass, is a grass from the family Poaceae. It is native to Europe, Asia and northern Africa, but is widely cultivated and naturalised around the world."
  },
  {
    "objectID": "posts/text-class/index.html#labeled-data",
    "href": "posts/text-class/index.html#labeled-data",
    "title": "notes on text classification",
    "section": "1 Labeled data",
    "text": "1 Labeled data\nFor demonstration purposes, we build a corpus using the quicknews package. The corpus is comprised of articles returned from a set of health-related queries. Search terms, then, serve as classification labels. An imperfect annotation process, but fine for our purposes here. As “distant” supervision.\n\nlibrary(tidyverse)\n\nterms <- c('heart disease', \n           'diabetes',\n           'mental health',\n           'substance abuse',\n           'obesity',\n           'kidney disease')\n\nrss <- lapply(terms, function(x) {\n  \n  quicknews::qnews_build_rss(x) %>%\n    quicknews::qnews_strip_rss() }) \n\nnames(rss) <- terms\n  \nrss0 <- rss %>%\n  bind_rows(.id = 'term') %>%\n  mutate(term = gsub(' ', '_', term)) %>%\n  distinct(link, .keep_all = TRUE) %>%\n  mutate(doc_id = as.character(row_number())) %>%\n  mutate(term = as.factor(term))  %>%\n  select(doc_id, term:link)\n\n\narticles <- quicknews::qnews_extract_article(url = rss0$link, cores = 7) \narticles0 <- articles %>% left_join(rss0)\n\nDescriptives for the resulting corpus by search term. So, a super small demo corpus.\n\narticles0 %>%\n  mutate(words = tokenizers::count_words(text)) %>%\n  group_by(term) %>%\n  summarize(n = n(), words = sum(words)) %>%\n  janitor::adorn_totals() %>%\n  knitr::kable()\n\n\n\n\nterm\nn\nwords\n\n\n\n\ndiabetes\n81\n72045\n\n\nheart_disease\n82\n65505\n\n\nkidney_disease\n80\n68379\n\n\nmental_health\n82\n73804\n\n\nobesity\n81\n69179\n\n\nsubstance_abuse\n84\n64133\n\n\nTotal\n490\n413045\n\n\n\n\n\nA sample of articles from the GoogleNews/quicknews query:\n\nset.seed(99)\narticles0 %>% \n  select(term, date, source, title) %>% \n  sample_n(5) %>%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nterm\ndate\nsource\ntitle\n\n\n\n\nkidney_disease\n2022-05-23\nReuters.com\nU.S. Task Force to consider routine kidney disease screening as new drugs available\n\n\nsubstance_abuse\n2022-06-15\nThe Sylva Herald\nDistrict Court | Court News | thesylvaherald.com\n\n\nsubstance_abuse\n2022-05-28\nWUSF News\nNortheast Florida’s health care concerns: from substance abuse to food deserts\n\n\nheart_disease\n2022-05-23\nBloomberg\nTake Breaks and Watch Less TV to Slash Heart Disease Risk, Experts Say\n\n\nsubstance_abuse\n2022-06-14\nWilliston Daily Herald\nReport: U.S. records highest ever rates of substance abuse, suicide-related deaths"
  },
  {
    "objectID": "posts/text-class/index.html#data-structures",
    "href": "posts/text-class/index.html#data-structures",
    "title": "notes on text classification",
    "section": "2 Data structures",
    "text": "2 Data structures\n\n2.1 Document-Term Matrix\n\nAs bag-of-words\n\n\ndtm <- articles0 %>% \n  mutate(wds = tokenizers::count_words(text)) %>%\n  filter(wds > 200 & wds < 1500) %>%\n  text2df::tif2token() %>%\n  text2df::token2df() %>%\n  mutate(token = tolower(token)) \n  # mutate(stem = quanteda::char_wordstem(token))\n\ndtm %>% head() %>% knitr::kable()\n\n\n\n\ndoc_id\ntoken\ntoken_id\n\n\n\n\n2\nheart\n1\n\n\n2\ndisease\n2\n\n\n2\nis\n3\n\n\n2\na\n4\n\n\n2\nvery\n5\n\n\n2\ngeneral\n6\n\n\n\n\n\n\ndtm_tok <- dtm %>%  \n  count(doc_id, token) %>%\n  group_by(token) %>%\n  mutate(docf = length(unique(doc_id))) %>% ungroup() %>%\n  mutate(docf = round(docf/length(unique(doc_id)), 3 )) %>%\n  filter(docf >= 0.01 & docf < 0.5 & \n           !grepl('^[0-9]|^[[:punct:]]', token))\n\ndtm_tok %>% head() %>% knitr::kable()\n\n\n\n\ndoc_id\ntoken\nn\ndocf\n\n\n\n\n10\nacknowledge\n1\n0.031\n\n\n10\nacross\n1\n0.207\n\n\n10\nadditional\n1\n0.151\n\n\n10\naddress\n3\n0.217\n\n\n10\nadhere\n1\n0.018\n\n\n10\nads\n1\n0.026\n\n\n\n\n\n\ndtm_sparse <- dtm_tok %>%\n  tidytext::bind_tf_idf(term = token, \n                        document = doc_id,\n                        n = n) %>% \n  tidytext::cast_sparse(row = doc_id, \n                        column = token, \n                        value = tf_idf)\n\n\n\n2.2 Cleaned text\n\nctext <- dtm %>%\n  group_by(doc_id) %>%\n  summarize(text = paste0(token, collapse = ' ')) %>% ungroup()\n\nstrwrap(ctext$text[5], width = 60)[1:5]\n\n[1] \"upon receiving a diagnosis of type 1 diabetes ( t1d ) ,\"    \n[2] \"many people have the same reaction : “ but why me ? ” some\" \n[3] \"people have t1d that runs in their family , while others\"   \n[4] \"have no idea how or why they received a diagnosis . often ,\"\n[5] \"to their frustration , those questions go unanswered . but\" \n\n\n\n\n2.3 Word embeddings\n\n## devtools::install_github(\"pommedeterresautee/fastrtext\") \ntmp_file_txt <- tempfile()\ntmp_file_model <- tempfile()\nwriteLines(text = ctext$text, con = tmp_file_txt)\ndims <- 25\nwindow <- 5\n\nfastrtext::execute(commands = c(\"skipgram\",\n                                \"-input\", tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", gsub('^.*\\\\.', '', dims),\n                                \"-ws\", window, \n                                \"-verbose\", 1))\n\n\nRead 0M words\nNumber of words:  5318\nNumber of labels: 0\n\nProgress: 100.0% words/sec/thread:   21912 lr:  0.000000 avg.loss:  2.412294 ETA:   0h 0m 0s\n\nfast.model <- fastrtext::load_model(tmp_file_model)\n\nadd .bin extension to the path\n\nfast.dict <- fastrtext::get_dictionary(fast.model)\nembeddings <- fastrtext::get_word_vectors(fast.model, fast.dict)"
  },
  {
    "objectID": "posts/text-class/index.html#classifiers",
    "href": "posts/text-class/index.html#classifiers",
    "title": "notes on text classification",
    "section": "3 Classifiers",
    "text": "3 Classifiers\n\narticles1 <- articles0 %>%\n  arrange(doc_id) %>%\n  filter(doc_id %in% unique(dtm_tok$doc_id))\n\n\nset.seed(99)\ntrainIndex <- caret::createDataPartition(articles1$term, p = .7)$Resample1\n\n\n3.1 Bag-of-words & Naive Bayes\n\nDocument represented as bag-of-words.\n\n\ndtm_train <- dtm_sparse[trainIndex, ]\ndtm_test <- dtm_sparse[-trainIndex, ] \ndtm_classifier <- e1071::naiveBayes(as.matrix(dtm_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\ndtm_predicted <- predict(dtm_classifier, as.matrix(dtm_test))\n\n\n\n3.2 Word embeddings & Naive Bayes\n\nDocument represented as an aggregate (here, mean) of constituent word embeddings. Custom/FastText word embeddings derived from quicknews corpus (above).\n\n\nv1 <- embeddings %>% \n  data.frame() %>%\n  mutate(token = rownames(embeddings)) %>%\n  filter(token %in% unique(dtm_tok$token)) %>%\n  inner_join(dtm)\n\navg0 <- lapply(unique(dtm$doc_id), function(y){\n  \n  d0 <- subset(v1, doc_id == y)\n  d1 <- as.matrix(d0[, 1:dims])\n  d2 <-Matrix.utils::aggregate.Matrix(d1,\n                                      groupings = rep(y, nrow(d0)),\n                                      fun = 'mean')\n  as.matrix(d2)\n})\n\ndoc_embeddings <- do.call(rbind, avg0)\n\n\nemb_train <- doc_embeddings[trainIndex, ]\nemb_test <- doc_embeddings[-trainIndex, ] \nemb_classifier <- e1071::naiveBayes(as.matrix(emb_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\nemb_predicted <- predict(emb_classifier, as.matrix(emb_test))\n\n\n\n3.3 FastText classifier\n\nfast_train <- articles1[trainIndex, ]\nfast_test <- articles1[-trainIndex, ]\n\nPrepare data for FastText:\n\ntmp_file_model <- tempfile()\n\ntrain_labels <- paste0(\"__label__\", fast_train$term)\ntrain_texts <- tolower(fast_train$text)\ntrain_to_write <- paste(train_labels, train_texts)\ntrain_tmp_file_txt <- tempfile()\nwriteLines(text = train_to_write, con = train_tmp_file_txt)\n\ntest_labels <- paste0(\"__label__\", fast_test$term)\ntest_texts <- tolower(fast_test$text)\ntest_to_write <- paste(test_labels, test_texts)\n\n\nfastrtext::execute(commands = c(\"supervised\", \n                                \"-input\", train_tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", 25, \n                                \"-lr\", 1, \n                                \"-epoch\", 20, \n                                \"-wordNgrams\", 2, \n                                \"-verbose\", 1))\n\nmodel <- fastrtext::load_model(tmp_file_model)\nfast_predicted0 <- predict(model, sentences = test_to_write)\nfast_predicted <- as.factor(names(unlist(fast_predicted0)))"
  },
  {
    "objectID": "posts/text-class/index.html#evaluation",
    "href": "posts/text-class/index.html#evaluation",
    "title": "notes on text classification",
    "section": "4 Evaluation",
    "text": "4 Evaluation\n\npredictions <- list('BOW' = dtm_predicted, \n                    'Word embeddings' = emb_predicted, \n                    'FastText' = fast_predicted)\n\n\n4.1 Model accuracy\n\nconf_mats <- lapply(predictions, \n                    caret::confusionMatrix,\n                    reference = articles1[-trainIndex, ]$term)\n\nsums <- lapply(conf_mats, '[[', 3) \nsums0 <- as.data.frame(do.call(rbind, sums)) %>%\n  select(1:4) %>%\n  mutate_at(1:4, round, 3)\n\nsums0 %>% arrange(desc(Accuracy)) %>% knitr::kable()\n\n\n\n\n\nAccuracy\nKappa\nAccuracyLower\nAccuracyUpper\n\n\n\n\nFastText\n0.757\n0.708\n0.668\n0.832\n\n\nBOW\n0.661\n0.593\n0.567\n0.747\n\n\nWord embeddings\n0.435\n0.320\n0.343\n0.530\n\n\n\n\n\n\n\n4.2 FastText classifier: Model accuracy by class\n\nconf_mats[['FastText']]$byClass %>% data.frame() %>%\n  select (Sensitivity, Specificity, Balanced.Accuracy) %>%\n  rownames_to_column(var = 'topic') %>%\n  mutate(topic = gsub('Class: ','', topic)) %>% \n  mutate_if(is.numeric, round, 2) %>% \n  knitr::kable() \n\n\n\n\ntopic\nSensitivity\nSpecificity\nBalanced.Accuracy\n\n\n\n\ndiabetes\n0.75\n1.00\n0.88\n\n\nheart_disease\n0.56\n0.99\n0.77\n\n\nkidney_disease\n0.89\n0.86\n0.88\n\n\nmental_health\n0.89\n0.96\n0.93\n\n\nobesity\n0.74\n0.93\n0.83\n\n\nsubstance_abuse\n0.70\n0.97\n0.83\n\n\n\n\n\n\n\n4.3 FastText classifier: Confusion matrix\n\ndp <- as.data.frame(conf_mats[['FastText']]$table)\n\nggplot(data = dp,\n       aes(x = Reference, y = Prediction)) +\n  \n    geom_tile(aes(fill = log(Freq)), \n              colour = \"white\") +\n    scale_fill_gradient(low = \"white\", \n                        high = \"steelblue\") +\n  \n    geom_text(data = dp,\n              aes(x = Reference, \n                  y = Prediction, \n                  label = Freq)) +\n    theme(legend.position = \"none\",\n          axis.text.x=element_text(angle=45,\n                                   hjust=1)) + \n    labs(title=\"Confusion Matrix\")"
  },
  {
    "objectID": "posts/text-class/index.html#summary",
    "href": "posts/text-class/index.html#summary",
    "title": "notes on text classification",
    "section": "5 Summary",
    "text": "5 Summary"
  }
]