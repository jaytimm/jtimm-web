[
  {
    "objectID": "posts/text-class/index.html#labeled-data",
    "href": "posts/text-class/index.html#labeled-data",
    "title": "notes on text classification",
    "section": "1 Labeled data",
    "text": "1 Labeled data\nFor demonstration purposes, we build a corpus using the quicknews package. The corpus is comprised of articles returned from a set of health-related queries. Search terms, then, serve as classification labels. An imperfect annotation process, but fine for our purposes here. As “distant” supervision.\n\nlibrary(tidyverse)\n\nterms <- c('heart disease', \n           'diabetes',\n           'mental health',\n           'substance abuse',\n           'obesity',\n           'kidney disease')\n\nrss <- lapply(terms, function(x) {\n  \n  quicknews::qnews_build_rss(x) %>%\n    quicknews::qnews_strip_rss() }) \n\nnames(rss) <- terms\n  \nrss0 <- rss %>%\n  bind_rows(.id = 'term') %>%\n  mutate(term = gsub(' ', '_', term)) %>%\n  distinct(link, .keep_all = TRUE) %>%\n  mutate(doc_id = as.character(row_number())) %>%\n  mutate(term = as.factor(term))  %>%\n  select(doc_id, term:link)\n\n\narticles <- quicknews::qnews_extract_article(url = rss0$link, cores = 7) \narticles0 <- articles %>% left_join(rss0)\n\nDescriptives for the resulting corpus by search term. So, a super small demo corpus.\n\narticles0 %>%\n  mutate(words = tokenizers::count_words(text)) %>%\n  group_by(term) %>%\n  summarize(n = n(), words = sum(words)) %>%\n  janitor::adorn_totals() %>%\n  knitr::kable()\n\n\n\n\nterm\nn\nwords\n\n\n\n\ndiabetes\n81\n72045\n\n\nheart_disease\n82\n65505\n\n\nkidney_disease\n80\n68379\n\n\nmental_health\n82\n73804\n\n\nobesity\n81\n69179\n\n\nsubstance_abuse\n84\n64133\n\n\nTotal\n490\n413045\n\n\n\n\n\nA sample of articles from the GoogleNews/quicknews query:\n\nset.seed(99)\narticles0 %>% \n  select(term, date, source, title) %>% \n  sample_n(5) %>%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\nterm\ndate\nsource\ntitle\n\n\n\n\nkidney_disease\n2022-05-23\nReuters.com\nU.S. Task Force to consider routine kidney disease screening as new drugs available\n\n\nsubstance_abuse\n2022-06-15\nThe Sylva Herald\nDistrict Court | Court News | thesylvaherald.com\n\n\nsubstance_abuse\n2022-05-28\nWUSF News\nNortheast Florida’s health care concerns: from substance abuse to food deserts\n\n\nheart_disease\n2022-05-23\nBloomberg\nTake Breaks and Watch Less TV to Slash Heart Disease Risk, Experts Say\n\n\nsubstance_abuse\n2022-06-14\nWilliston Daily Herald\nReport: U.S. records highest ever rates of substance abuse, suicide-related deaths"
  },
  {
    "objectID": "posts/text-class/index.html#data-structures",
    "href": "posts/text-class/index.html#data-structures",
    "title": "notes on text classification",
    "section": "2 Data structures",
    "text": "2 Data structures\n\n2.1 Document-Term Matrix\n\nAs bag-of-words\n\n\ndtm <- articles0 %>% \n  mutate(wds = tokenizers::count_words(text)) %>%\n  filter(wds > 200 & wds < 1500) %>%\n  text2df::tif2token() %>%\n  text2df::token2df() %>%\n  mutate(token = tolower(token)) \n  # mutate(stem = quanteda::char_wordstem(token))\n\ndtm %>% head() %>% knitr::kable()\n\n\n\n\ndoc_id\ntoken\ntoken_id\n\n\n\n\n2\nheart\n1\n\n\n2\ndisease\n2\n\n\n2\nis\n3\n\n\n2\na\n4\n\n\n2\nvery\n5\n\n\n2\ngeneral\n6\n\n\n\n\n\n\ndtm_tok <- dtm %>%  \n  count(doc_id, token) %>%\n  group_by(token) %>%\n  mutate(docf = length(unique(doc_id))) %>% ungroup() %>%\n  mutate(docf = round(docf/length(unique(doc_id)), 3 )) %>%\n  filter(docf >= 0.01 & docf < 0.5 & \n           !grepl('^[0-9]|^[[:punct:]]', token))\n\ndtm_tok %>% head() %>% knitr::kable()\n\n\n\n\ndoc_id\ntoken\nn\ndocf\n\n\n\n\n10\nacknowledge\n1\n0.031\n\n\n10\nacross\n1\n0.207\n\n\n10\nadditional\n1\n0.151\n\n\n10\naddress\n3\n0.217\n\n\n10\nadhere\n1\n0.018\n\n\n10\nads\n1\n0.026\n\n\n\n\n\n\ndtm_sparse <- dtm_tok %>%\n  tidytext::bind_tf_idf(term = token, \n                        document = doc_id,\n                        n = n) %>% \n  tidytext::cast_sparse(row = doc_id, \n                        column = token, \n                        value = tf_idf)\n\n\n\n2.2 Cleaned text\n\nctext <- dtm %>%\n  group_by(doc_id) %>%\n  summarize(text = paste0(token, collapse = ' ')) %>% ungroup()\n\nstrwrap(ctext$text[5], width = 60)[1:5]\n\n[1] \"upon receiving a diagnosis of type 1 diabetes ( t1d ) ,\"    \n[2] \"many people have the same reaction : “ but why me ? ” some\" \n[3] \"people have t1d that runs in their family , while others\"   \n[4] \"have no idea how or why they received a diagnosis . often ,\"\n[5] \"to their frustration , those questions go unanswered . but\" \n\n\n\n\n2.3 Word embeddings\n\n## devtools::install_github(\"pommedeterresautee/fastrtext\") \ntmp_file_txt <- tempfile()\ntmp_file_model <- tempfile()\nwriteLines(text = ctext$text, con = tmp_file_txt)\ndims <- 25\nwindow <- 5\n\nfastrtext::execute(commands = c(\"skipgram\",\n                                \"-input\", tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", gsub('^.*\\\\.', '', dims),\n                                \"-ws\", window, \n                                \"-verbose\", 1))\n\n\nRead 0M words\nNumber of words:  5318\nNumber of labels: 0\n\nProgress: 100.0% words/sec/thread:  121781 lr:  0.000000 avg.loss:  2.421614 ETA:   0h 0m 0s\n\nfast.model <- fastrtext::load_model(tmp_file_model)\n\nadd .bin extension to the path\n\nfast.dict <- fastrtext::get_dictionary(fast.model)\nembeddings <- fastrtext::get_word_vectors(fast.model, fast.dict)"
  },
  {
    "objectID": "posts/text-class/index.html#classifiers",
    "href": "posts/text-class/index.html#classifiers",
    "title": "notes on text classification",
    "section": "3 Classifiers",
    "text": "3 Classifiers\n\narticles1 <- articles0 %>%\n  arrange(doc_id) %>%\n  filter(doc_id %in% unique(dtm_tok$doc_id))\n\n\nset.seed(99)\ntrainIndex <- caret::createDataPartition(articles1$term, p = .7)$Resample1\n\n\n3.1 Bag-of-words & Naive Bayes\n\nDocument represented as bag-of-words.\n\n\ndtm_train <- dtm_sparse[trainIndex, ]\ndtm_test <- dtm_sparse[-trainIndex, ] \ndtm_classifier <- e1071::naiveBayes(as.matrix(dtm_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\ndtm_predicted <- predict(dtm_classifier, as.matrix(dtm_test))\n\n\n\n3.2 Word embeddings & Naive Bayes\n\nDocument represented as an aggregate (here, mean) of constituent word embeddings. Custom/FastText word embeddings derived from quicknews corpus (above).\n\n\nv1 <- embeddings %>% \n  data.frame() %>%\n  mutate(token = rownames(embeddings)) %>%\n  filter(token %in% unique(dtm_tok$token)) %>%\n  inner_join(dtm)\n\navg0 <- lapply(unique(dtm$doc_id), function(y){\n  \n  d0 <- subset(v1, doc_id == y)\n  d1 <- as.matrix(d0[, 1:dims])\n  d2 <-Matrix.utils::aggregate.Matrix(d1,\n                                      groupings = rep(y, nrow(d0)),\n                                      fun = 'mean')\n  as.matrix(d2)\n})\n\ndoc_embeddings <- do.call(rbind, avg0)\n\n\nemb_train <- doc_embeddings[trainIndex, ]\nemb_test <- doc_embeddings[-trainIndex, ] \nemb_classifier <- e1071::naiveBayes(as.matrix(emb_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\nemb_predicted <- predict(emb_classifier, as.matrix(emb_test))\n\n\n\n3.3 FastText classifier\n\nfast_train <- articles1[trainIndex, ]\nfast_test <- articles1[-trainIndex, ]\n\nPrepare data for FastText:\n\ntmp_file_model <- tempfile()\n\ntrain_labels <- paste0(\"__label__\", fast_train$term)\ntrain_texts <- tolower(fast_train$text)\ntrain_to_write <- paste(train_labels, train_texts)\ntrain_tmp_file_txt <- tempfile()\nwriteLines(text = train_to_write, con = train_tmp_file_txt)\n\ntest_labels <- paste0(\"__label__\", fast_test$term)\ntest_texts <- tolower(fast_test$text)\ntest_to_write <- paste(test_labels, test_texts)\n\n\nfastrtext::execute(commands = c(\"supervised\", \n                                \"-input\", train_tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", 25, \n                                \"-lr\", 1, \n                                \"-epoch\", 20, \n                                \"-wordNgrams\", 2, \n                                \"-verbose\", 1))\n\nmodel <- fastrtext::load_model(tmp_file_model)\nfast_predicted0 <- predict(model, sentences = test_to_write)\nfast_predicted <- as.factor(names(unlist(fast_predicted0)))"
  },
  {
    "objectID": "posts/text-class/index.html#evaluation",
    "href": "posts/text-class/index.html#evaluation",
    "title": "notes on text classification",
    "section": "4 Evaluation",
    "text": "4 Evaluation\n\npredictions <- list('BOW' = dtm_predicted, \n                    'Word embeddings' = emb_predicted, \n                    'FastText' = fast_predicted)\n\n\n4.1 Model accuracy\n\nconf_mats <- lapply(predictions, \n                    caret::confusionMatrix,\n                    reference = articles1[-trainIndex, ]$term)\n\nsums <- lapply(conf_mats, '[[', 3) \nsums0 <- as.data.frame(do.call(rbind, sums)) %>%\n  select(1:4) %>%\n  mutate_at(1:4, round, 3)\n\nsums0 %>% arrange(desc(Accuracy)) %>% knitr::kable()\n\n\n\n\n\nAccuracy\nKappa\nAccuracyLower\nAccuracyUpper\n\n\n\n\nFastText\n0.757\n0.708\n0.668\n0.832\n\n\nBOW\n0.661\n0.593\n0.567\n0.747\n\n\nWord embeddings\n0.452\n0.341\n0.359\n0.548\n\n\n\n\n\n\n\n4.2 FastText classifier: Model accuracy by class\n\nconf_mats[['FastText']]$byClass %>% data.frame() %>%\n  select (Sensitivity, Specificity, Balanced.Accuracy) %>%\n  rownames_to_column(var = 'topic') %>%\n  mutate(topic = gsub('Class: ','', topic)) %>% \n  mutate_if(is.numeric, round, 2) %>% \n  knitr::kable() \n\n\n\n\ntopic\nSensitivity\nSpecificity\nBalanced.Accuracy\n\n\n\n\ndiabetes\n0.75\n1.00\n0.88\n\n\nheart_disease\n0.56\n0.99\n0.77\n\n\nkidney_disease\n0.89\n0.86\n0.88\n\n\nmental_health\n0.89\n0.96\n0.93\n\n\nobesity\n0.74\n0.93\n0.83\n\n\nsubstance_abuse\n0.70\n0.97\n0.83\n\n\n\n\n\n\n\n4.3 FastText classifier: Confusion matrix\n\ndp <- as.data.frame(conf_mats[['FastText']]$table)\n\nggplot(data = dp,\n       aes(x = Reference, y = Prediction)) +\n  \n    geom_tile(aes(fill = log(Freq)), \n              colour = \"white\") +\n    scale_fill_gradient(low = \"white\", \n                        high = \"steelblue\") +\n  \n    geom_text(data = dp,\n              aes(x = Reference, \n                  y = Prediction, \n                  label = Freq)) +\n    theme(legend.position = \"none\",\n          axis.text.x=element_text(angle=45,\n                                   hjust=1)) + \n    labs(title=\"Confusion Matrix\")"
  },
  {
    "objectID": "posts/text-class/index.html#summary",
    "href": "posts/text-class/index.html#summary",
    "title": "notes on text classification",
    "section": "5 Summary",
    "text": "5 Summary"
  },
  {
    "objectID": "posts/cracking-and-packing/index.html",
    "href": "posts/cracking-and-packing/index.html",
    "title": "cracking and packing",
    "section": "",
    "text": "A brief note on gerrymandering, and cracking & packing. Specifically, a simple simulation demonstrating how gross partisan asymmetries in the composition of state legislatures can be crafted from statewide populations evenly split between two parties.\nPer function below, we designate individuals in a population of N voters as either Republican or Democrat (50-50 split). Then we randomly assign each voter a district."
  },
  {
    "objectID": "posts/cracking-and-packing/index.html#example-1",
    "href": "posts/cracking-and-packing/index.html#example-1",
    "title": "cracking and packing",
    "section": "1 Example #1",
    "text": "1 Example #1\nWe simulate 1,000 election results for a state with a population of 10,000, evenly distributed across 10 legislative districts. Here, a comparison of two of these elections.\n\nset.seed(999)\nf1a <- lapply(1:1000, function(x) {simulate_election(state_pop = 10000,\n                                                     district_n = 10,\n                                                     dem_prop = .50)}) %>%\n  bind_rows(.id = 'vote') \n\nIn the first, per the vote tally below, Democrats won 5/10 seats in the legislature. A reasonable result in a state split 50-50 among Ds and Rs.\n\nf1a %>% \n  filter(dseats == 5) %>% \n  slice(1:10) %>%\n  select(district, dwin, D, R) %>%\n  mutate(Total = R + D) %>%\n  janitor::adorn_totals(where = c('row')) %>%\n  knitr::kable()\n\n\n\n\ndistrict\ndwin\nD\nR\nTotal\n\n\n\n\n1\n0\n497\n503\n1000\n\n\n2\n0\n472\n528\n1000\n\n\n3\n1\n506\n494\n1000\n\n\n4\n1\n508\n492\n1000\n\n\n5\n0\n486\n514\n1000\n\n\n6\n1\n521\n479\n1000\n\n\n7\n0\n499\n501\n1000\n\n\n8\n1\n500\n500\n1000\n\n\n9\n0\n498\n502\n1000\n\n\n10\n1\n513\n487\n1000\n\n\nTotal\n5\n5000\n5000\n10000\n\n\n\n\n\nIn the second, Democrats won 8/10 seats, despite the statewide 50-50 split. Clearly a preferable outcome for Democrats. How did they do it? Well, election results show that Dems won lots of seats by very slim margins in the simulation – maxing out at only 511 votes in districts 2 and 6. Republicans, on the other hand, won only two seats (districts 1 & 10); however, both by more sizable margins.\n\nf1a %>% \n  filter(dseats == 8) %>% \n  slice(1:10) %>%\n  select(district, dwin, D, R) %>%\n  mutate(Total = R + D) %>%\n  janitor::adorn_totals(where = c('row')) %>%\n  knitr::kable()\n\n\n\n\ndistrict\ndwin\nD\nR\nTotal\n\n\n\n\n1\n0\n483\n517\n1000\n\n\n2\n1\n511\n489\n1000\n\n\n3\n1\n503\n497\n1000\n\n\n4\n1\n502\n498\n1000\n\n\n5\n1\n506\n494\n1000\n\n\n6\n1\n511\n489\n1000\n\n\n7\n1\n505\n495\n1000\n\n\n8\n1\n504\n496\n1000\n\n\n9\n1\n502\n498\n1000\n\n\n10\n0\n473\n527\n1000\n\n\nTotal\n8\n5000\n5000\n10000\n\n\n\n\n\nSo, if we were to ascribe some agency to how individuals in our simulation were assigned to districts, and perhaps call it a “decennial redistricting panel,” for example, we would say that they created district boundaries such that Republicans did not have the numbers to win in most of the state’s districts, and achieved this by creating two districts (1 & 10) in which Republicans were many. The first part of this plan is traditionally called “cracking”; the second, “packing”."
  },
  {
    "objectID": "posts/cracking-and-packing/index.html#example-2",
    "href": "posts/cracking-and-packing/index.html#example-2",
    "title": "cracking and packing",
    "section": "2 Example #2",
    "text": "2 Example #2\nA closer look at cracking and packing, then. Parameters of our new simulation include a population of 10,000 and a legislative body comprised of 20 districts. The histogram below summarizes counts of seats won by Democrats based on election results from 1,000 simulations. The most common outcome is Democrats winning 10/20 seats. The most favorable outcome for Democrats is 14/20 seats; however, this outcome occurs in less than 0.5% of simulations.\n\nset.seed(123)\nf1 <- lapply(1:1000, function(x) {\n  simulate_election(state_pop = 10000,\n                    district_n = 20,\n                    dem_prop = .50)}) %>%\n  bind_rows(.id = 'vote')\n\nf1 %>% \n  group_by(vote) %>% \n  summarise(D = sum(dwin)) %>%\n  ggplot() +\n  geom_histogram(aes(D), binwidth = .5) +\n  scale_x_continuous(breaks=seq(min(f1$dseats), max(f1$dseats), 1))+\n  ggtitle('Dem seats won in 1K simulated elections')\n\n\n\n\nThe next plot summarizes the vote distribution for a simulated election in which Dems won 14 seats; districts have been sorted in increasing order of Republican vote share. The fourteen seats won by Dems are denoted in blue; Rs in red.\n\nset.seed(99)\nf1 %>%\n  mutate(rank = rank(r, ties.method = 'first'),\n         party = ifelse(r > 50, 'r', 'd')) %>%\n  filter(dseats == max(dseats)) %>%\n  group_by(vote) %>% nest() %>% ungroup() %>%\n  sample_n(1) %>% unnest(cols = c(data)) %>%\n  \n  ggplot(aes(x = factor(rank), \n                 y = r,\n                 color = party)) + \n  geom_point(size = 2) +\n  geom_hline(yintercept = 50, lty = 3) +\n \n  ylim(40, 60) +\n  scale_color_manual(values = c('#437193', '#ae4952')) +\n  \n  theme_minimal() +\n  theme(axis.text.x=element_blank(),\n        legend.position = 'none') +\n  xlab('Districts ordered from least to most Republican') +\n  ylab('Percentage of votes for a Republican') \n\n\n\n\nAs can be noted, at the 50% threshold, the slope of this vote distribution shows a marked increase, with Republicans garnering higher vote shares for the six seats they won in comparison to vote shares garnered by Dems for their fourteen seats. So, lots of mis-spent votes for Republicans. This is generally what crack-and-pack gerrymandering looks like (see, eg, Warrington 2018).\nA cleaner vote distribution – results from a simulated election in which Dems won 10/20 seats."
  },
  {
    "objectID": "posts/bert/index.html",
    "href": "posts/bert/index.html",
    "title": "BERT, reticulate & lexical semantics",
    "section": "",
    "text": "This post provides some quick details on using reticulate to interface Python from RStudio; and, more specifically, using the spacy library and BERT for fine-grained lexical semantic investigation. Here we present a (very cursory) usage-based/BERT-based perspective on the semantic distinction between further and farther, using example contexts extracted from the Corpus of Contemporary American English (COCA)."
  },
  {
    "objectID": "posts/bert/index.html#python-reticulate-set-up",
    "href": "posts/bert/index.html#python-reticulate-set-up",
    "title": "BERT, reticulate & lexical semantics",
    "section": "1 Python & reticulate set-up",
    "text": "1 Python & reticulate set-up\nThe Python code below sets up a conda environment and installs relevant libraries, as well as the BERT transformer, en_core_web_trf. The following should be run in the terminal.\n\nconda create -n poly1\nsource activate poly1\nconda install -c conda-forge spacy\npython -m spacy download en_core_web_trf\nconda install numpy scipy pandas\n\nThe R code below directs R to our Python environment and Python installation.\n\nSys.setenv(RETICULATE_PYTHON = \"/home/jtimm/anaconda3/envs/m3demo/bin/python\")\n\nlibrary(reticulate)\nreticulate::use_condaenv(condaenv = \"poly1\",\n                         conda = \"/home/jtimm/anaconda3/bin/conda\")"
  },
  {
    "objectID": "posts/bert/index.html#coca",
    "href": "posts/bert/index.html#coca",
    "title": "BERT, reticulate & lexical semantics",
    "section": "2 COCA",
    "text": "2 COCA\nThe Corpus of Contemporary American English (COCA) is an absolutely lovely resource, and is one of many corpora made available by the folks at BYU. Here, we utilize COCA to build a simple data set of further-farther example usages. I have copied/pasted from COCA’s online search interface – the data set includes ~500 contexts of usage per form.\n\nlibrary(tidyverse)\ngw <- read.csv(paste0(ld, 'further-farther.csv'), sep = '\\t')\ngw$sent <- tolower(gsub(\"([[:punct:]])\", \" \\\\1 \", gw$text))\ngw$sent <- gsub(\"^ *|(?<= ) | *$\", \"\", gw$sent, perl = TRUE)\n\ngw$count <- stringr::str_count(gw$sent, 'further|farther')\ngw0 <- subset(gw, count == 1)\n\nFor a nice discussion on the semantics of further-farther, see this Merriam-Webster post. The standard semantic distinction drawn between the two forms is physical versus metaphorical distance.\nSome highlighting & sample data below.\n\nfu <- '\\\\1 <span style=\"background-color:lightgreen\">\\\\2</span> \\\\3'\nfa <- '\\\\1 <span style=\"background-color:lightblue\">\\\\2</span> \\\\3'\n\ngw0$text <- gsub('(^.+)(further)(.+$)', fu, gw0$text, ignore.case = T)\ngw0$text <- gsub('(^.+)(farther)(.+$)', fa, gw0$text, ignore.case = T)\ngw0$text <- paste0('... ', gw0$text, ' ...')\n\nset.seed(99)\ngw0 %>% select(year, genre, text) %>% sample_n(10) %>% \n  DT::datatable(rownames = F, escape = F,\n                options = list(dom = 't',\n                               pageLength = 10,\n                               scrollX = TRUE))\n\n\n\n\n\n\nLastly, we identify the location (ie, context position) of the target token within each context (as token index).\n\ngw0$idx <- sapply(gsub(' (farther|further).*$', '', gw0$sent, ignore.case = T), \n                  function(x){\n                    length(corpus::text_tokens(x)[[1]]) })"
  },
  {
    "objectID": "posts/bert/index.html#bert-contextual-embeddings",
    "href": "posts/bert/index.html#bert-contextual-embeddings",
    "title": "BERT, reticulate & lexical semantics",
    "section": "3 BERT & contextual embeddings",
    "text": "3 BERT & contextual embeddings\nUsing BERT and spacy for computing contextual word embeddings is actually fairly straightforward. A very nice resource for some theoretical overview as well as code demo with BERT/spacy is available here.\nGetting started, we pass our data set from R to Python via the r_to_py function.\n\ndf <- reticulate::r_to_py(gw0)\n\nThen, from a Python console, we load the BERT transformer using spacy.\n\nimport spacy\nnlp = spacy.load('en_core_web_trf')\n\nThe stretch of Python code below does all the work here. The transformer computes a 768 dimension vector per token/sub-token comprising each context – then we extract the tensor for either further/farther using the token index. The resulting data structure is matrix-like, with each instantiation of further-farther represented in 768 dimensions.\n\ndef encode(sent, index):\n  doc = nlp(sent.lower())\n  tensor_ix = doc._.trf_data.align[index].data.flatten()\n  out_dim = doc._.trf_data.tensors[0].shape[-1]\n  tensor = doc._.trf_data.tensors[0].reshape(-1, out_dim)[tensor_ix]\n  ## tensor.__len__()\n  return tensor.mean(axis=0)\n\nr.df[\"emb\"] = r.df[[\"sent\", \"idx\"]].apply(lambda x: encode(x[0], x[1]), axis = 1)"
  },
  {
    "objectID": "posts/bert/index.html#tsne",
    "href": "posts/bert/index.html#tsne",
    "title": "BERT, reticulate & lexical semantics",
    "section": "4 tSNE",
    "text": "4 tSNE\nTo plot these contexts in two dimensions, we use tSNE to reduce the 768-dimension word embeddings to two. Via Python and numpy, we create a matrix-proper from the further-farther token embeddings extracted above.\n\nimport numpy as np\nX, y  = r.df[\"emb\"].values, r.df[\"id\"].values\nX = np.vstack(X)\n\nFor good measure, we switch back to R to run tSNE. The matrix X, built in Python, is accessed in the R console below via reticulate::py$X.\n\nset.seed(999) ## \ntsne <- Rtsne::Rtsne(X = as.matrix(reticulate::py$X), \n                     check_duplicates = FALSE)\n\ntsne_clean <- data.frame(reticulate::py_to_r(df), tsne$Y) %>%\n  \n  mutate(t1 = gsub('(further|farther)', '\\\\<\\\\1\\\\>', text, ignore.case = T),\n         t2 = stringr::str_wrap(string = t1,\n                                  width = 20,\n                                  indent = 1,\n                                  exdent = 1),\n         id = row_number()) %>%\n  select(id, form, X1, X2, t1, t2) \n\nThe scatter plot below summarizes contextual embeddings for individual tokens of further-farther. So, a nice space for further used adjectivally on the right side of the plot. Other spaces less obviously structured, and some confused spaces as well where speakers seem to have quite a bit of leeway.\n\np <- ggplot2::ggplot(tsne_clean, \n                          aes(x = X1, \n                              y = X2,\n                              color = form,\n                              text = t2,\n                              key = id )) + \n  \n  geom_hline(yintercept = 0, color = 'gray') +\n  geom_vline(xintercept = 0, color = 'gray') +\n  \n  geom_point(alpha = 0.5) +\n  theme_minimal() +\n  ggthemes::scale_colour_economist() +\n  ggtitle('further-farther') \n\nplotly::ggplotly(p,  tooltip = 'text') \n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nℹ Please use `gather()` instead.\nℹ The deprecated feature was likely used in the plotly package.\n  Please report the issue at <\u001b]8;;https://github.com/plotly/plotly.R/issues\u0007https://github.com/plotly/plotly.R/issues\u001b]8;;\u0007>."
  },
  {
    "objectID": "posts/bert/index.html#summary",
    "href": "posts/bert/index.html#summary",
    "title": "BERT, reticulate & lexical semantics",
    "section": "5 Summary",
    "text": "5 Summary\nSo, some notes on reticulate and Python environments, and spacy and BERT. While a computational beast, BERT seems fantastically suited for more fine-grained, qualitative semantic analyses and case studies, and lexicography in general."
  },
  {
    "objectID": "posts/seven-generations/index.html",
    "href": "posts/seven-generations/index.html",
    "title": "the seven living generations in america",
    "section": "",
    "text": "A quick look at the composition of American generations. Per Pew Research definitions & US Census data."
  },
  {
    "objectID": "posts/seven-generations/index.html#american-generations",
    "href": "posts/seven-generations/index.html#american-generations",
    "title": "the seven living generations in america",
    "section": "1 American generations",
    "text": "1 American generations\n\nlibrary(tidyverse)\ngen <- c('Post-Z', 'Gen Z', 'Millennial', \n         'Gen X', 'Boomers', 'Silent', \n         'Greatest')\n\nrange <- c('> 2012', '1997-2012', '1981-1996', \n           '1965-1980', '1946-1964', '1928-1945', \n           '< 1927')\n\ngen_desc <- data.frame(rank = 7:1,\n                       gen = gen,\n                       range = range,\n                       stringsAsFactors = FALSE) %>%\n  arrange(rank)\n\nA summary:\n\n\n\n\n\n\n\nFour of America’s seven living generations are more or less “complete,” and only getting smaller (albeit at different rates): Greatest, Silent, Boomers, and Gen X. The generation comprised of Millenials is complete as well, in that it has been delineated chronologically; however, the group likely continues to grow via immigration.\nWhile Gen Z has been tentatively stamped chronologically by the folks at Pew Research, only the very eldest in the group have just entered the work force. So lots can happen still. And although we include them here, the Post-Z generation is mostly but a thought; half of the group has yet to be born."
  },
  {
    "objectID": "posts/seven-generations/index.html#monthly-us-population-estimates",
    "href": "posts/seven-generations/index.html#monthly-us-population-estimates",
    "title": "the seven living generations in america",
    "section": "2 Monthly US population estimates",
    "text": "2 Monthly US population estimates\nMonthly Postcensal Resident Population plus Armed Forces Overseas, December 2018. Made available by the US Census here. The census has transitioned to a new online interface, and (seemingly) many data sets have been discontinued. Hence, the data set utilized here is slightly dated.\n\npops <- read.csv (\n  url('https://www2.census.gov/programs-surveys/popest/datasets/2010-2018/national/asrh/nc-est2018-alldata-p-File18.csv')) %>%\n  filter(MONTH == '12' & YEAR == '2018') %>%\n  gather(key = 'race', value = 'pop', -UNIVERSE:-AGE)\n\nA more detailed description of the population estimates can be found here. Note: Race categories reflect non-Hispanic populations.\n\nrace <- c('NHWA', 'NHBA', 'NHIA', \n          'NHAA', 'NHNA', 'NHTOM', 'H')\n\nrace1 <- c('White Alone',\n           'Black Alone',\n           'American Indian Alone',\n           'Asian Alone',\n           'Native Hawaiian Alone',\n           'Two or More Races',\n           'Hispanic')\n\nlabels <- data.frame(race = race, \n                     race1=race1, \n                     stringsAsFactors = FALSE)\n\nsearch <- paste(paste0('^',race, '_'), collapse =  '|')\n\nThe following table details a random sample of the data set – with Pew Research defined generations & estimated year-of-birth.\n\ngen_pops <- pops %>%\n  filter(grepl(search, race)) %>%\n  mutate(race = gsub('_.*$', '', race)) %>%\n  group_by(AGE, race) %>%\n  summarise(pop = sum(pop))%>%\n  left_join(labels) %>%\n  filter(AGE != '999') %>%\n  mutate(yob = 2019 - AGE)  %>% ##\n  mutate (gen = case_when (\n    yob < 2013 & yob > 1996 ~ 'Gen Z',\n    yob < 1997 & yob > 1980 ~ 'Millennial',\n    yob < 1981 & yob > 1964 ~ 'Gen X',\n    yob < 1965 & yob > 1945 ~ 'Boomers',\n    yob < 1946 & yob > 1927 ~ 'Silent',\n    yob < 1928 ~ 'Greatest',\n    yob > 2012 ~ 'Post-Z')) %>%\n  left_join(gen_desc) %>%\n  ungroup() %>%\n  select(gen, rank, range, race, \n         race1, yob, AGE, pop)\n\nset.seed(999)\ngen_pops %>% sample_n(7)  %>%\n  select(gen, range, race1:pop) %>%\n  DT::datatable(rownames = FALSE, options = list(dom = 't',\n                                                 scrollX = TRUE))"
  },
  {
    "objectID": "posts/seven-generations/index.html#composition-of-american-generations",
    "href": "posts/seven-generations/index.html#composition-of-american-generations",
    "title": "the seven living generations in america",
    "section": "3 Composition of American generations",
    "text": "3 Composition of American generations\n\n3.1 Population by generation\nThe figure below summarizes the US population by generation. These numbers will vary some depending on the data source. Millenials constitute the plurality of Americans, more recently overtaking a Boomer generation on the wane.\n\ngen_pops %>%\n  group_by(gen, rank) %>%\n  summarize(pop = sum(pop)) %>%\n  mutate(lab = round(pop/1000000, 1)) %>%\n  ggplot(aes(x = reorder(gen, rank), \n             y = pop, \n             fill = gen)) +\n  geom_col(show.legend = FALSE, \n           alpha = 0.75)  +\n  geom_text(aes(label = lab), \n            size = 3.5)+\n  theme(axis.text.x=element_blank(),\n        axis.ticks.x=element_blank())+\n  xlab('') + ylab('') +\n  coord_flip()+\n  ggthemes::scale_fill_stata() +\n  theme_minimal() +\n  labs(title = 'Population by American generation',\n       caption = 'SOURCE: US Census, Monthly Postcensal Resident Population plus Armed Forces Overseas, December 2018.')\n\n`summarise()` has grouped output by 'gen'. You can override using the `.groups`\nargument.\n\n\n\n\n\n\n\n3.2 Population by single year of age & generation\n\ngg <- gen_pops %>% \n  group_by(yob, AGE, gen) %>%\n  summarize(tot = sum(pop)) %>%\n  group_by(gen) %>%\n  mutate(tot = max(tot)) %>% #For labels below.\n  filter(yob %in% c('1919', '1928', '1946', '1965', \n                    '1981', '1997', '2013'))\n\n`summarise()` has grouped output by 'yob', 'AGE'. You can override using the\n`.groups` argument.\n\n\nThe figure below illustrates the US population by single year of age, ranging from the population aged less than a year to the population over 100 (as of December 2018). Generation membership per single year of age is specified by color.\n\ngen_pops %>%\n  ggplot(aes(x = AGE, \n             y = pop, \n             fill = gen)) +\n  geom_vline(xintercept = gg$AGE,\n             linetype =2, \n             color = 'gray', \n             size = .25)+\n  \n  geom_col(show.legend = FALSE, \n           alpha = 0.85,\n           width = .7)   +\n  annotate(geom=\"text\", \n           x = gg$AGE - 4.5, \n           y = gg$tot + 70000, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('Age')+ \n  ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank(),\n        panel.grid.minor.y=element_blank()) +\n  ggthemes::scale_fill_stata()+\n  scale_x_reverse(breaks = rev(gg$AGE)) +\n  labs(title = 'American population by single-year age & generation')\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n3.3 Population by single year of age, race & generation\n\ngen_pal <- c('#b0bcc1','#ead8c3', '#437193', \n             '#c66767', '#55752f', '#dae2ba', \n             '#7b9bb3')\n\nNext, we crosscut the single year of age counts presented above by race & ethnicity.\n\ngen_pops %>%\n  ggplot(aes(x = AGE, \n             y = pop, \n             fill = race1)) +\n  geom_area(stat = \"identity\",\n            color = 'white',\n            alpha = 0.85) +\n  scale_fill_manual(values = gen_pal) +\n  geom_vline(xintercept = gg$AGE,\n             linetype =2, color = 'gray', size = .25)+\n  annotate(geom=\"text\", \n           x = gg$AGE - 4.5, \n           y = gg$tot + 70000, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('')+ ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank(),\n        panel.grid.minor.y=element_blank()) +\n  \n  scale_x_reverse(breaks = rev(gg$AGE) )+\n  labs(title ='American population by age, race & generation')\n\n\n\n\n\n\n3.4 White America on the wane\n\nwhite_label <- gen_pops %>% \n  group_by(gen, AGE) %>%\n  mutate(per = pop/sum(pop))%>%\n  filter(race1 == 'White Alone') %>%\n  group_by(gen) %>%\n  mutate(per = max(per)) %>% #For labels below.\n  arrange(yob) %>%\n  filter(yob %in% c('1919', '1928', '1946', '1965', \n                    '1981', '1997', '2013'))\n\nThe last figure illustrates a proportional perspective of race & ethnicity in America by single year of age. Per figure, generational differences (at a single point in time) can shed light on (the direction of) potential changes in the overall composition of a given populace. As well as a view of what that populace may have looked like in the past.\n\ngen_pops %>%\n  group_by(gen, AGE) %>%\n  mutate(per = pop/sum(pop)) %>%\n  ggplot(aes(x = (AGE), \n             y = per, \n             fill = race1)) +\n  geom_area(stat = \"identity\",\n            color = 'white',\n            alpha = 0.85) +\n  geom_hline(yintercept = .5, \n             linetype = 4,\n             color = 'white') +\n  scale_fill_manual(values = gen_pal) +\n  geom_vline(xintercept = gg$AGE,\n             linetype = 2, \n             color = 'gray', \n             size = .25)+\n  annotate(geom=\"text\", \n           x = gg$AGE-4.5, \n           y = white_label$per - .05, \n           label = gg$gen,\n           size = 3.25) +\n  xlab('')+ ylab('') +\n  theme_minimal() +\n  theme(legend.position=\"bottom\",\n        legend.title = element_blank(),\n        panel.grid.major.x=element_blank(),\n        panel.grid.minor.x=element_blank()) +\n  \n  scale_x_reverse(breaks = rev(gg$AGE)) +\n  labs(title = 'American population by age, race & generation')"
  },
  {
    "objectID": "posts/seven-generations/index.html#summary",
    "href": "posts/seven-generations/index.html#summary",
    "title": "the seven living generations in america",
    "section": "4 Summary",
    "text": "4 Summary\nSome different perspectives on the composition of America & American generations."
  },
  {
    "objectID": "posts/daily-potus/index.html#dailypotus",
    "href": "posts/daily-potus/index.html#dailypotus",
    "title": "dailypotus: an update",
    "section": "1 dailypotus",
    "text": "1 dailypotus\n\ndevtools::install_github(\"jaytimm/dailypotus\")\n\nThe two functions are at work below. The daily_wiki_trump returns a table of executive branch happenings from 20 Jan 2017 to 20 Jan 2021. The daily_wiki_biden function returns an up-to-date (and identically structured) table for the ongoing Biden Presidency.\n\nx45.46 <- dailypotus::daily_wiki_trump() |>\n  rbind(dailypotus::daily_wiki_biden())\n\nTable structure is detailed some below. The folks at Wikipedia have delineated events via bullet points per each day of the last two presidencies, which have been enumerated here in the bullets column as independent rows.\n\nlibrary(dplyr)\nx45.46 |>\n  select(-quarter:-daypres, -dow) |>\n  head() |>\n  DT::datatable(rownames = FALSE, \n                options = list(dom = 't',\n                               pageLength = 6,\n                               scrollX = TRUE))"
  },
  {
    "objectID": "posts/daily-potus/index.html#a-simple-use-case",
    "href": "posts/daily-potus/index.html#a-simple-use-case",
    "title": "dailypotus: an update",
    "section": "2 A simple use-case",
    "text": "2 A simple use-case\n\nsummary <- x45.46 |>\n  filter(!is.na(Events)) |>\n  mutate(tokens = tokenizers::count_words(Events)) |>\n  group_by(date) |>\n  mutate(daily_count = sum(tokens)) |>\n  slice(1) |>\n  ungroup()\n\nFor demonstration, we calculate total word counts of event descriptions per day. The plot below, then, summarizes these daily word counts since Jan 2017. The plot itself is fairly meaningless, but the hover-action should be useful (to contextualize other data points, eg). For clarity purposes, only the first event for each day is included in the pop-up.\n\nsummary |>\n  mutate(text = stringr::str_wrap(string = Events,\n                                  width = 20,\n                                  indent = 1,\n                                  exdent = 1)) |>\n  \n  plotly::plot_ly(x = ~date, \n                  y = ~daily_count,\n                  color = ~pres,\n                  text = ~text,\n                  type = 'scatter',\n                  mode = 'lines') |>\n  \n  plotly::layout(#atitle = \"Top 10 Drug Types\", \n                 tooltip = c('Events'),\n                 yaxis = list (title = \"Daily event word count per Wikipedia\"))"
  },
  {
    "objectID": "posts/meadow/index.html",
    "href": "posts/meadow/index.html",
    "title": "My backyard meadow",
    "section": "",
    "text": "I had a vision for turning my mess of a backyard into a high desert meadow. So, I planted a mixture of native grasses and wildflowers. The former is comprised of 12 warm-season grasses; the latter a collection of 18 southwest annuals/perennials. Things grow for sure. But I don’t know what is what, or what is weed.\nI wanted an online reference guide specific to the 30 species of wildflower and grass in my meadow. And I wanted to build this programmatically, using a single block of code, based on a simple table of scientific/common names. While also having a table of contents, headers, etc in quarto.\n\n\n\ndf |> DT::datatable(df, rownames = F)"
  },
  {
    "objectID": "posts/meadow/index.html#building-guide",
    "href": "posts/meadow/index.html#building-guide",
    "title": "My backyard meadow",
    "section": "2 Building guide",
    "text": "2 Building guide\nProcess:\n\nCollect photos for each genus/species from Google using the photomoe package;\ncollect a sentence or two of species description from Wikipedia using the quicknews package; and\noutput everything in one fell swoop.\n\nThe code below is for the wildflower section; we do the same thing again for grasses. The trick, which you can’t see below, is the chunk option results = 'asis', which basically allows you to generate raw markdown by cat-ing and print-ing everything. See this section of the R Markdown Cookbook for a complete discussion.\n\nfor(j in 1:18){\n  ## get photos fro google -- build collage\n  link0 <- photomoe::img_get_gurls(df$scientific[j])\n  photomoe::img_download_images(link = link0, \n                                dir = tempdir(), \n                                prefix = df$scientific[j])\n\n  gg <- photomoe::img_build_collage(\n    paths = list.files(tempdir(), full.names = T),\n    dimx = 5,\n    dimy = 4,\n    prefix = df$scientific[j])\n  \n  ## get first p node from wikipedia\n  urls <- paste0('https://en.wikipedia.org/wiki/',\n               gsub(' ', '_', df$scientific[j]))\n\n  wko <- quicknews::get_site(urls) |>\n    subset(type == 'p' & nchar(text) > 3) |>\n    slice(1) |> \n    mutate(text = gsub('\\\\[[0-9]\\\\]', '', text))\n  \n  ##output\n  cat('\\n\\n### ', df$common[j], '\\n', '> ', \n      df$scientific[j], '\\n\\n')\n  par(bg = 'white', mar=c(0,0,0,0))\n  plot(gg)\n  if(nrow(wko) > 0){cat('\\n', '> ', wko$text)}\n  cat('\\n\\n ---')\n}"
  },
  {
    "objectID": "posts/meadow/index.html#wildflowers",
    "href": "posts/meadow/index.html#wildflowers",
    "title": "My backyard meadow",
    "section": "3 Wildflowers",
    "text": "3 Wildflowers\n\n3.1 Plains Coreopsis\n\nCoreopsis tinctoria\n\n\n\n\n\nPlains coreopsis, garden tickseed,golden tickseed, or calliopsis, Coreopsis tinctoria, is an annual forb. The plant is common in Canada (from Quebec to British Columbia), northeast Mexico (Coahuila, Nuevo León, Tamaulipas), and much of the United States, especially the Great Plains and Southern states where it is often called “calliopsis.” The species is also widely cultivated and naturalized in China.\n\n\n\n\n3.2 California Poppy\n\nEschscholzia\n\n\n\n\n\nEschscholzia /ɛˈʃɒltsiə/ is a genus of 12 annual or perennial plants in the Papaveraceae (poppy) family. The genus was named after the Baltic German/Imperial Russian botanist Johann Friedrich von Eschscholtz (1793–1831). All species are native to Mexico or the southern United States.\n\n\n\n\n3.3 Mexican Gold Poppy\n\nEschscholzia mexicana\n\n\n\n\n\nEschscholzia californica, the California poppy, golden poppy, California sunlight or cup of gold, is a species of flowering plant in the family Papaveraceae, native to the United States and Mexico. It is cultivated as an ornamental plant flowering in summer (spring in southern Australia), with showy cup-shaped flowers in brilliant shades of red, orange and yellow (occasionally pink and white). It is also used as food or a garnish. It became the official state flower of California in 1903.\n\n\n\n\n3.4 Indian Blanket\n\nGaillardia pulchella\n\n\n\n\n\nGaillardia pulchella (firewheel, Indian blanket, Indian blanketflower, or sundance) is a North American species of short-lived perennial or annual flowering plants in the sunflower family.\n\n\n\n\n3.5 Bird’s Eyes\n\nGilia tricolor\n\n\n\n\n\nIt is native to the Central Valley and foothills of the Sierra Nevada and Coast Ranges in California. Its native habitats include open, grassy plains and slopes below 2,000 feet (610 m).\n\n\n\n\n3.6 Blue Flax\n\nLinum perenne lewisii\n\n\n\n\n\nLinum lewisii (Linum perenne var. lewisii) (Lewis flax, blue flax or prairie flax) is a perennial plant in the family Linaceae, native to western North America from Alaska south to Baja California, and from the Pacific Coast east to the Mississippi River. It grows on ridges and dry slopes, from sea level in the north up to 11,000 feet (3,400 metres) in the Sierra Nevada.\n\n\n\n\n3.7 Tidy Tips\n\nLayia platyglossa\n\n\n\n\n\nTidytips was formerly found throughout low-elevation dry habitats in California including the Mojave Desert and into Arizona and Utah. In pre-European times this plant was common in solid stands at lower elevations. Found in grassy valley floors, slopes of hills, openings in coastal sage scrub and chaparral, coastal plains, and in the High Desert. A member of Spring wildflower ‘displays,’ blooming March to June.\n\n\n\n\n3.8 Arizona Lupine\n\nLupinus arizonicus\n\n\n\n\n\nLupinus arizonicus, the Arizona lupine, is a flowering plant in the legume family Fabaceae, native to the Mojave and Sonoran Deserts of North America, where it can be found growing in open places and sandy washes below 1,100 metres (3,600 ft) elevation. It is common around Joshua Tree National Park and Death Valley National Park in California.\n\n\n\n\n3.9 Arroyo Lupine\n\nLupinus succulentus\n\n\n\n\n\nIt is native to California, where it is common throughout much of the state, and adjacent sections of Arizona and Baja California. L. succulentus is known from many types of habitat and it can colonize disturbed areas.\n\n\n\n\n3.10 Blazing Star\n\nMentzelia lindleyi\n\n\n\n\n\nMentzelia lindleyi, commonly known as golden bartonia,Lindley’s blazingstar,evening star, or blazing star, is an annual wildflower of western North America.\n\n\n\n\n3.11 Five Spot\n\nNemophila maculata\n\n\n\n\n\nThe wildflower is found on slopes in elevations between 20–1,000 metres (66–3,281 ft). The plant is endemic to California. It is most common in the Sierra Nevada, Sacramento Valley, and the California Coast Ranges in the San Francisco Bay Area.\n\n\n\n\n3.12 White Evening Primrose\n\nOenothera pallida\n\n\n\n\n\n\n\n3.13 Showy Pink Evening Primrose\n\nOenothera speciosa\n\n\n\n\n\nOenothera speciosa is a species in the Oenothera (evening primrose) family known by several common names, including pinkladies, pink evening primrose, showy evening primrose, Mexican primrose, amapola, and buttercups (not to be confused with true buttercups in the genus Ranunculus).\n\n\n\n\n3.14 California Bluebell\n\nPhacelia campanularia\n\n\n\n\n\nPhacelia campanularia is a species of flowering plant in the borage family, Boraginaceae, known by the common names desertbells,desert bluebells,California-bluebell,desert scorpionweed, and desert Canterbury bells. Its true native range is within the borders of California, in the Mojave and Sonoran Deserts, but it is commonly cultivated as an ornamental plant and it can be found growing elsewhere as an introduced species.\n\n\n\n\n3.15 Mexican Hat\n\nRatibida columnifera\n\n\n\n\n\nRatibida columnifera, commonly known as upright prairie coneflower,Mexican hat, and longhead prairie coneflower, is a perennial species of flowering plant in the genus Ratibida in the family Asteraceae. It is native to much of North America and inhabits prairies, plains, roadsides, and disturbed areas from southern Canada through most of the United States to northern Mexico.\n\n\n\n\n3.16 Prairie Aster\n\nAster tanacetifolius\n\n\n\n\n\n\n\n3.17 Desert Marigold\n\nBaileya multiradiata\n\n\n\n\n\nBaileya multiradiata is a North American species of sun-loving wildflowers native to the deserts of northern Mexico and the Southwestern United States. It has been found in the States of Sonora, Chihuahua, Coahuila, Durango, Aguascalientes, California, Arizona, Nevada, Utah, New Mexico, and Texas.\n\n\n\n\n3.18 Farewell to Spring\n\nClarkia unguiculata\n\n\n\n\n\nClarkia unguiculata is a species of wildflower known by the common name elegant clarkia or mountain garland. This plant is endemic to California, where it is found in many woodland habitats. Specifically it is common on the forest floor of many oak woodlands, along with typical understory wildflowers that include Calochortus luteus, Cynoglossum grande and Delphinium variegatum.C. unguiculata presents a spindly, hairless, waxy stem not exceeding a meter in height and bears occasional narrow leaves. The showy flowers have hairy, fused sepals forming a cup beneath the corolla, and four petals each one to 2.5 centimeters long. The paddle-like petals are a shade of pink to reddish to purple and are slender and diamond-shaped or triangular. There are eight long stamens, the outer four of which have large red anthers. The stigma protrudes from the flower and can be quite large. Flowers of the genus Clarkia are primarily pollinated by specialist bees found in their native habitat “Clarkias independently developed self-pollination in 12 lineages.”"
  },
  {
    "objectID": "posts/meadow/index.html#grasses",
    "href": "posts/meadow/index.html#grasses",
    "title": "My backyard meadow",
    "section": "4 Grasses",
    "text": "4 Grasses\n\n4.1 Blue grama, Hachita\n\nBouteloua gracilis\n\n\n\n\n\nBouteloua gracilis, the blue grama, is a long-lived, warm-season (C4) perennial grass, native to North America.\n\n\n\n\n4.2 Little bluestem\n\nSchizachyrium scoparium\n\n\n\n\n\nSchizachyrium scoparium, commonly known as little bluestem or beard grass, is a species of North American prairie grass native to most of the contiguous United States (except California, Nevada, and Oregon) as well as a small area north of the Canada–US border and northern Mexico. It is most common in the Midwestern prairies and is one of the most abundant native plants in Texas grasslands.\n\n\n\n\n4.3 Indian Ricegrass\n\nAchnatherum hymenoides\n\n\n\n\n\nEriocoma hymenoides (common names: Indian ricegrass and sand rice grass) is a cool-season, perennial bunchgrass with narrow, rolled leaf blades. It is native to western North America east of the Cascades from British Columbia and Alberta south to southern California, northeastern Mexico, and Texas.\n\n\n\n\n4.4 Sideoats grama, El Reno\n\nBouteloua curtipendula\n\n\n\n\n\nBouteloua curtipendula, commonly known as sideoats grama, is a perennial, short prairie grass that is native throughout the temperate and tropical Western Hemisphere, from Canada south to Argentina.\n\n\n\n\n4.5 Galleta, Viva\n\nHilaria jamesii\n\n\n\n\n\nIt is native to the southwestern United States, where it is widespread in scrub, woodland, grassland, and plateau habitat. It is tolerant of arid environments such as desert floors. It is common in the northern Mojave Desert.\n\n\n\n\n4.6 Alkali sacaton, VNS\n\nSporobolus airoides\n\n\n\n\n\nSporobolus airoides is a species of grass known by the common name alkali sacaton. It is native to western North America, including the Western United States west of the Mississippi River, British Columbia and Alberta in Canada, and northern and central Mexico. It grows in many types of habitat, often in alkali soils, such as in California desert regions.\n\n\n\n\n4.7 Western wheatgrass, Arriba\n\nPascopyrum smithii\n\n\n\n\n\nPascopyrum is a monotypic genus of grass containing the sole species Pascopyrum smithii, which is known by the common names western wheatgrass and red-joint wheatgrass, after the red coloration of the nodes. It is native to North America.\n\n\n\n\n4.8 Sand dropseed, VNS\n\nSporobolus cryptandrus\n\n\n\n\n\nSporobolus cryptandrus is a species of grass known as sand dropseed. It is native to North America, where it is widespread in southern Canada, most of the United States, and northern Mexico.\n\n\n\n\n4.9 Buffalo grass, Texoka\n\nBuchloe dactyloides\n\n\n\n\n\nBouteloua dactyloides, commonly known as buffalograss or a buffalo grass, is a North American prairie grass native to Canada, Mexico, and the United States. It is a shortgrass found mainly on the High Plains and is co-dominant with blue grama (B. gracilis) over most of the shortgrass prairie.\n\n\n\n\n4.10 Sheep fescue, Covar\n\nFestuca ovina\n\n\n\n\n\nFestuca ovina, sheep’s fescue or sheep fescue, is a species of grass. It is sometimes confused with hard fescue (Festuca trachyphylla).\n\n\n\n\n4.11 Green needlegrass, Lodorm\n\nNassella viridula\n\n\n\n\n\nNassella viridula is a species of grass known by the common name green needlegrass. It is native to North America, where it is widespread in western Canada and the western and central United States. It is introduced in parts of eastern North America.\n\n\n\n\n4.12 Perennial ryegrass, Linn\n\nLolium perenne\n\n\n\n\n\nLolium perenne, common name perennial ryegrass,[failed verification]English ryegrass, winter ryegrass, or ray grass, is a grass from the family Poaceae. It is native to Europe, Asia and northern Africa, but is widely cultivated and naturalised around the world."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jason Timm",
    "section": "",
    "text": "meadows\n\n\n\nA programmatic build of a plant species reference guide using Google Images and Wikipedia\n\n\n\n\n\n\nAug 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\n\nA structured, Wikipedia-based timeline of American presidencies 45 & 46\n\n\n\n\n\n\nJun 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnlp\n\n\nclassification\n\n\n\nsome approaches to text classification using R\n\n\n\n\n\n\nJun 14, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npython\n\n\nlexical semantics\n\n\n\nSome notes on implementing reticulate, spacy, and BERT\n\n\n\n\n\n\nJul 9, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\namerican politics\n\n\n\nThe basics of gerrymandering: a simple simulation\n\n\n\n\n\n\nFeb 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndemographics\n\n\n\nA quick look at the composition of American generations\n\n\n\n\n\n\nJun 10, 2020\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "linguistic innovation and change in online speech communities;\npolitical communication on social media;\nthe integration of corpus methods and geographic information systems; and\nthe development of open source tools for linguistic analysis & text featurization.\n\nThis site is built with r::blogdown and Hugo, and deployed with Netlify. Check out R-bloggers & rweekly.org for additonal R resources.\nI live in Albuquerque, New Mexico, and mostly just bicycle about."
  }
]