{
  "hash": "a7c3f474f4eed8d73099c5e724406d2d",
  "result": {
    "markdown": "---\ntitle: \"notes on text classification\"\ndate: \"2022-06-14\"\ncategories: [nlp, classification]\ndescription: \"some approaches to text classification using R\"\n\nbibliography: /home/jtimm/pCloudDrive/GitHub/jtimm_web/biblio.bib\nimage: preview.png\n\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    number-sections: true\n---\n\n\n\n![](preview2.png){width=100% .preview-image}\n\n\n\n---\n\n\n> A re-worked version of a previous post.  A very small survey of some simple, but effective approaches to text classification using R, with a focus on Naive Bayes and FastText classifiers.  \n\n\n\n## Labeled data\n\nFor demonstration purposes, we build a corpus using the [quicknews](https://github.com/jaytimm/quicknews) package.  The corpus is comprised of articles returned from a set of health-related queries.  Search terms, then, serve as classification labels.  An imperfect annotation process, but fine for our purposes here.  As \"distant\" supervision.\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nterms <- c('heart disease', \n           'diabetes',\n           'mental health',\n           'substance abuse',\n           'obesity',\n           'kidney disease')\n\nrss <- lapply(terms, function(x) {\n  \n  quicknews::qnews_build_rss(x) %>%\n    quicknews::qnews_strip_rss() }) \n\nnames(rss) <- terms\n  \nrss0 <- rss %>%\n  bind_rows(.id = 'term') %>%\n  mutate(term = gsub(' ', '_', term)) %>%\n  distinct(link, .keep_all = TRUE) %>%\n  mutate(doc_id = as.character(row_number())) %>%\n  mutate(term = as.factor(term))  %>%\n  select(doc_id, term:link)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\narticles <- quicknews::qnews_extract_article(url = rss0$link, cores = 7) \narticles0 <- articles %>% left_join(rss0)\n```\n:::\n\n\n\n\n\n\n\n\n**Descriptives** for the resulting corpus by search term.  So, a super small demo corpus.\n\n::: {.cell}\n\n```{.r .cell-code}\narticles0 %>%\n  mutate(words = tokenizers::count_words(text)) %>%\n  group_by(term) %>%\n  summarize(n = n(), words = sum(words)) %>%\n  janitor::adorn_totals() %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|term            |   n|  words|\n|:---------------|---:|------:|\n|diabetes        |  81|  72045|\n|heart_disease   |  82|  65505|\n|kidney_disease  |  80|  68379|\n|mental_health   |  82|  73804|\n|obesity         |  81|  69179|\n|substance_abuse |  84|  64133|\n|Total           | 490| 413045|\n:::\n:::\n\n\n\n**A sample of articles** from the GoogleNews/`quicknews` query:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(99)\narticles0 %>% \n  select(term, date, source, title) %>% \n  sample_n(5) %>%\n  knitr::kable()\n```\n\n::: {.cell-output-display}\n|term            |date       |source                 |title                                                                               |\n|:---------------|:----------|:----------------------|:-----------------------------------------------------------------------------------|\n|kidney_disease  |2022-05-23 |Reuters.com            |U.S. Task Force to consider routine kidney disease screening as new drugs available |\n|substance_abuse |2022-06-15 |The Sylva Herald       |District Court &#124; Court News &#124; thesylvaherald.com                          |\n|substance_abuse |2022-05-28 |WUSF News              |Northeast Florida's health care concerns: from substance abuse to food deserts      |\n|heart_disease   |2022-05-23 |Bloomberg              |Take Breaks and Watch Less TV to Slash Heart Disease Risk, Experts Say              |\n|substance_abuse |2022-06-14 |Williston Daily Herald |Report: U.S. records highest ever rates of substance abuse, suicide-related deaths  |\n:::\n:::\n\n\n\n\n## Data structures \n\n### Document-Term Matrix\n\n> As bag-of-words\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm <- articles0 %>% \n  mutate(wds = tokenizers::count_words(text)) %>%\n  filter(wds > 200 & wds < 1500) %>%\n  text2df::tif2token() %>%\n  text2df::token2df() %>%\n  mutate(token = tolower(token)) \n  # mutate(stem = quanteda::char_wordstem(token))\n\ndtm %>% head() %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n|doc_id |token   | token_id|\n|:------|:-------|--------:|\n|2      |heart   |        1|\n|2      |disease |        2|\n|2      |is      |        3|\n|2      |a       |        4|\n|2      |very    |        5|\n|2      |general |        6|\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm_tok <- dtm %>%  \n  count(doc_id, token) %>%\n  group_by(token) %>%\n  mutate(docf = length(unique(doc_id))) %>% ungroup() %>%\n  mutate(docf = round(docf/length(unique(doc_id)), 3 )) %>%\n  filter(docf >= 0.01 & docf < 0.5 & \n           !grepl('^[0-9]|^[[:punct:]]', token))\n\ndtm_tok %>% head() %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n|doc_id |token       |  n|  docf|\n|:------|:-----------|--:|-----:|\n|10     |acknowledge |  1| 0.031|\n|10     |across      |  1| 0.207|\n|10     |additional  |  1| 0.151|\n|10     |address     |  3| 0.217|\n|10     |adhere      |  1| 0.018|\n|10     |ads         |  1| 0.026|\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm_sparse <- dtm_tok %>%\n  tidytext::bind_tf_idf(term = token, \n                        document = doc_id,\n                        n = n) %>% \n  tidytext::cast_sparse(row = doc_id, \n                        column = token, \n                        value = tf_idf)\n```\n:::\n\n\n\n\n### Cleaned text\n\n::: {.cell}\n\n```{.r .cell-code}\nctext <- dtm %>%\n  group_by(doc_id) %>%\n  summarize(text = paste0(token, collapse = ' ')) %>% ungroup()\n\nstrwrap(ctext$text[5], width = 60)[1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"upon receiving a diagnosis of type 1 diabetes ( t1d ) ,\"    \n[2] \"many people have the same reaction : “ but why me ? ” some\" \n[3] \"people have t1d that runs in their family , while others\"   \n[4] \"have no idea how or why they received a diagnosis . often ,\"\n[5] \"to their frustration , those questions go unanswered . but\" \n```\n:::\n:::\n\n\n\n### Word embeddings\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## devtools::install_github(\"pommedeterresautee/fastrtext\") \ntmp_file_txt <- tempfile()\ntmp_file_model <- tempfile()\nwriteLines(text = ctext$text, con = tmp_file_txt)\ndims <- 25\nwindow <- 5\n\nfastrtext::execute(commands = c(\"skipgram\",\n                                \"-input\", tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", gsub('^.*\\\\.', '', dims),\n                                \"-ws\", window, \n                                \"-verbose\", 1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRead 0M words\nNumber of words:  5318\nNumber of labels: 0\n\nProgress: 100.0% words/sec/thread:   21912 lr:  0.000000 avg.loss:  2.412294 ETA:   0h 0m 0s\n```\n:::\n\n```{.r .cell-code}\nfast.model <- fastrtext::load_model(tmp_file_model)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nadd .bin extension to the path\n```\n:::\n\n```{.r .cell-code}\nfast.dict <- fastrtext::get_dictionary(fast.model)\nembeddings <- fastrtext::get_word_vectors(fast.model, fast.dict)\n```\n:::\n\n\n## Classifiers\n\n::: {.cell}\n\n```{.r .cell-code}\narticles1 <- articles0 %>%\n  arrange(doc_id) %>%\n  filter(doc_id %in% unique(dtm_tok$doc_id))\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(99)\ntrainIndex <- caret::createDataPartition(articles1$term, p = .7)$Resample1\n```\n:::\n\n\n\n### Bag-of-words & Naive Bayes\n\n> Document represented as bag-of-words.\n\n::: {.cell}\n\n```{.r .cell-code}\ndtm_train <- dtm_sparse[trainIndex, ]\ndtm_test <- dtm_sparse[-trainIndex, ] \ndtm_classifier <- e1071::naiveBayes(as.matrix(dtm_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\ndtm_predicted <- predict(dtm_classifier, as.matrix(dtm_test))\n```\n:::\n\n\n\n\n### Word embeddings & Naive Bayes\n\n> Document represented as an aggregate (here, mean) of constituent word embeddings.  Custom/FastText word embeddings derived from `quicknews` corpus (above).\n\n::: {.cell}\n\n```{.r .cell-code}\nv1 <- embeddings %>% \n  data.frame() %>%\n  mutate(token = rownames(embeddings)) %>%\n  filter(token %in% unique(dtm_tok$token)) %>%\n  inner_join(dtm)\n\navg0 <- lapply(unique(dtm$doc_id), function(y){\n  \n  d0 <- subset(v1, doc_id == y)\n  d1 <- as.matrix(d0[, 1:dims])\n  d2 <-Matrix.utils::aggregate.Matrix(d1,\n                                      groupings = rep(y, nrow(d0)),\n                                      fun = 'mean')\n  as.matrix(d2)\n})\n\ndoc_embeddings <- do.call(rbind, avg0)\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemb_train <- doc_embeddings[trainIndex, ]\nemb_test <- doc_embeddings[-trainIndex, ] \nemb_classifier <- e1071::naiveBayes(as.matrix(emb_train), \n                                    articles1[trainIndex, ]$term, \n                                    laplace = 0.5) \n\nemb_predicted <- predict(emb_classifier, as.matrix(emb_test))\n```\n:::\n\n\n\n\n### FastText classifier\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfast_train <- articles1[trainIndex, ]\nfast_test <- articles1[-trainIndex, ]\n```\n:::\n\n\nPrepare data for FastText:\n\n::: {.cell}\n\n```{.r .cell-code}\ntmp_file_model <- tempfile()\n\ntrain_labels <- paste0(\"__label__\", fast_train$term)\ntrain_texts <- tolower(fast_train$text)\ntrain_to_write <- paste(train_labels, train_texts)\ntrain_tmp_file_txt <- tempfile()\nwriteLines(text = train_to_write, con = train_tmp_file_txt)\n\ntest_labels <- paste0(\"__label__\", fast_test$term)\ntest_texts <- tolower(fast_test$text)\ntest_to_write <- paste(test_labels, test_texts)\n```\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfastrtext::execute(commands = c(\"supervised\", \n                                \"-input\", train_tmp_file_txt, \n                                \"-output\", tmp_file_model, \n                                \"-dim\", 25, \n                                \"-lr\", 1, \n                                \"-epoch\", 20, \n                                \"-wordNgrams\", 2, \n                                \"-verbose\", 1))\n\nmodel <- fastrtext::load_model(tmp_file_model)\nfast_predicted0 <- predict(model, sentences = test_to_write)\nfast_predicted <- as.factor(names(unlist(fast_predicted0)))\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n## Evaluation\n\n::: {.cell}\n\n```{.r .cell-code}\npredictions <- list('BOW' = dtm_predicted, \n                    'Word embeddings' = emb_predicted, \n                    'FastText' = fast_predicted)\n```\n:::\n\n\n\n### Model accuracy\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mats <- lapply(predictions, \n                    caret::confusionMatrix,\n                    reference = articles1[-trainIndex, ]$term)\n\nsums <- lapply(conf_mats, '[[', 3) \nsums0 <- as.data.frame(do.call(rbind, sums)) %>%\n  select(1:4) %>%\n  mutate_at(1:4, round, 3)\n\nsums0 %>% arrange(desc(Accuracy)) %>% knitr::kable()\n```\n\n::: {.cell-output-display}\n|                | Accuracy| Kappa| AccuracyLower| AccuracyUpper|\n|:---------------|--------:|-----:|-------------:|-------------:|\n|FastText        |    0.757| 0.708|         0.668|         0.832|\n|BOW             |    0.661| 0.593|         0.567|         0.747|\n|Word embeddings |    0.435| 0.320|         0.343|         0.530|\n:::\n:::\n\n\n\n### FastText classifier: Model accuracy by class\n\n::: {.cell}\n\n```{.r .cell-code}\nconf_mats[['FastText']]$byClass %>% data.frame() %>%\n  select (Sensitivity, Specificity, Balanced.Accuracy) %>%\n  rownames_to_column(var = 'topic') %>%\n  mutate(topic = gsub('Class: ','', topic)) %>% \n  mutate_if(is.numeric, round, 2) %>% \n  knitr::kable() \n```\n\n::: {.cell-output-display}\n|topic           | Sensitivity| Specificity| Balanced.Accuracy|\n|:---------------|-----------:|-----------:|-----------------:|\n|diabetes        |        0.75|        1.00|              0.88|\n|heart_disease   |        0.56|        0.99|              0.77|\n|kidney_disease  |        0.89|        0.86|              0.88|\n|mental_health   |        0.89|        0.96|              0.93|\n|obesity         |        0.74|        0.93|              0.83|\n|substance_abuse |        0.70|        0.97|              0.83|\n:::\n:::\n\n\n### FastText classifier: Confusion matrix\n\n::: {.cell}\n\n```{.r .cell-code}\ndp <- as.data.frame(conf_mats[['FastText']]$table)\n\nggplot(data = dp,\n       aes(x = Reference, y = Prediction)) +\n  \n    geom_tile(aes(fill = log(Freq)), \n              colour = \"white\") +\n    scale_fill_gradient(low = \"white\", \n                        high = \"steelblue\") +\n  \n    geom_text(data = dp,\n              aes(x = Reference, \n                  y = Prediction, \n                  label = Freq)) +\n    theme(legend.position = \"none\",\n          axis.text.x=element_text(angle=45,\n                                   hjust=1)) + \n    labs(title=\"Confusion Matrix\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n## Summary\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}